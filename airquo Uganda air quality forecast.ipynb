{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Until recently there has been a lack of data on air quality across sub-Saharan Africa. Reference grade monitors are extremely expensive and without access to data it is very difficult to raise awareness of the issues, or for government, business and individuals to know which actions to take to improve air quality and protect community health.\n",
    "\n",
    "AirQo has built a low-cost network of sensors and collected data across 65 locations in Uganda with some sites monitoring for over three years. There is now a wealth of data which can be used to achieve impact in this critical area. Birmingham University’s ASAP project makes use of this and similar data to gain insights into the relationship between urbanisation and air quality.\n",
    "\n",
    "The increase in availability of air quality data allows us to analyse historical and up to the minute results to gain insights into trends, hotspots, causes and consequences of poor air, potential policy solutions and so much more.\n",
    "\n",
    "The ability to accurately predict what air quality will be in the coming days is also essential for empowering everyone from governments to families to make informed decisions to protect health and guide action, just as we do with weather.\n",
    "\n",
    "Activities that are known to contribute to poor air quality such as high traffic volumes, rubbish burning, cooking using charcoal and firewood, even construction or other government works can also be reconsidered depending on the forecast.\n",
    "\n",
    "If it is known that the next day will be a high pollution day then sports or other events may need to be rescheduled or relocated. Sensitive groups such as children, the elderly, sick or those with respiratory illnesses may need to remain inside. Schools can plan the timing of outdoor activities such as field trips or sports events with confidence.\n",
    "\n",
    "We are hopeful that these forecasts will be used to inform public awareness and be built into safety alerts. They can become part of daily news coverage whether in traditional or social media in the same way weather is currently presented. The solution could literally be life saving.\n",
    "\n",
    "The objective of this challenge is to accurately forecast air quality (as measured by PM2.5 µ/m3) for each hour of the coming 25 hours across five locations in Kampala Uganda. Forecasts will be based on the past 5 days of hourly air quality measurements at each site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main indicators of air quality is PM2.5 (particulate matter smaller than 2.5 micrometers in diameter or around 1/30th the thickness of a human hair). These particles can be generated by traffic exhaust, industry, burning of fossil fuels and many other sources. The particles are so small that they are invisible to the naked eye and when inhaled do not just collect in the lungs and cause respiratory disease but can also enter the bloodstream and contribute to heart disease and stroke. The critical measure is the mass of PM2.5 particles in a volume of air, given by micrograms per cubic meter (µ/m3).\n",
    "\n",
    "Guidelines on hazardous levels of PM2.5 are given below\n",
    "\n",
    "\n",
    "This data has been collected from five sensors stationed across Uganda. Readings are taken every hour. As is the reality with all data there are missing values. This is a challenge for you to overcome.\n",
    "\n",
    "Your solution needs to be generalizable and be able to applied to all test periods without changing the solution parameters.\n",
    "\n",
    "The objective of this challenge is to predict the air quality level at exactly 24 hours after a 5-day series of hourly weather data readings which include temperature, rainfall, wind, and humidity.\n",
    "\n",
    "For example, you may be given weather indicators (but no air quality data) from 3:00 am on 9 March to 3:00 am on 14 March. Based on these weather indicators, you will need to predict the air quality reading at exactly 3:00 am on 15 March (24 hours after the last weather data reading). Note that you are not given the date or time for any of the data.\n",
    "\n",
    "The weather indicators available in the train and test are:\n",
    "\n",
    "temp: mean temperature recorded at the site over the hour (degrees Celsius)*\n",
    "\n",
    "precip: total rainfall in mm recorded at the site over the hour (mm)*\n",
    "\n",
    "wind_dir: mean direction of the wind over the hour (degrees)*\n",
    "\n",
    "wind_spd: mean wind speed at the site over the hour (metres per second)*\n",
    "\n",
    "atmos_press: mean atmospheric pressure(atm)*\n",
    "\n",
    "Each series of weather and air quality readings will be associated with a unique sensor. You will have a set of features on each of the five sensor:\n",
    "\n",
    "Location: one of either A,B,C, D or E referring to the five different locations selected\n",
    "\n",
    "loc_altitude: The height above sea level at the location in metres\n",
    "\n",
    "Km2: the area of the parish in which the device is located in square km\n",
    "\n",
    "Aspect: the direction the slope on which the device is located faces\n",
    "\n",
    "Dist_motorway: distance of the device from the nearest motorway in metres (values greater than \n",
    "5000m are NaN)✝✝\n",
    "\n",
    "Dist_trunk: distance of the device from the nearest trunk road in metres (values greater than 5000m are NaN)✝✝\n",
    "\n",
    "Dist_primary: distance of the device from the nearest primary road in metres (values greater than 5000m are NaN)✝✝\n",
    "\n",
    "Dist_secondary: distance of the device from the nearest secondary road in metres (values greater than 5000m are NaN)✝✝\n",
    "\n",
    "Dist_tertiary: distance of the device from the nearest tertiary road in metres (values greater than 5000m are NaN)✝✝\n",
    "\n",
    "Dist_unclassified: distance of the device from the nearest unclassified road in metres (values greater than 5000m are NaN)✝✝\n",
    "\n",
    "Dist_residential: distance of the device from the nearest residential road in metres (values greater than 5000m are NaN)✝✝\n",
    "\n",
    "Popn: The population of the parish in which the device is located✝\n",
    "\n",
    "Hh: the number of households in the parish in which the device is located✝\n",
    "\n",
    "Hh_cook_charcoal: number of households in the parish in which the device is located which cook \n",
    "using charcoal✝\n",
    "\n",
    "Hh_cook_firewood: number of households in the parish in which the device is located which cook using firewood✝\n",
    "\n",
    "Hh_burn_waste: number of households in the parish in which the device is located which dispose of solid household waste by burning.✝\n",
    "\n",
    "\n",
    "*Data courtesy of Tahmo network - https://tahmo.org/\n",
    "\n",
    "✝Data courtesy of Ugandan Bureau of Statistics - https://www.ubos.org/\n",
    "\n",
    "✝✝For definition of road classification see - \n",
    "\n",
    "https://wiki.openstreetmap.org/wiki/Key:highway#Roads\n",
    "The target variable is pm2_5, i.e. mean mass of particulate matter smaller than 2.5 micrometres per cubic metre of air (µ/m3), as read exactly 24 hours after the last weather indicators’ reading.\n",
    "\n",
    "The training data consists of 15,000 sets of 5 days of hourly weather data readings plus one air quality reading exactly 24 hours after the last weather reading. The test set consists of a different 5,000 sets of 5-day hourly weather data readings.\n",
    "\n",
    "Files available for download are:\n",
    "\n",
    "Airqo_metadata.csv - This provides additional background information about the location and features of the parish in which each sensor is located.\n",
    "\n",
    "\n",
    "StarterNotebook.ipynb - this is a starter notebook.\n",
    "\n",
    "\n",
    "sample_sub.csv - is an example of what your submission file should look like. The order of the rows does not matter, but the names of the ID must be correct.\n",
    "\n",
    "\n",
    "Train.p - this is what you will use to train your model\n",
    "\n",
    "\n",
    "Test.p - this is what you will test your model on\n",
    "\n",
    "\n",
    "Train.csv - this is the same file as Train.p but in csv format. The sequences are a string object and the values are separated by “,”. You will need to convert the string sequence into a list sequence.\n",
    "\n",
    "\n",
    "Test.csv - this is the same file as Test.p but in csv format. The sequences are a string object and the values are separated by “,”. You will need to convert the string sequence into a list sequence.\n",
    "\n",
    "Please note that the public leaderboard may not represent the full distribution of the year.\n",
    "\n",
    "Notes for implementation\n",
    "\n",
    "After the close of the challenge and the first meeting with Airqo the winner chosen for implementation will be given access to the full data set, including the reference files. They will also be given access to the undoctored forecast data for all 5 sensors including dates and locations.\n",
    "\n",
    "AirQo collects data from 65 sensors around Uganda with recordings every 1.5 minutes. Data received from devices undergoes basic cleaning and is stored in BigQuery on Google Cloud Platform. Our website calls an API on an hourly basis and generates a forecast for each location for each of the coming 24 hours and caches it. Very little work is required to get the data into the desired form and a very basic model is currently implemented. The forecast is then updated live on our website and app so your solution will be available to all Ugandans, helping them plan and make decisions based on air quality.\n",
    "\n",
    "Thought will need to be given to process speed, resources needed and sustainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.stats import t\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import  lightgbm as lgbm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"Train.csv\")\n",
    "test=pd.read_csv(\"Test.csv\")\n",
    "sample_sub=pd.read_csv(\"sample_sub.csv\")\n",
    "\n",
    "def replace_nan(x):\n",
    "    if x==\" \":\n",
    "        return np.nan\n",
    "    else :\n",
    "        return float(x)\n",
    "features=[\"temp\",\"precip\",\"rel_humidity\",\"wind_dir\",\"wind_spd\",\"atmos_press\"]\n",
    "for feature in features : \n",
    "    train[feature]=train[feature].apply(lambda x: [ replace_nan(X) for X in x.replace(\"nan\",\" \").split(\",\")])\n",
    "    test[feature]=test[feature].apply(lambda x: [ replace_nan(X)  for X in x.replace(\"nan\",\" \").split(\",\")])    \n",
    "def remove_nan_values(x):\n",
    "    return [e for e in x if not math.isnan(e)]\n",
    "data=pd.concat([train,test],sort=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then generate 2 data_sets based on the original one : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_one(data):\n",
    "    '''this nested loop clusters the hourly means of our features in the last \n",
    "    24 columns'''\n",
    "    features=[\"temp\",\"precip\",\"rel_humidity\",\"wind_dir\",\"wind_spd\",\"atmos_press\"]\n",
    "\n",
    "    for col_name in tqdm(features):\n",
    "        for j in range(len(data)): \n",
    "            for i in range(96,121): \n",
    "                    try :\n",
    "                        data[col_name][j][i] = np.nanmean(data[col_name][j][(i%24):121:24])\n",
    "                    except ValueError as e:\n",
    "                        data[col_name][j][i]  = np.nan\n",
    "    '''this one computes the number of hours since an event , for example \n",
    "    hours since max temp ...'''\n",
    "    for col_name in tqdm(features):\n",
    "        for j in range(len(data)): \n",
    "            try :\n",
    "                data[\"hours_since_min_\"+col_name] =  (24 - np.nanargmin(data[col_name][j][96 : 121])) \n",
    "            except ValueError as e:\n",
    "                data[\"hours_since_min_\"+col_name] = np.nan\n",
    "            try :\n",
    "                data[\"hours_since_max_\"+col_name] =  (24 - np.nanargmax(data[col_name][j][96 : 121]) )  \n",
    "            except ValueError as e:\n",
    "                data[\"hours_since_max_\"+col_name] = np.nan\n",
    "    for col_name in tqdm(features):\n",
    "        for j in range(len(data)): \n",
    "            try :\n",
    "                data[\"hours_since_min_\"+col_name][j] =  (24 - np.nanargmin(data[col_name][j][96 : 121])) \n",
    "            except ValueError as e:\n",
    "                data[\"hours_since_min_\"+col_name][j] = np.nan\n",
    "            try :\n",
    "                data[\"hours_since_max_\"+col_name][j] =  (24 - np.nanargmax(data[col_name][j][96 : 121]) )  \n",
    "            except ValueError as e:\n",
    "                data[\"hours_since_max_\"+col_name][j] = np.nan\n",
    "    '''this part calculates the value of features at max and min temperature'''\n",
    "    def get_max(w,df_test):\n",
    "        w=w*24\n",
    "        x=df_test[temp_cols[w:w+24+1]].idxmax(axis=1).apply(lambda x : x.split(\"newtemp\")[-1])\n",
    "        return x\n",
    "    def get_min(w,df_test):\n",
    "        w*=24\n",
    "        x=df_test[temp_cols[w:w+24+1]].idxmin(axis=1).apply(lambda x : x.split(\"newtemp\")[-1])\n",
    "        return x\n",
    "\n",
    "\n",
    "    def feature_aug_max(w,feature,df_test):\n",
    "        df_test[feature+\"_at_max_temp_at_day_\"+str(w)] = df_test.lookup((feature+get_max(w,df_test)).index,(feature+get_max(w,df_test)).values)\n",
    "        return df_test\n",
    "    def feature_aug_min(w,feature,df_test):\n",
    "        df_test[feature+\"_at_min_temp_at_day_\"+str(w)] = df_test.lookup((feature+get_min(w,df_test)).index,(feature+get_min(w,df_test)).values)\n",
    "        return df_test\n",
    "    new_features = [\"newtemp\",\"newprecip\",\"newrel_humidity\" ,\"newwind_dir\" ,\"windspeed\",\"atmospherepressure\"]\n",
    "\n",
    "    '''in this part we extract the last 24 columns of each list in our data '''\n",
    "    temp_cols = [\"newtemp\"+ str(x) for x in range(121)] \n",
    "    precip_cols =[\"newprecip\"+ str(x) for x in range(121)]\n",
    "    rel_humidity_cols = [\"newrel_humidity\"+ str(x) for x in range(121)] \n",
    "    wind_dir_cols = [\"newwind_dir\"+ str(x) for x in range(121)] \n",
    "    wind_spd_cols = [\"windspeed\"+ str(x) for x in range(121)] \n",
    "    atmos_press_cols = [\"atmospherepressure\"+ str(x) for x in range(121)]\n",
    "    for x in tqdm(range(96,121)):\n",
    "        data[\"newtemp\"+ str(x)] = data.temp.str[x]\n",
    "        data[\"newprecip\"+ str(x)] = data.precip.str[x]\n",
    "        data[\"newrel_humidity\"+ str(x)] = data.rel_humidity.str[x]\n",
    "        data[\"newwind_dir\"+ str(x)] = data.wind_dir.str[x]\n",
    "        data[\"windspeed\"+ str(x)] = data.wind_spd.str[x]\n",
    "        data[\"atmospherepressure\"+ str(x)] = data.atmos_press.str[x]\n",
    "    '''we apply the functions defined above to the data'''\n",
    "    for feature in tqdm(new_features):\n",
    "        for w in range(4,5):\n",
    "            data=feature_aug_min(w,feature,data)\n",
    "    for feature in tqdm(new_features):\n",
    "        for w in range(4,5):\n",
    "            data=feature_aug_max(w,feature,data)\n",
    "    for col_name in tqdm(features):\n",
    "        data[col_name]=data[col_name].apply(remove_nan_values)\n",
    "    '''after removing nan values we calculate some statistics for each\n",
    "    feature, remember that the last 24 columns contains the means calculated discarding nan values \n",
    "    this helps controling the contribution of each day to the statistics we are calculating below'''\n",
    "    def aggregate_features(x,col_name):\n",
    "        x[\"max_\"+col_name]=x[col_name].apply(np.max)\n",
    "        x[\"min_\"+col_name]=x[col_name].apply(np.min)\n",
    "        x[\"mean_\"+col_name]=x[col_name].apply(np.mean)\n",
    "        x[\"std_\"+col_name]=x[col_name].apply(np.std)\n",
    "        x[\"var_\"+col_name]=x[col_name].apply(np.var)\n",
    "        x[\"median_\"+col_name]=x[col_name].apply(np.median)\n",
    "        x[\"ptp_\"+col_name]=x[col_name].apply(np.ptp)\n",
    "        return x\n",
    "    for col_name in tqdm(features):\n",
    "        data=aggregate_features(data,col_name)\n",
    "    '''we ampute data using KNN'''\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    data[wind_spd_cols[108:]] = imputer.fit_transform(data[wind_spd_cols[108:]])\n",
    "    data[temp_cols[108:]] = imputer.fit_transform(data[temp_cols[108:]])\n",
    "    data[precip_cols[108:]] = imputer.fit_transform(data[precip_cols[108:]])\n",
    "    data[rel_humidity_cols[108:]] = imputer.fit_transform(data[rel_humidity_cols[108:]])\n",
    "    data[wind_dir_cols[108:]] = imputer.fit_transform(data[wind_dir_cols[108:]])\n",
    "    data[atmos_press_cols[108:]] = imputer.fit_transform(data[atmos_press_cols[108:]])\n",
    "    \n",
    "    '''I used multiple runs of this code to determine the most relevant features , from the features '''\n",
    "    keep = [\n",
    "    'ID','location','target','hours_since_min_temp','hours_since_max_temp',\n",
    "     'hours_since_min_precip','hours_since_max_precip','hours_since_min_rel_humidity',\n",
    "     'hours_since_max_rel_humidity','hours_since_min_wind_dir','hours_since_max_wind_dir',\n",
    "     'hours_since_min_wind_spd','hours_since_max_wind_spd','hours_since_min_atmos_press',\n",
    "     'hours_since_max_atmos_press','newtemp117',\n",
    "     'newprecip117','newrel_humidity117','newwind_dir117','windspeed117','atmospherepressure117','newtemp118','newprecip118','newrel_humidity118',\n",
    "     'newwind_dir118',\n",
    "     'windspeed118',\n",
    "     'atmospherepressure118',\n",
    "     'newtemp119',\n",
    "     'newprecip119',\n",
    "     'newrel_humidity119',\n",
    "     'newwind_dir119',\n",
    "     'windspeed119',\n",
    "     'atmospherepressure119',\n",
    "     'newtemp120',\n",
    "     'newprecip120',\n",
    "     'newrel_humidity120',\n",
    "     'newwind_dir120',\n",
    "     'windspeed120',\n",
    "     'atmospherepressure120',\n",
    "     'atmospherepressure96',\n",
    "     'newprecip106',\n",
    "     'newrel_humidity106',\n",
    "     'newwind_dir106',\n",
    "     'windspeed106',\n",
    "     'atmospherepressure106',\n",
    "     'windspeed107',\n",
    "     'atmospherepressure107',\n",
    "     'newtemp_at_max_temp_at_day_4',\n",
    "     'newprecip_at_max_temp_at_day_4',\n",
    "     'newrel_humidity_at_max_temp_at_day_4',\n",
    "     'newwind_dir_at_max_temp_at_day_4',\n",
    "     'windspeed_at_max_temp_at_day_4',\n",
    "     'atmospherepressure_at_max_temp_at_day_4',\n",
    "     'max_temp',\n",
    "     'min_temp',\n",
    "     'mean_temp',\n",
    "     'std_temp',\n",
    "     'var_temp',\n",
    "     'median_temp',\n",
    "     'ptp_temp',\n",
    "     'max_precip',\n",
    "     'min_precip',\n",
    "     'mean_precip',\n",
    "     'std_precip',\n",
    "     'var_precip',\n",
    "     'median_precip',\n",
    "     'ptp_precip',\n",
    "     'max_rel_humidity',\n",
    "     'min_rel_humidity',\n",
    "     'mean_rel_humidity',\n",
    "     'std_rel_humidity',\n",
    "     'var_rel_humidity',\n",
    "     'median_rel_humidity',\n",
    "     'ptp_rel_humidity',\n",
    "     'max_wind_dir',\n",
    "     'min_wind_dir',\n",
    "     'mean_wind_dir',\n",
    "     'std_wind_dir',\n",
    "     'var_wind_dir',\n",
    "     'median_wind_dir',\n",
    "     'ptp_wind_dir',\n",
    "     'max_wind_spd',\n",
    "     'min_wind_spd',\n",
    "     'mean_wind_spd',\n",
    "     'std_wind_spd',\n",
    "     'var_wind_spd',\n",
    "     'median_wind_spd',\n",
    "     'ptp_wind_spd',\n",
    "     'max_atmos_press',\n",
    "     'min_atmos_press',\n",
    "     'mean_atmos_press',\n",
    "     'std_atmos_press',\n",
    "     'var_atmos_press',\n",
    "     'median_atmos_press',\n",
    "     'ptp_atmos_press']\n",
    "    data = data[keep] \n",
    "    \n",
    "    '''In this part we add some statistics about the target data, I noticed that target follows a ognormal distribution \n",
    "    with slightly different parameters based on location, so i added those parameters, it is worth noting though that after \n",
    "    log transformation i noticed that the kurtosis is slightly different from normal so i uned the t distribution instead'''\n",
    "    data['log_target'] = np.log(data.target)\n",
    "    # data[]['target'].where(data.location == 0)\n",
    "    data['location0'] = np.nan\n",
    "    data['location1'] = np.nan\n",
    "    data['location2'] = np.nan\n",
    "\n",
    "    def adding_more_juice(loca,df):\n",
    "        tup = t.fit(df.log_target[df.log_target.where(df.location == loca ).notnull()])\n",
    "        df.loc[df['location'] == loca, ['location0']] = tup[0]\n",
    "        df.loc[df['location'] == loca, ['location1']] = tup[1]\n",
    "        df.loc[df['location'] == loca, ['location2']] = tup[2]\n",
    "        return df\n",
    "    def adding_stats(df):\n",
    "        df['target_tmode']  = np.exp(df['location1'])\n",
    "        df['target_tmean']  = np.exp(df['location1'] + (df['location2'] ) / 2 )\n",
    "        df['target_tstd']  =  np.sqrt(np.exp(df['location1']*2 + df['location2'] )*np.exp((df['location2'] )- 1))\n",
    "        return df\n",
    "\n",
    "    for loca in tqdm(data.location.unique()):\n",
    "        data = adding_more_juice(loca,data)\n",
    "    data = adding_stats(data)\n",
    "    \n",
    "    '''here we encoded the caegorical data i.e location'''\n",
    "    cat_encoder = LabelEncoder()\n",
    "    data[\"location\"] = cat_encoder.fit_transform(data.location)\n",
    "    '''spliting test and train data'''\n",
    "    train=data[data.target.notnull()].reset_index(drop=True)\n",
    "    test=data[data.target.isna()].reset_index(drop=True)\n",
    "    del data\n",
    "    gc.collect()\n",
    "    Experiment_name = 'model1'\n",
    "    '''creating folds '''\n",
    "    os.makedirs(\"proc_data\", exist_ok=True)\n",
    "    try : \n",
    "        folds=pd.read_csv(\"./proc_data/folds_id.csv\")\n",
    "        train=train.merge(folds,on=\"ID\",how=\"left\")\n",
    "        train.fold.nunique()\n",
    "    except : \n",
    "\n",
    "        from sklearn.model_selection import KFold \n",
    "        kfold=KFold(n_splits=5,shuffle=True,random_state=2020) # change this random_state or all of you will have the same score  :D \n",
    "        train.reset_index(drop=True,inplace=True)\n",
    "        folds=train[[\"ID\"]].copy()\n",
    "        folds[\"fold\"]=0\n",
    "        for fold,(tr_indx,val_ind) in enumerate(kfold.split(folds)) : \n",
    "            folds.loc[val_ind,\"fold\"]=fold\n",
    "        folds.to_csv(\"./proc_data/folds_id.csv\",index=False)\n",
    "        train=train.merge(folds,on=\"ID\",how=\"left\")\n",
    "\n",
    "        del folds\n",
    "    \n",
    "    target_name=\"target\"\n",
    "    id_name=\"ID\"\n",
    "    features_to_remove=[target_name,id_name,\"fold\",\"log_target\"]\n",
    "    features=train.columns.tolist()\n",
    "    features=[ fea for fea in  features if fea not in features_to_remove  ]\n",
    "    '''scaling the data'''\n",
    "    scaler = StandardScaler()\n",
    "    unscaled_train = train\n",
    "    train[features] = scaler.fit_transform(unscaled_train[features])\n",
    "    test_unscaled=test\n",
    "    test[features] = scaler.transform(test_unscaled[features])\n",
    "    '''modelisation part'''\n",
    "    def metric(y,x):\n",
    "        return np.sqrt(mean_squared_error(x,y))\n",
    "    def train_function(model,train,test,params,other_params,target_name,features,metric):\n",
    "        folds_num=train.fold.nunique()\n",
    "        validation=train[[\"ID\",\"fold\",target_name]].copy()\n",
    "        validation[\"pred_\"+target_name]=0\n",
    "        sub=test[[\"ID\"]].copy()\n",
    "        sub[target_name]=0\n",
    "        for fold in np.sort(train.fold.unique()):\n",
    "            print(\"#\"*50+\" {} \".format(fold)+\"#\"*50)\n",
    "            os.makedirs(\"model_save/lgbm/{}/{}\".format(Experiment_name,str(int(fold))), exist_ok=True)\n",
    "            X_train=train[train.fold!=fold]\n",
    "            X_val=train[train.fold==fold]\n",
    "\n",
    "            train_pred,validation_pred,test_pred,model_save=model(X_train,X_val,test,params,other_params)\n",
    "\n",
    "            validation.loc[validation.fold==fold,\"pred_\"+target_name]=validation_pred\n",
    "            sub[target_name]+=test_pred/folds_num\n",
    "            train_score=metric(X_train[target_name],train_pred)\n",
    "            val_score=metric(X_val[target_name],validation_pred)\n",
    "            print(\"train score : {} validation score : {}\".format(round(train_score,4),round(val_score,4)))\n",
    "        final_validation_score=metric(validation[target_name],validation[\"pred_\"+target_name])\n",
    "        print(\"final validation score : {}\".format(final_validation_score))\n",
    "\n",
    "        return sub,validation,final_validation_score,model_save\n",
    "\n",
    "    def lgbm_model(X_train,X_val,X_test,params,other_params):\n",
    "        dtrain = lgbm.Dataset(data=X_train[features], label=X_train[target_name], feature_name=features)\n",
    "        dval = lgbm.Dataset(data=X_val[features], label=X_val[target_name], feature_name=features)\n",
    "\n",
    "        model = lgbm.train(\n",
    "            params=params,\n",
    "            train_set=dtrain,\n",
    "            num_boost_round=other_params[\"num_boost_round\"],\n",
    "            valid_sets=(dtrain, dval),\n",
    "            early_stopping_rounds=other_params[\"early_stopping_rounds\"],\n",
    "            verbose_eval=other_params[\"verbose_eval\"],\n",
    "        )        \n",
    "        best_iteration = model.best_iteration\n",
    "        train_pred=model.predict(X_train[features], num_iteration=best_iteration)\n",
    "        validation_pred=model.predict(X_val[features], num_iteration=best_iteration)\n",
    "        test_pred=model.predict(test[features], num_iteration=best_iteration)\n",
    "\n",
    "        return train_pred,validation_pred,test_pred,model\n",
    "    other_params={\"num_boost_round\":1000000,\n",
    "              \"early_stopping_rounds\":2000,\n",
    "              \"verbose_eval\":1000,\n",
    "    }\n",
    "    lgbm_params = {\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 2,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"feature_fraction\": 0.6,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"max_depth\": 14,\n",
    "        \"num_threads\": 2,\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"seed\": 2020,\n",
    "        \"lambda_l1\" : 15,\n",
    "        \"lambda_l2\" : 4, # this was 5\n",
    "    }\n",
    "    sub,validation,score,model=train_function(model=lgbm_model,\n",
    "                                        train=train,\n",
    "                                        test=test,\n",
    "                                        params=lgbm_params,\n",
    "                                        other_params=other_params,\n",
    "                                        target_name=target_name,\n",
    "                                        features=features,\n",
    "                                        metric=metric)\n",
    "    return sub, validation, score, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now we create the second model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_two(data):\n",
    "    '''in this model I imputed the the data first using KNN'''\n",
    "    features=[\"temp\",\"precip\",\"rel_humidity\",\"wind_dir\",\"wind_spd\",\"atmos_press\"]\n",
    "\n",
    "    def custom_imputer(data,group,feature,n_neighbors):\n",
    "        tempA = pd.DataFrame(item for item in data.groupby('location').get_group(group)[feature])\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        tempA = imputer.fit_transform(tempA)\n",
    "        tempA_mod =pd.Series(list(item) for item in tempA)\n",
    "        tempA_mod.index = data.groupby('location').get_group(group)[feature].index\n",
    "        data.loc[data.location==group, feature] = tempA_mod\n",
    "        return data\n",
    "    \n",
    "    locations = list(data.location.unique())\n",
    "    for loca in tqdm(locations) :\n",
    "        for feature in features:\n",
    "            data = custom_imputer(data,loca,feature,5)\n",
    "    '''feature engineering part'''\n",
    "    for col_name in tqdm(features):\n",
    "        for j in range(len(data)): \n",
    "            try :\n",
    "                data[\"hours_since_min_\"+col_name] =  (24 - np.nanargmin(data[col_name][j][96 : 121])) % 24 \n",
    "            except ValueError as e:\n",
    "                data[\"hours_since_min_\"+col_name] = np.nan\n",
    "            try :\n",
    "                data[\"hours_since_max_\"+col_name] =  (24 - np.nanargmax(data[col_name][j][96 : 121]) ) % 24 \n",
    "            except ValueError as e:\n",
    "                data[\"hours_since_max_\"+col_name] = np.nan\n",
    "    for col_name in tqdm(features):\n",
    "        for j in range(len(data)): \n",
    "            try :\n",
    "                data[\"hours_since_min_\"+col_name][j] =  (24 - np.nanargmin(data[col_name][j][96 : 121])) \n",
    "            except ValueError as e:\n",
    "                data[\"hours_since_min_\"+col_name][j] = np.nan\n",
    "            try :\n",
    "                data[\"hours_since_max_\"+col_name][j] =  (24 - np.nanargmax(data[col_name][j][96 : 121]) ) \n",
    "            except ValueError as e:\n",
    "                data[\"hours_since_max_\"+col_name][j] = np.nan\n",
    "    def get_max(w,df_test):\n",
    "        tempA = pd.DataFrame(item for item in df_test[\"temp\"])\n",
    "        w=w*24\n",
    "        x=tempA.iloc[:,w:w+24+1].idxmax(axis=1)\n",
    "        return x \n",
    "    def get_min(w,df_test):\n",
    "        tempA = pd.DataFrame(item for item in df_test[\"temp\"])\n",
    "        w=w*24\n",
    "        x=tempA.iloc[:,w:w+24+1].idxmin(axis=1)\n",
    "        return x \n",
    "\n",
    "\n",
    "    def feature_aug_max(w,feature,df_test):\n",
    "        tempB = pd.DataFrame(item for item in df_test[feature])\n",
    "        df_test[feature+\"_at_max_temp_day_\"+str(w)]  = tempB.lookup(get_max(w,df_test).index,\n",
    "                                                                    get_max(w,df_test).values) \n",
    "        return df_test\n",
    "    def feature_aug_min(w,feature,df_test):\n",
    "        tempB = pd.DataFrame(item for item in df_test[feature])\n",
    "        df_test[feature+\"_at_min_temp_day_\"+str(w)] = tempB.lookup(get_min(w,df_test).index,\n",
    "                                                                      get_min(w,df_test).values)\n",
    "\n",
    "        return df_test\n",
    "    \n",
    "    temp_cols = [\"temp\"+ str(x) for x in range(121)] \n",
    "    precip_cols =[\"precip\"+ str(x) for x in range(121)]\n",
    "    rel_humidity_cols = [\"rel_humidity\"+ str(x) for x in range(121)] \n",
    "    wind_dir_cols = [\"wind_dir\"+ str(x) for x in range(121)] \n",
    "    wind_spd_cols = [\"wind_spd\"+ str(x) for x in range(121)] \n",
    "    atmos_press_cols = [\"atmos_press\"+ str(x) for x in range(121)]\n",
    "    def stats_hourwise2(x,col_name,start):\n",
    "        tempA = pd.DataFrame(item for item in x[col_name])\n",
    "        y=tempA.iloc[:,(start%24):121:24]\n",
    "        x[col_name+str(start)]=y.apply(np.mean,axis=1)\n",
    "        return x \n",
    "    for col_name in tqdm(features):\n",
    "        for start in range(97,121):\n",
    "            data = stats_hourwise2(data,col_name,start)\n",
    "    for feature in tqdm(features):\n",
    "        for w in range(4,5):\n",
    "            data=feature_aug_min(w,feature,data)\n",
    "    for feature in tqdm(features):\n",
    "        for w in range(4,5):\n",
    "            data=feature_aug_max(w,feature,data)\n",
    "    def aggregate_features(x,col_name):\n",
    "        x[\"max_\"+col_name]=x[col_name].apply(np.max)\n",
    "        x[\"min_\"+col_name]=x[col_name].apply(np.min)\n",
    "        x[\"mean_\"+col_name]=x[col_name].apply(np.mean)\n",
    "        x[\"std_\"+col_name]=x[col_name].apply(np.std)\n",
    "        x[\"var_\"+col_name]=x[col_name].apply(np.var)\n",
    "        x[\"median_\"+col_name]=x[col_name].apply(np.median)\n",
    "        x[\"ptp_\"+col_name]=x[col_name].apply(np.ptp)\n",
    "        return x\n",
    "    for col_name in tqdm(features):\n",
    "        data=aggregate_features(data,col_name)\n",
    "    keep = ['ID','location','target','hours_since_min_temp','hours_since_max_temp',\n",
    "     'hours_since_min_precip','hours_since_max_precip','hours_since_min_rel_humidity',\n",
    "     'hours_since_max_rel_humidity','hours_since_min_wind_dir','hours_since_max_wind_dir',\n",
    "     'hours_since_min_wind_spd','hours_since_max_wind_spd','hours_since_min_atmos_press',\n",
    "     'hours_since_max_atmos_press','temp117','precip117','rel_humidity117','wind_dir117',\n",
    "     'wind_spd117','atmos_press117','temp118','precip118','rel_humidity118','wind_dir118',\n",
    "     'wind_spd118','atmos_press118','temp119','precip119','rel_humidity119','wind_dir119',\n",
    "     'wind_spd119','atmos_press119','temp120','precip120','rel_humidity120','wind_dir120',\n",
    "     'wind_spd120','atmos_press120','precip106','rel_humidity106','wind_dir106',\n",
    "     'wind_spd106','atmos_press106','wind_spd107','atmos_press107','temp_at_max_temp_day_4','precip_at_max_temp_day_4',\n",
    "     'rel_humidity_at_max_temp_day_4','wind_dir_at_max_temp_day_4','wind_spd_at_max_temp_day_4','atmos_press_at_max_temp_day_4',\n",
    "     'max_temp','min_temp','mean_temp','std_temp','var_temp','median_temp','ptp_temp','max_precip','min_precip','mean_precip',\n",
    "     'std_precip','var_precip','median_precip','ptp_precip',\n",
    "     'max_rel_humidity','min_rel_humidity','mean_rel_humidity','std_rel_humidity','var_rel_humidity','median_rel_humidity',\n",
    "    'ptp_rel_humidity','max_wind_dir','min_wind_dir','mean_wind_dir','std_wind_dir','var_wind_dir','median_wind_dir',\n",
    "    'ptp_wind_dir','max_wind_spd','min_wind_spd','mean_wind_spd','std_wind_spd','var_wind_spd','median_wind_spd','ptp_wind_spd',\n",
    "    'max_atmos_press','min_atmos_press','mean_atmos_press','std_atmos_press','var_atmos_press','median_atmos_press','ptp_atmos_press']\n",
    "\n",
    "    data = data[keep] \n",
    "    data['log_target'] = np.log(data.target)\n",
    "    data['location0'] = np.nan\n",
    "    data['location1'] = np.nan\n",
    "    data['location2'] = np.nan\n",
    "\n",
    "    def adding_more_features(loca,df):\n",
    "        tup = t.fit(df.log_target[df.log_target.where(df.location == loca ).notnull()])\n",
    "        df.loc[df['location'] == loca, ['location0']] = tup[0]\n",
    "        df.loc[df['location'] == loca, ['location1']] = tup[1]\n",
    "        df.loc[df['location'] == loca, ['location2']] = tup[2]\n",
    "        return df\n",
    "    def adding_stats(df):\n",
    "        df['target_tmode']  = np.exp(df['location1'])\n",
    "        df['target_tmean']  = np.exp(df['location1'] + (df['location2'] ) / 2 )\n",
    "        df['target_tstd']  =  np.sqrt(np.exp(df['location1']*2 + df['location2'] )*np.exp((df['location2'] )- 1))\n",
    "        return df\n",
    "\n",
    "    for loca in tqdm(data.location.unique()):\n",
    "        data = adding_more_features(loca,data)\n",
    "    data = adding_stats(data)\n",
    "    cat_encoder = LabelEncoder()\n",
    "    data[\"location\"] = cat_encoder.fit_transform(data.location)\n",
    "    train=data[data.target.notnull()].reset_index(drop=True)\n",
    "    test=data[data.target.isna()].reset_index(drop=True)\n",
    "    del data  \n",
    "    gc.collect()\n",
    "    Experiment_name = 'model0'\n",
    "    os.makedirs(\"proc_data\", exist_ok=True)\n",
    "    try : \n",
    "        folds=pd.read_csv(\"./proc_data/folds_id.csv\")\n",
    "        train=train.merge(folds,on=\"ID\",how=\"left\")\n",
    "        train.fold.nunique()\n",
    "    except : \n",
    "        #  you run this cell  only for the first time \n",
    "        from sklearn.model_selection import KFold \n",
    "        kfold=KFold(n_splits=5,shuffle=True,random_state=2020) # change this random_state or all of you will have the same score  :D \n",
    "        train.reset_index(drop=True,inplace=True)\n",
    "        folds=train[[\"ID\"]].copy()\n",
    "        folds[\"fold\"]=0\n",
    "        for fold,(tr_indx,val_ind) in enumerate(kfold.split(folds)) : \n",
    "            folds.loc[val_ind,\"fold\"]=fold\n",
    "        folds.to_csv(\"./proc_data/folds_id.csv\",index=False)\n",
    "        train=train.merge(folds,on=\"ID\",how=\"left\")\n",
    "\n",
    "        del folds\n",
    "    target_name=\"log_target\"\n",
    "    id_name=\"ID\"\n",
    "    features_to_remove=[target_name,id_name,\"fold\",\"target\"]\n",
    "    features=train.columns.tolist()\n",
    "    features=[ fea for fea in  features if fea not in features_to_remove  ]\n",
    "    scaler = StandardScaler()\n",
    "    unscaled_train = train\n",
    "    train[features] = scaler.fit_transform(unscaled_train[features])\n",
    "    test_unscaled=test\n",
    "    test[features] = scaler.transform(test_unscaled[features])\n",
    "    def metric(y,x):\n",
    "        return np.sqrt(mean_squared_error(x,y))\n",
    "    def train_function(model,train,test,params,other_params,target_name,features,metric):\n",
    "        folds_num=train.fold.nunique()\n",
    "        validation=train[[id_name,\"fold\",target_name]].copy()\n",
    "        validation[\"pred_\"+target_name]=0\n",
    "        sub=test[[id_name]].copy()\n",
    "        sub[target_name]=0\n",
    "        for fold in np.sort(train.fold.unique()):\n",
    "            print(\"#\"*50+\" {} \".format(fold)+\"#\"*50)\n",
    "            os.makedirs(\"model_save/lgbm/{}/{}\".format(Experiment_name,str(int(fold))), exist_ok=True)\n",
    "            X_train=train[train.fold!=fold]\n",
    "            X_val=train[train.fold==fold]\n",
    "\n",
    "            train_pred,validation_pred,test_pred,model_save=model(X_train,X_val,test,params,other_params)\n",
    "\n",
    "            validation.loc[validation.fold==fold,\"pred_\"+target_name]=validation_pred\n",
    "            sub[target_name]+=test_pred/folds_num\n",
    "            train_score=metric(X_train[target_name],train_pred)\n",
    "            val_score=metric(X_val[target_name],validation_pred)\n",
    "            print(\"train score : {} validation score : {}\".format(round(train_score,4),round(val_score,4)))\n",
    "        final_validation_score=metric(validation[target_name],validation[\"pred_\"+target_name])\n",
    "        print(\"final validation score : {}\".format(final_validation_score))\n",
    "\n",
    "        return sub,validation,final_validation_score,model_save\n",
    "\n",
    "    def lgbm_model(X_train,X_val,X_test,params,other_params):\n",
    "        dtrain = lgbm.Dataset(data=X_train[features], label=X_train[target_name], feature_name=features)\n",
    "        dval = lgbm.Dataset(data=X_val[features], label=X_val[target_name], feature_name=features)\n",
    "\n",
    "        model = lgbm.train(\n",
    "            params=params,\n",
    "            train_set=dtrain,\n",
    "            num_boost_round=other_params[\"num_boost_round\"],\n",
    "            valid_sets=(dtrain, dval),\n",
    "            early_stopping_rounds=other_params[\"early_stopping_rounds\"],\n",
    "            verbose_eval=other_params[\"verbose_eval\"],\n",
    "        )        \n",
    "        best_iteration = model.best_iteration\n",
    "        train_pred=model.predict(X_train[features], num_iteration=best_iteration)\n",
    "        validation_pred=model.predict(X_val[features], num_iteration=best_iteration)\n",
    "        test_pred=model.predict(test[features], num_iteration=best_iteration)\n",
    "\n",
    "        return train_pred,validation_pred,test_pred,model\n",
    "    other_params={\"num_boost_round\":1000000,\n",
    "              \"early_stopping_rounds\":2000,\n",
    "              \"verbose_eval\":1000,\n",
    "    }\n",
    "    lgbm_params = {\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 2,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"feature_fraction\": 0.6,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"max_depth\": 14,\n",
    "        \"num_threads\": 4,\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"seed\": 2020,\n",
    "        \"lambda_l1\" : 0.9\n",
    "\n",
    "    }\n",
    "    sub,validation,score,model=train_function(model=lgbm_model,\n",
    "                                    train=train,\n",
    "                                    test=test,\n",
    "                                    params=lgbm_params,\n",
    "                                    other_params=other_params,\n",
    "                                    target_name=target_name,\n",
    "                                    features=features,\n",
    "                                    metric=metric)\n",
    "    return sub , validation , score , model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [06:11<00:00, 61.94s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:10<00:00, 11.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:46<00:00,  7.77s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:24<00:00,  4.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 0 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 18.415\tvalid_1's rmse: 25.7659\n",
      "[2000]\ttraining's rmse: 14.5198\tvalid_1's rmse: 24.7391\n",
      "[3000]\ttraining's rmse: 12.0125\tvalid_1's rmse: 24.3205\n",
      "[4000]\ttraining's rmse: 10.1912\tvalid_1's rmse: 24.081\n",
      "[5000]\ttraining's rmse: 8.76213\tvalid_1's rmse: 23.9259\n",
      "[6000]\ttraining's rmse: 7.61514\tvalid_1's rmse: 23.8312\n",
      "[7000]\ttraining's rmse: 6.6784\tvalid_1's rmse: 23.7498\n",
      "[8000]\ttraining's rmse: 5.9007\tvalid_1's rmse: 23.697\n",
      "[9000]\ttraining's rmse: 5.2425\tvalid_1's rmse: 23.6649\n",
      "[10000]\ttraining's rmse: 4.68184\tvalid_1's rmse: 23.6401\n",
      "[11000]\ttraining's rmse: 4.20377\tvalid_1's rmse: 23.6205\n",
      "[12000]\ttraining's rmse: 3.78718\tvalid_1's rmse: 23.604\n",
      "[13000]\ttraining's rmse: 3.42634\tvalid_1's rmse: 23.5884\n",
      "[14000]\ttraining's rmse: 3.11334\tvalid_1's rmse: 23.5797\n",
      "[15000]\ttraining's rmse: 2.84029\tvalid_1's rmse: 23.568\n",
      "[16000]\ttraining's rmse: 2.59998\tvalid_1's rmse: 23.5589\n",
      "[17000]\ttraining's rmse: 2.39017\tvalid_1's rmse: 23.5564\n",
      "[18000]\ttraining's rmse: 2.20586\tvalid_1's rmse: 23.5526\n",
      "[19000]\ttraining's rmse: 2.0429\tvalid_1's rmse: 23.5465\n",
      "[20000]\ttraining's rmse: 1.89853\tvalid_1's rmse: 23.543\n",
      "[21000]\ttraining's rmse: 1.76852\tvalid_1's rmse: 23.5388\n",
      "[22000]\ttraining's rmse: 1.65329\tvalid_1's rmse: 23.5368\n",
      "[23000]\ttraining's rmse: 1.54944\tvalid_1's rmse: 23.5334\n",
      "[24000]\ttraining's rmse: 1.45599\tvalid_1's rmse: 23.5302\n",
      "[25000]\ttraining's rmse: 1.37091\tvalid_1's rmse: 23.5296\n",
      "[26000]\ttraining's rmse: 1.2941\tvalid_1's rmse: 23.5266\n",
      "[27000]\ttraining's rmse: 1.22535\tvalid_1's rmse: 23.5252\n",
      "[28000]\ttraining's rmse: 1.16158\tvalid_1's rmse: 23.5226\n",
      "[29000]\ttraining's rmse: 1.10457\tvalid_1's rmse: 23.5225\n",
      "[30000]\ttraining's rmse: 1.05187\tvalid_1's rmse: 23.5211\n",
      "[31000]\ttraining's rmse: 1.00309\tvalid_1's rmse: 23.5189\n",
      "[32000]\ttraining's rmse: 0.958758\tvalid_1's rmse: 23.5187\n",
      "[33000]\ttraining's rmse: 0.917776\tvalid_1's rmse: 23.5181\n",
      "[34000]\ttraining's rmse: 0.880059\tvalid_1's rmse: 23.5169\n",
      "[35000]\ttraining's rmse: 0.844817\tvalid_1's rmse: 23.516\n",
      "[36000]\ttraining's rmse: 0.812375\tvalid_1's rmse: 23.5157\n",
      "[37000]\ttraining's rmse: 0.782324\tvalid_1's rmse: 23.5151\n",
      "[38000]\ttraining's rmse: 0.754005\tvalid_1's rmse: 23.5141\n",
      "[39000]\ttraining's rmse: 0.727907\tvalid_1's rmse: 23.5138\n",
      "[40000]\ttraining's rmse: 0.703337\tvalid_1's rmse: 23.5125\n",
      "[41000]\ttraining's rmse: 0.680128\tvalid_1's rmse: 23.5119\n",
      "[42000]\ttraining's rmse: 0.658649\tvalid_1's rmse: 23.5117\n",
      "[43000]\ttraining's rmse: 0.638478\tvalid_1's rmse: 23.511\n",
      "[44000]\ttraining's rmse: 0.619575\tvalid_1's rmse: 23.5106\n",
      "[45000]\ttraining's rmse: 0.60176\tvalid_1's rmse: 23.5105\n",
      "[46000]\ttraining's rmse: 0.584926\tvalid_1's rmse: 23.5097\n",
      "[47000]\ttraining's rmse: 0.568977\tvalid_1's rmse: 23.5095\n",
      "[48000]\ttraining's rmse: 0.554014\tvalid_1's rmse: 23.5092\n",
      "[49000]\ttraining's rmse: 0.539885\tvalid_1's rmse: 23.5091\n",
      "[50000]\ttraining's rmse: 0.52647\tvalid_1's rmse: 23.5088\n",
      "[51000]\ttraining's rmse: 0.513612\tvalid_1's rmse: 23.5085\n",
      "[52000]\ttraining's rmse: 0.501717\tvalid_1's rmse: 23.5084\n",
      "[53000]\ttraining's rmse: 0.490227\tvalid_1's rmse: 23.5083\n",
      "[54000]\ttraining's rmse: 0.479285\tvalid_1's rmse: 23.5079\n",
      "[55000]\ttraining's rmse: 0.468986\tvalid_1's rmse: 23.5075\n",
      "[56000]\ttraining's rmse: 0.459219\tvalid_1's rmse: 23.5073\n",
      "[57000]\ttraining's rmse: 0.449868\tvalid_1's rmse: 23.5068\n",
      "[58000]\ttraining's rmse: 0.440878\tvalid_1's rmse: 23.5068\n",
      "[59000]\ttraining's rmse: 0.43241\tvalid_1's rmse: 23.5065\n",
      "[60000]\ttraining's rmse: 0.424374\tvalid_1's rmse: 23.5062\n",
      "[61000]\ttraining's rmse: 0.416737\tvalid_1's rmse: 23.5061\n",
      "[62000]\ttraining's rmse: 0.409496\tvalid_1's rmse: 23.5059\n",
      "[63000]\ttraining's rmse: 0.402631\tvalid_1's rmse: 23.5059\n",
      "[64000]\ttraining's rmse: 0.396248\tvalid_1's rmse: 23.5057\n",
      "[65000]\ttraining's rmse: 0.390075\tvalid_1's rmse: 23.5054\n",
      "[66000]\ttraining's rmse: 0.384124\tvalid_1's rmse: 23.5052\n",
      "[67000]\ttraining's rmse: 0.378291\tvalid_1's rmse: 23.505\n",
      "[68000]\ttraining's rmse: 0.373051\tvalid_1's rmse: 23.5048\n",
      "[69000]\ttraining's rmse: 0.367811\tvalid_1's rmse: 23.5049\n",
      "[70000]\ttraining's rmse: 0.362818\tvalid_1's rmse: 23.5047\n",
      "[71000]\ttraining's rmse: 0.358171\tvalid_1's rmse: 23.5047\n",
      "[72000]\ttraining's rmse: 0.353572\tvalid_1's rmse: 23.5048\n",
      "Early stopping, best iteration is:\n",
      "[70298]\ttraining's rmse: 0.36144\tvalid_1's rmse: 23.5047\n",
      "train score : 0.3614 validation score : 23.5047\n",
      "################################################## 1 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 18.545\tvalid_1's rmse: 25.0733\n",
      "[2000]\ttraining's rmse: 14.701\tvalid_1's rmse: 24.2127\n",
      "[3000]\ttraining's rmse: 12.1864\tvalid_1's rmse: 23.8346\n",
      "[4000]\ttraining's rmse: 10.3373\tvalid_1's rmse: 23.6318\n",
      "[5000]\ttraining's rmse: 8.88473\tvalid_1's rmse: 23.5098\n",
      "[6000]\ttraining's rmse: 7.73402\tvalid_1's rmse: 23.4159\n",
      "[7000]\ttraining's rmse: 6.78135\tvalid_1's rmse: 23.363\n",
      "[8000]\ttraining's rmse: 5.9869\tvalid_1's rmse: 23.3302\n",
      "[9000]\ttraining's rmse: 5.31248\tvalid_1's rmse: 23.3052\n",
      "[10000]\ttraining's rmse: 4.74232\tvalid_1's rmse: 23.2831\n",
      "[11000]\ttraining's rmse: 4.25087\tvalid_1's rmse: 23.2718\n",
      "[12000]\ttraining's rmse: 3.82901\tvalid_1's rmse: 23.2581\n",
      "[13000]\ttraining's rmse: 3.46225\tvalid_1's rmse: 23.2495\n",
      "[14000]\ttraining's rmse: 3.14383\tvalid_1's rmse: 23.2406\n",
      "[15000]\ttraining's rmse: 2.86639\tvalid_1's rmse: 23.238\n",
      "[16000]\ttraining's rmse: 2.62265\tvalid_1's rmse: 23.2391\n",
      "[17000]\ttraining's rmse: 2.40822\tvalid_1's rmse: 23.2346\n",
      "[18000]\ttraining's rmse: 2.22032\tvalid_1's rmse: 23.2341\n",
      "[19000]\ttraining's rmse: 2.05344\tvalid_1's rmse: 23.2335\n",
      "[20000]\ttraining's rmse: 1.90467\tvalid_1's rmse: 23.2316\n",
      "[21000]\ttraining's rmse: 1.77292\tvalid_1's rmse: 23.2311\n",
      "[22000]\ttraining's rmse: 1.65648\tvalid_1's rmse: 23.2286\n",
      "[23000]\ttraining's rmse: 1.55081\tvalid_1's rmse: 23.2273\n",
      "[24000]\ttraining's rmse: 1.4565\tvalid_1's rmse: 23.2268\n",
      "[25000]\ttraining's rmse: 1.37198\tvalid_1's rmse: 23.2264\n",
      "[26000]\ttraining's rmse: 1.29535\tvalid_1's rmse: 23.226\n",
      "[27000]\ttraining's rmse: 1.22545\tvalid_1's rmse: 23.2249\n",
      "[28000]\ttraining's rmse: 1.16132\tvalid_1's rmse: 23.2234\n",
      "[29000]\ttraining's rmse: 1.10351\tvalid_1's rmse: 23.2231\n",
      "[30000]\ttraining's rmse: 1.05128\tvalid_1's rmse: 23.2226\n",
      "[31000]\ttraining's rmse: 1.00265\tvalid_1's rmse: 23.2223\n",
      "[32000]\ttraining's rmse: 0.958137\tvalid_1's rmse: 23.2215\n",
      "[33000]\ttraining's rmse: 0.917669\tvalid_1's rmse: 23.2208\n",
      "[34000]\ttraining's rmse: 0.879691\tvalid_1's rmse: 23.2207\n",
      "[35000]\ttraining's rmse: 0.844745\tvalid_1's rmse: 23.2203\n",
      "[36000]\ttraining's rmse: 0.812285\tvalid_1's rmse: 23.2196\n",
      "[37000]\ttraining's rmse: 0.782129\tvalid_1's rmse: 23.2192\n",
      "[38000]\ttraining's rmse: 0.754475\tvalid_1's rmse: 23.2192\n",
      "[39000]\ttraining's rmse: 0.728404\tvalid_1's rmse: 23.2193\n",
      "Early stopping, best iteration is:\n",
      "[37496]\ttraining's rmse: 0.768131\tvalid_1's rmse: 23.2189\n",
      "train score : 0.7681 validation score : 23.2189\n",
      "################################################## 2 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 18.4273\tvalid_1's rmse: 25.7553\n",
      "[2000]\ttraining's rmse: 14.6078\tvalid_1's rmse: 24.8436\n",
      "[3000]\ttraining's rmse: 12.1089\tvalid_1's rmse: 24.4508\n",
      "[4000]\ttraining's rmse: 10.2755\tvalid_1's rmse: 24.2454\n",
      "[5000]\ttraining's rmse: 8.83955\tvalid_1's rmse: 24.1214\n",
      "[6000]\ttraining's rmse: 7.69179\tvalid_1's rmse: 24.0347\n",
      "[7000]\ttraining's rmse: 6.73947\tvalid_1's rmse: 23.9682\n",
      "[8000]\ttraining's rmse: 5.94685\tvalid_1's rmse: 23.9178\n",
      "[9000]\ttraining's rmse: 5.28668\tvalid_1's rmse: 23.8839\n",
      "[10000]\ttraining's rmse: 4.72932\tvalid_1's rmse: 23.8701\n",
      "[11000]\ttraining's rmse: 4.24263\tvalid_1's rmse: 23.8554\n",
      "[12000]\ttraining's rmse: 3.8255\tvalid_1's rmse: 23.839\n",
      "[13000]\ttraining's rmse: 3.4619\tvalid_1's rmse: 23.8252\n",
      "[14000]\ttraining's rmse: 3.14626\tvalid_1's rmse: 23.8166\n",
      "[15000]\ttraining's rmse: 2.86941\tvalid_1's rmse: 23.8132\n",
      "[16000]\ttraining's rmse: 2.62798\tvalid_1's rmse: 23.8058\n",
      "[17000]\ttraining's rmse: 2.41343\tvalid_1's rmse: 23.8066\n",
      "[18000]\ttraining's rmse: 2.22535\tvalid_1's rmse: 23.8034\n",
      "[19000]\ttraining's rmse: 2.05681\tvalid_1's rmse: 23.8008\n",
      "[20000]\ttraining's rmse: 1.90703\tvalid_1's rmse: 23.8028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21000]\ttraining's rmse: 1.77354\tvalid_1's rmse: 23.8002\n",
      "[22000]\ttraining's rmse: 1.65506\tvalid_1's rmse: 23.8\n",
      "[23000]\ttraining's rmse: 1.54892\tvalid_1's rmse: 23.7978\n",
      "[24000]\ttraining's rmse: 1.45346\tvalid_1's rmse: 23.7972\n",
      "[25000]\ttraining's rmse: 1.36749\tvalid_1's rmse: 23.7973\n",
      "[26000]\ttraining's rmse: 1.28977\tvalid_1's rmse: 23.7981\n",
      "Early stopping, best iteration is:\n",
      "[24853]\ttraining's rmse: 1.37955\tvalid_1's rmse: 23.7966\n",
      "train score : 1.3795 validation score : 23.7966\n",
      "################################################## 3 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 18.7799\tvalid_1's rmse: 23.6304\n",
      "[2000]\ttraining's rmse: 14.8567\tvalid_1's rmse: 22.7549\n",
      "[3000]\ttraining's rmse: 12.312\tvalid_1's rmse: 22.3741\n",
      "[4000]\ttraining's rmse: 10.4214\tvalid_1's rmse: 22.1573\n",
      "[5000]\ttraining's rmse: 8.94479\tvalid_1's rmse: 22.0217\n",
      "[6000]\ttraining's rmse: 7.76567\tvalid_1's rmse: 21.9221\n",
      "[7000]\ttraining's rmse: 6.80195\tvalid_1's rmse: 21.8591\n",
      "[8000]\ttraining's rmse: 6.00017\tvalid_1's rmse: 21.81\n",
      "[9000]\ttraining's rmse: 5.32579\tvalid_1's rmse: 21.7825\n",
      "[10000]\ttraining's rmse: 4.75464\tvalid_1's rmse: 21.7583\n",
      "[11000]\ttraining's rmse: 4.26507\tvalid_1's rmse: 21.7372\n",
      "[12000]\ttraining's rmse: 3.84266\tvalid_1's rmse: 21.7213\n",
      "[13000]\ttraining's rmse: 3.47677\tvalid_1's rmse: 21.7054\n",
      "[14000]\ttraining's rmse: 3.15637\tvalid_1's rmse: 21.6982\n",
      "[15000]\ttraining's rmse: 2.87811\tvalid_1's rmse: 21.6859\n",
      "[16000]\ttraining's rmse: 2.63363\tvalid_1's rmse: 21.6758\n",
      "[17000]\ttraining's rmse: 2.4206\tvalid_1's rmse: 21.6711\n",
      "[18000]\ttraining's rmse: 2.23174\tvalid_1's rmse: 21.6652\n",
      "[19000]\ttraining's rmse: 2.0628\tvalid_1's rmse: 21.6603\n",
      "[20000]\ttraining's rmse: 1.91418\tvalid_1's rmse: 21.6578\n",
      "[21000]\ttraining's rmse: 1.78115\tvalid_1's rmse: 21.6562\n",
      "[22000]\ttraining's rmse: 1.6636\tvalid_1's rmse: 21.6536\n",
      "[23000]\ttraining's rmse: 1.55733\tvalid_1's rmse: 21.6514\n",
      "[24000]\ttraining's rmse: 1.46284\tvalid_1's rmse: 21.6489\n",
      "[25000]\ttraining's rmse: 1.37672\tvalid_1's rmse: 21.6492\n",
      "[26000]\ttraining's rmse: 1.29841\tvalid_1's rmse: 21.6477\n",
      "[27000]\ttraining's rmse: 1.22802\tvalid_1's rmse: 21.6454\n",
      "[28000]\ttraining's rmse: 1.16393\tvalid_1's rmse: 21.6438\n",
      "[29000]\ttraining's rmse: 1.10573\tvalid_1's rmse: 21.643\n",
      "[30000]\ttraining's rmse: 1.05236\tvalid_1's rmse: 21.6413\n",
      "[31000]\ttraining's rmse: 1.00352\tvalid_1's rmse: 21.6393\n",
      "[32000]\ttraining's rmse: 0.958756\tvalid_1's rmse: 21.6381\n",
      "[33000]\ttraining's rmse: 0.917355\tvalid_1's rmse: 21.6369\n",
      "[34000]\ttraining's rmse: 0.879341\tvalid_1's rmse: 21.6358\n",
      "[35000]\ttraining's rmse: 0.844328\tvalid_1's rmse: 21.6348\n",
      "[36000]\ttraining's rmse: 0.812096\tvalid_1's rmse: 21.6343\n",
      "[37000]\ttraining's rmse: 0.782126\tvalid_1's rmse: 21.634\n",
      "[38000]\ttraining's rmse: 0.754447\tvalid_1's rmse: 21.634\n",
      "[39000]\ttraining's rmse: 0.728222\tvalid_1's rmse: 21.6335\n",
      "[40000]\ttraining's rmse: 0.70357\tvalid_1's rmse: 21.633\n",
      "[41000]\ttraining's rmse: 0.680593\tvalid_1's rmse: 21.6331\n",
      "[42000]\ttraining's rmse: 0.659005\tvalid_1's rmse: 21.6324\n",
      "[43000]\ttraining's rmse: 0.638686\tvalid_1's rmse: 21.632\n",
      "[44000]\ttraining's rmse: 0.619541\tvalid_1's rmse: 21.6319\n",
      "[45000]\ttraining's rmse: 0.601658\tvalid_1's rmse: 21.6315\n",
      "[46000]\ttraining's rmse: 0.584881\tvalid_1's rmse: 21.6316\n",
      "[47000]\ttraining's rmse: 0.568998\tvalid_1's rmse: 21.6315\n",
      "[48000]\ttraining's rmse: 0.553832\tvalid_1's rmse: 21.6311\n",
      "[49000]\ttraining's rmse: 0.539733\tvalid_1's rmse: 21.6306\n",
      "[50000]\ttraining's rmse: 0.526349\tvalid_1's rmse: 21.6304\n",
      "[51000]\ttraining's rmse: 0.513657\tvalid_1's rmse: 21.6305\n",
      "[52000]\ttraining's rmse: 0.501655\tvalid_1's rmse: 21.6301\n",
      "[53000]\ttraining's rmse: 0.490161\tvalid_1's rmse: 21.6299\n",
      "[54000]\ttraining's rmse: 0.479169\tvalid_1's rmse: 21.63\n",
      "[55000]\ttraining's rmse: 0.468842\tvalid_1's rmse: 21.6299\n",
      "[56000]\ttraining's rmse: 0.45897\tvalid_1's rmse: 21.6297\n",
      "[57000]\ttraining's rmse: 0.449742\tvalid_1's rmse: 21.6296\n",
      "[58000]\ttraining's rmse: 0.440923\tvalid_1's rmse: 21.6296\n",
      "[59000]\ttraining's rmse: 0.432554\tvalid_1's rmse: 21.6296\n",
      "[60000]\ttraining's rmse: 0.424511\tvalid_1's rmse: 21.6297\n",
      "Early stopping, best iteration is:\n",
      "[58109]\ttraining's rmse: 0.439992\tvalid_1's rmse: 21.6294\n",
      "train score : 0.44 validation score : 21.6294\n",
      "################################################## 4 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 18.7154\tvalid_1's rmse: 24.0301\n",
      "[2000]\ttraining's rmse: 14.7718\tvalid_1's rmse: 23.1389\n",
      "[3000]\ttraining's rmse: 12.2122\tvalid_1's rmse: 22.7627\n",
      "[4000]\ttraining's rmse: 10.3358\tvalid_1's rmse: 22.5887\n",
      "[5000]\ttraining's rmse: 8.87393\tvalid_1's rmse: 22.459\n",
      "[6000]\ttraining's rmse: 7.7025\tvalid_1's rmse: 22.3845\n",
      "[7000]\ttraining's rmse: 6.74896\tvalid_1's rmse: 22.3291\n",
      "[8000]\ttraining's rmse: 5.95427\tvalid_1's rmse: 22.3022\n",
      "[9000]\ttraining's rmse: 5.27797\tvalid_1's rmse: 22.2825\n",
      "[10000]\ttraining's rmse: 4.7089\tvalid_1's rmse: 22.2695\n",
      "[11000]\ttraining's rmse: 4.21931\tvalid_1's rmse: 22.2596\n",
      "[12000]\ttraining's rmse: 3.80087\tvalid_1's rmse: 22.2514\n",
      "[13000]\ttraining's rmse: 3.437\tvalid_1's rmse: 22.2489\n",
      "[14000]\ttraining's rmse: 3.1197\tvalid_1's rmse: 22.2448\n",
      "[15000]\ttraining's rmse: 2.84483\tvalid_1's rmse: 22.2447\n",
      "[16000]\ttraining's rmse: 2.60425\tvalid_1's rmse: 22.2447\n",
      "[17000]\ttraining's rmse: 2.39524\tvalid_1's rmse: 22.2452\n",
      "[18000]\ttraining's rmse: 2.20759\tvalid_1's rmse: 22.2443\n",
      "[19000]\ttraining's rmse: 2.04215\tvalid_1's rmse: 22.244\n",
      "[20000]\ttraining's rmse: 1.89556\tvalid_1's rmse: 22.2451\n",
      "[21000]\ttraining's rmse: 1.76533\tvalid_1's rmse: 22.2468\n",
      "Early stopping, best iteration is:\n",
      "[19049]\ttraining's rmse: 2.03445\tvalid_1's rmse: 22.2432\n",
      "train score : 2.0344 validation score : 22.2432\n",
      "final validation score : 22.89309746659016\n"
     ]
    }
   ],
   "source": [
    "sub1,val1,score1,model1 = generate_model_one(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"Train.csv\")\n",
    "test=pd.read_csv(\"Test.csv\")\n",
    "sample_sub=pd.read_csv(\"sample_sub.csv\")\n",
    "\n",
    "def replace_nan(x):\n",
    "    if x==\" \":\n",
    "        return np.nan\n",
    "    else :\n",
    "        return float(x)\n",
    "features=[\"temp\",\"precip\",\"rel_humidity\",\"wind_dir\",\"wind_spd\",\"atmos_press\"]\n",
    "for feature in features : \n",
    "    train[feature]=train[feature].apply(lambda x: [ replace_nan(X) for X in x.replace(\"nan\",\" \").split(\",\")])\n",
    "    test[feature]=test[feature].apply(lambda x: [ replace_nan(X)  for X in x.replace(\"nan\",\" \").split(\",\")])    \n",
    "def remove_nan_values(x):\n",
    "    return [e for e in x if not math.isnan(e)]\n",
    "data=pd.concat([train,test],sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.18s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:09<00:00, 11.53s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:43<00:00,  7.30s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:01<00:00, 70.28s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:09<00:00,  1.59s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:09<00:00,  1.57s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:26<00:00,  4.36s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 0 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 0.284673\tvalid_1's rmse: 0.353415\n",
      "[2000]\ttraining's rmse: 0.226861\tvalid_1's rmse: 0.332009\n",
      "[3000]\ttraining's rmse: 0.189913\tvalid_1's rmse: 0.323036\n",
      "[4000]\ttraining's rmse: 0.162745\tvalid_1's rmse: 0.317655\n",
      "[5000]\ttraining's rmse: 0.141536\tvalid_1's rmse: 0.314112\n",
      "[6000]\ttraining's rmse: 0.124601\tvalid_1's rmse: 0.311747\n",
      "[7000]\ttraining's rmse: 0.110895\tvalid_1's rmse: 0.310054\n",
      "[8000]\ttraining's rmse: 0.0996088\tvalid_1's rmse: 0.308745\n",
      "[9000]\ttraining's rmse: 0.0901269\tvalid_1's rmse: 0.307816\n",
      "[10000]\ttraining's rmse: 0.0820944\tvalid_1's rmse: 0.30704\n",
      "[11000]\ttraining's rmse: 0.0752778\tvalid_1's rmse: 0.30638\n",
      "[12000]\ttraining's rmse: 0.0694209\tvalid_1's rmse: 0.30592\n",
      "[13000]\ttraining's rmse: 0.0643715\tvalid_1's rmse: 0.305508\n",
      "[14000]\ttraining's rmse: 0.0599262\tvalid_1's rmse: 0.305204\n",
      "[15000]\ttraining's rmse: 0.0560467\tvalid_1's rmse: 0.304928\n",
      "[16000]\ttraining's rmse: 0.0526218\tvalid_1's rmse: 0.304679\n",
      "[17000]\ttraining's rmse: 0.0495514\tvalid_1's rmse: 0.304479\n",
      "[18000]\ttraining's rmse: 0.0468433\tvalid_1's rmse: 0.304294\n",
      "[19000]\ttraining's rmse: 0.0444086\tvalid_1's rmse: 0.304102\n",
      "[20000]\ttraining's rmse: 0.0422077\tvalid_1's rmse: 0.303927\n",
      "[21000]\ttraining's rmse: 0.0402137\tvalid_1's rmse: 0.303823\n",
      "[22000]\ttraining's rmse: 0.0383944\tvalid_1's rmse: 0.303688\n",
      "[23000]\ttraining's rmse: 0.0367346\tvalid_1's rmse: 0.3036\n",
      "[24000]\ttraining's rmse: 0.0352228\tvalid_1's rmse: 0.303499\n",
      "[25000]\ttraining's rmse: 0.0338252\tvalid_1's rmse: 0.303407\n",
      "[26000]\ttraining's rmse: 0.0325359\tvalid_1's rmse: 0.303322\n",
      "[27000]\ttraining's rmse: 0.0313564\tvalid_1's rmse: 0.303247\n",
      "[28000]\ttraining's rmse: 0.0302636\tvalid_1's rmse: 0.303171\n",
      "[29000]\ttraining's rmse: 0.0292602\tvalid_1's rmse: 0.303112\n",
      "[30000]\ttraining's rmse: 0.0283227\tvalid_1's rmse: 0.303065\n",
      "[31000]\ttraining's rmse: 0.0274394\tvalid_1's rmse: 0.303022\n",
      "[32000]\ttraining's rmse: 0.0266255\tvalid_1's rmse: 0.302985\n",
      "[33000]\ttraining's rmse: 0.0258749\tvalid_1's rmse: 0.302945\n",
      "[34000]\ttraining's rmse: 0.0251595\tvalid_1's rmse: 0.302891\n",
      "[35000]\ttraining's rmse: 0.0244938\tvalid_1's rmse: 0.302841\n",
      "[36000]\ttraining's rmse: 0.0238719\tvalid_1's rmse: 0.302802\n",
      "[37000]\ttraining's rmse: 0.0232922\tvalid_1's rmse: 0.302774\n",
      "[38000]\ttraining's rmse: 0.0227436\tvalid_1's rmse: 0.302745\n",
      "[39000]\ttraining's rmse: 0.0222331\tvalid_1's rmse: 0.302709\n",
      "[40000]\ttraining's rmse: 0.0217533\tvalid_1's rmse: 0.302686\n",
      "[41000]\ttraining's rmse: 0.0213025\tvalid_1's rmse: 0.302658\n",
      "[42000]\ttraining's rmse: 0.0208806\tvalid_1's rmse: 0.302639\n",
      "[43000]\ttraining's rmse: 0.0204847\tvalid_1's rmse: 0.302608\n",
      "[44000]\ttraining's rmse: 0.0201127\tvalid_1's rmse: 0.302593\n",
      "[45000]\ttraining's rmse: 0.0197698\tvalid_1's rmse: 0.302578\n",
      "[46000]\ttraining's rmse: 0.0194422\tvalid_1's rmse: 0.302564\n",
      "[47000]\ttraining's rmse: 0.0191365\tvalid_1's rmse: 0.302554\n",
      "[48000]\ttraining's rmse: 0.0188418\tvalid_1's rmse: 0.302527\n",
      "[49000]\ttraining's rmse: 0.0185717\tvalid_1's rmse: 0.302509\n",
      "[50000]\ttraining's rmse: 0.0183203\tvalid_1's rmse: 0.302493\n",
      "[51000]\ttraining's rmse: 0.0180739\tvalid_1's rmse: 0.302483\n",
      "[52000]\ttraining's rmse: 0.0178419\tvalid_1's rmse: 0.302469\n",
      "[53000]\ttraining's rmse: 0.0176275\tvalid_1's rmse: 0.302461\n",
      "[54000]\ttraining's rmse: 0.0174217\tvalid_1's rmse: 0.302456\n",
      "[55000]\ttraining's rmse: 0.0172277\tvalid_1's rmse: 0.302446\n",
      "[56000]\ttraining's rmse: 0.0170519\tvalid_1's rmse: 0.302439\n",
      "[57000]\ttraining's rmse: 0.0168807\tvalid_1's rmse: 0.30243\n",
      "[58000]\ttraining's rmse: 0.0167182\tvalid_1's rmse: 0.30242\n",
      "[59000]\ttraining's rmse: 0.0165602\tvalid_1's rmse: 0.30241\n",
      "[60000]\ttraining's rmse: 0.0164115\tvalid_1's rmse: 0.302405\n",
      "[61000]\ttraining's rmse: 0.0163272\tvalid_1's rmse: 0.3024\n",
      "[62000]\ttraining's rmse: 0.0163272\tvalid_1's rmse: 0.3024\n",
      "Early stopping, best iteration is:\n",
      "[60532]\ttraining's rmse: 0.0163315\tvalid_1's rmse: 0.302399\n",
      "train score : 0.0163 validation score : 0.3024\n",
      "################################################## 1 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 0.285839\tvalid_1's rmse: 0.345087\n",
      "[2000]\ttraining's rmse: 0.228665\tvalid_1's rmse: 0.324166\n",
      "[3000]\ttraining's rmse: 0.191433\tvalid_1's rmse: 0.314405\n",
      "[4000]\ttraining's rmse: 0.164066\tvalid_1's rmse: 0.308814\n",
      "[5000]\ttraining's rmse: 0.142828\tvalid_1's rmse: 0.305358\n",
      "[6000]\ttraining's rmse: 0.125728\tvalid_1's rmse: 0.302952\n",
      "[7000]\ttraining's rmse: 0.111832\tvalid_1's rmse: 0.301448\n",
      "[8000]\ttraining's rmse: 0.100381\tvalid_1's rmse: 0.300264\n",
      "[9000]\ttraining's rmse: 0.0908178\tvalid_1's rmse: 0.299199\n",
      "[10000]\ttraining's rmse: 0.0827525\tvalid_1's rmse: 0.298534\n",
      "[11000]\ttraining's rmse: 0.0758649\tvalid_1's rmse: 0.298025\n",
      "[12000]\ttraining's rmse: 0.0699521\tvalid_1's rmse: 0.29757\n",
      "[13000]\ttraining's rmse: 0.064821\tvalid_1's rmse: 0.297158\n",
      "[14000]\ttraining's rmse: 0.0603624\tvalid_1's rmse: 0.296862\n",
      "[15000]\ttraining's rmse: 0.0564263\tvalid_1's rmse: 0.296566\n",
      "[16000]\ttraining's rmse: 0.0529581\tvalid_1's rmse: 0.29638\n",
      "[17000]\ttraining's rmse: 0.0499036\tvalid_1's rmse: 0.296166\n",
      "[18000]\ttraining's rmse: 0.0471607\tvalid_1's rmse: 0.296031\n",
      "[19000]\ttraining's rmse: 0.0447162\tvalid_1's rmse: 0.295906\n",
      "[20000]\ttraining's rmse: 0.0425036\tvalid_1's rmse: 0.295797\n",
      "[21000]\ttraining's rmse: 0.0404985\tvalid_1's rmse: 0.295696\n",
      "[22000]\ttraining's rmse: 0.0386813\tvalid_1's rmse: 0.295572\n",
      "[23000]\ttraining's rmse: 0.0370152\tvalid_1's rmse: 0.295486\n",
      "[24000]\ttraining's rmse: 0.0354904\tvalid_1's rmse: 0.295406\n",
      "[25000]\ttraining's rmse: 0.0341095\tvalid_1's rmse: 0.295319\n",
      "[26000]\ttraining's rmse: 0.032811\tvalid_1's rmse: 0.295261\n",
      "[27000]\ttraining's rmse: 0.0316165\tvalid_1's rmse: 0.295199\n",
      "[28000]\ttraining's rmse: 0.0305198\tvalid_1's rmse: 0.295142\n",
      "[29000]\ttraining's rmse: 0.0294869\tvalid_1's rmse: 0.295094\n",
      "[30000]\ttraining's rmse: 0.0285409\tvalid_1's rmse: 0.295046\n",
      "[31000]\ttraining's rmse: 0.0276513\tvalid_1's rmse: 0.294985\n",
      "[32000]\ttraining's rmse: 0.0268219\tvalid_1's rmse: 0.294942\n",
      "[33000]\ttraining's rmse: 0.0260616\tvalid_1's rmse: 0.294907\n",
      "[34000]\ttraining's rmse: 0.0253376\tvalid_1's rmse: 0.294874\n",
      "[35000]\ttraining's rmse: 0.024677\tvalid_1's rmse: 0.294834\n",
      "[36000]\ttraining's rmse: 0.0240435\tvalid_1's rmse: 0.294803\n",
      "[37000]\ttraining's rmse: 0.0234482\tvalid_1's rmse: 0.294785\n",
      "[38000]\ttraining's rmse: 0.0228869\tvalid_1's rmse: 0.294771\n",
      "[39000]\ttraining's rmse: 0.0223705\tvalid_1's rmse: 0.294749\n",
      "[40000]\ttraining's rmse: 0.0218943\tvalid_1's rmse: 0.294729\n",
      "[41000]\ttraining's rmse: 0.0214403\tvalid_1's rmse: 0.294714\n",
      "[42000]\ttraining's rmse: 0.0210107\tvalid_1's rmse: 0.294689\n",
      "[43000]\ttraining's rmse: 0.0206047\tvalid_1's rmse: 0.294676\n",
      "[44000]\ttraining's rmse: 0.0202327\tvalid_1's rmse: 0.294665\n",
      "[45000]\ttraining's rmse: 0.0198789\tvalid_1's rmse: 0.294652\n",
      "[46000]\ttraining's rmse: 0.019549\tvalid_1's rmse: 0.294639\n",
      "[47000]\ttraining's rmse: 0.0192411\tvalid_1's rmse: 0.294616\n",
      "[48000]\ttraining's rmse: 0.0189524\tvalid_1's rmse: 0.294606\n",
      "[49000]\ttraining's rmse: 0.0186757\tvalid_1's rmse: 0.294589\n",
      "[50000]\ttraining's rmse: 0.0184139\tvalid_1's rmse: 0.294587\n",
      "[51000]\ttraining's rmse: 0.0181681\tvalid_1's rmse: 0.294579\n",
      "[52000]\ttraining's rmse: 0.0179334\tvalid_1's rmse: 0.294574\n",
      "[53000]\ttraining's rmse: 0.0177115\tvalid_1's rmse: 0.294565\n",
      "[54000]\ttraining's rmse: 0.0175047\tvalid_1's rmse: 0.294555\n",
      "[55000]\ttraining's rmse: 0.017302\tvalid_1's rmse: 0.294548\n",
      "[56000]\ttraining's rmse: 0.01712\tvalid_1's rmse: 0.294537\n",
      "[57000]\ttraining's rmse: 0.0169496\tvalid_1's rmse: 0.29453\n",
      "[58000]\ttraining's rmse: 0.01689\tvalid_1's rmse: 0.294526\n",
      "[59000]\ttraining's rmse: 0.01689\tvalid_1's rmse: 0.294526\n",
      "Early stopping, best iteration is:\n",
      "[57367]\ttraining's rmse: 0.0168913\tvalid_1's rmse: 0.294526\n",
      "train score : 0.0169 validation score : 0.2945\n",
      "################################################## 2 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 0.285334\tvalid_1's rmse: 0.344631\n",
      "[2000]\ttraining's rmse: 0.228464\tvalid_1's rmse: 0.324789\n",
      "[3000]\ttraining's rmse: 0.191571\tvalid_1's rmse: 0.315717\n",
      "[4000]\ttraining's rmse: 0.164176\tvalid_1's rmse: 0.310108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000]\ttraining's rmse: 0.142861\tvalid_1's rmse: 0.306607\n",
      "[6000]\ttraining's rmse: 0.125815\tvalid_1's rmse: 0.304156\n",
      "[7000]\ttraining's rmse: 0.11206\tvalid_1's rmse: 0.302326\n",
      "[8000]\ttraining's rmse: 0.100626\tvalid_1's rmse: 0.301029\n",
      "[9000]\ttraining's rmse: 0.0910761\tvalid_1's rmse: 0.299986\n",
      "[10000]\ttraining's rmse: 0.0829181\tvalid_1's rmse: 0.299328\n",
      "[11000]\ttraining's rmse: 0.0760211\tvalid_1's rmse: 0.298709\n",
      "[12000]\ttraining's rmse: 0.0700648\tvalid_1's rmse: 0.298247\n",
      "[13000]\ttraining's rmse: 0.0649208\tvalid_1's rmse: 0.297817\n",
      "[14000]\ttraining's rmse: 0.0604406\tvalid_1's rmse: 0.297489\n",
      "[15000]\ttraining's rmse: 0.0564527\tvalid_1's rmse: 0.297208\n",
      "[16000]\ttraining's rmse: 0.0529614\tvalid_1's rmse: 0.296963\n",
      "[17000]\ttraining's rmse: 0.04986\tvalid_1's rmse: 0.29674\n",
      "[18000]\ttraining's rmse: 0.047136\tvalid_1's rmse: 0.296558\n",
      "[19000]\ttraining's rmse: 0.0446624\tvalid_1's rmse: 0.296365\n",
      "[20000]\ttraining's rmse: 0.0424237\tvalid_1's rmse: 0.296285\n",
      "[21000]\ttraining's rmse: 0.0403892\tvalid_1's rmse: 0.296166\n",
      "[22000]\ttraining's rmse: 0.0385605\tvalid_1's rmse: 0.296084\n",
      "[23000]\ttraining's rmse: 0.0368699\tvalid_1's rmse: 0.295993\n",
      "[24000]\ttraining's rmse: 0.0353336\tvalid_1's rmse: 0.295914\n",
      "[25000]\ttraining's rmse: 0.0339234\tvalid_1's rmse: 0.295872\n",
      "[26000]\ttraining's rmse: 0.0326224\tvalid_1's rmse: 0.295813\n",
      "[27000]\ttraining's rmse: 0.0314147\tvalid_1's rmse: 0.295762\n",
      "[28000]\ttraining's rmse: 0.0303173\tvalid_1's rmse: 0.295706\n",
      "[29000]\ttraining's rmse: 0.0293002\tvalid_1's rmse: 0.295642\n",
      "[30000]\ttraining's rmse: 0.0283462\tvalid_1's rmse: 0.295588\n",
      "[31000]\ttraining's rmse: 0.0274567\tvalid_1's rmse: 0.295561\n",
      "[32000]\ttraining's rmse: 0.0266286\tvalid_1's rmse: 0.295524\n",
      "[33000]\ttraining's rmse: 0.0258466\tvalid_1's rmse: 0.295478\n",
      "[34000]\ttraining's rmse: 0.025122\tvalid_1's rmse: 0.29543\n",
      "[35000]\ttraining's rmse: 0.0244496\tvalid_1's rmse: 0.295404\n",
      "[36000]\ttraining's rmse: 0.0238238\tvalid_1's rmse: 0.295381\n",
      "[37000]\ttraining's rmse: 0.0232364\tvalid_1's rmse: 0.295371\n",
      "[38000]\ttraining's rmse: 0.0226884\tvalid_1's rmse: 0.295347\n",
      "[39000]\ttraining's rmse: 0.0221746\tvalid_1's rmse: 0.295316\n",
      "[40000]\ttraining's rmse: 0.0217017\tvalid_1's rmse: 0.295295\n",
      "[41000]\ttraining's rmse: 0.0212435\tvalid_1's rmse: 0.29528\n",
      "[42000]\ttraining's rmse: 0.0208234\tvalid_1's rmse: 0.295257\n",
      "[43000]\ttraining's rmse: 0.0204207\tvalid_1's rmse: 0.295245\n",
      "[44000]\ttraining's rmse: 0.0200531\tvalid_1's rmse: 0.29523\n",
      "[45000]\ttraining's rmse: 0.0197042\tvalid_1's rmse: 0.295214\n",
      "[46000]\ttraining's rmse: 0.0193696\tvalid_1's rmse: 0.295206\n",
      "[47000]\ttraining's rmse: 0.0190575\tvalid_1's rmse: 0.295196\n",
      "[48000]\ttraining's rmse: 0.0187687\tvalid_1's rmse: 0.295188\n",
      "[49000]\ttraining's rmse: 0.0184993\tvalid_1's rmse: 0.295179\n",
      "[50000]\ttraining's rmse: 0.0182459\tvalid_1's rmse: 0.295164\n",
      "[51000]\ttraining's rmse: 0.0180055\tvalid_1's rmse: 0.295161\n",
      "[52000]\ttraining's rmse: 0.0177814\tvalid_1's rmse: 0.295149\n",
      "[53000]\ttraining's rmse: 0.0175663\tvalid_1's rmse: 0.295146\n",
      "[54000]\ttraining's rmse: 0.0173532\tvalid_1's rmse: 0.29514\n",
      "[55000]\ttraining's rmse: 0.0172602\tvalid_1's rmse: 0.295143\n",
      "Early stopping, best iteration is:\n",
      "[53554]\ttraining's rmse: 0.0174465\tvalid_1's rmse: 0.295138\n",
      "train score : 0.0174 validation score : 0.2951\n",
      "################################################## 3 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 0.285425\tvalid_1's rmse: 0.355994\n",
      "[2000]\ttraining's rmse: 0.22812\tvalid_1's rmse: 0.334219\n",
      "[3000]\ttraining's rmse: 0.190661\tvalid_1's rmse: 0.324306\n",
      "[4000]\ttraining's rmse: 0.163122\tvalid_1's rmse: 0.318629\n",
      "[5000]\ttraining's rmse: 0.141963\tvalid_1's rmse: 0.314925\n",
      "[6000]\ttraining's rmse: 0.124947\tvalid_1's rmse: 0.312425\n",
      "[7000]\ttraining's rmse: 0.111198\tvalid_1's rmse: 0.310428\n",
      "[8000]\ttraining's rmse: 0.099838\tvalid_1's rmse: 0.309148\n",
      "[9000]\ttraining's rmse: 0.0902531\tvalid_1's rmse: 0.308104\n",
      "[10000]\ttraining's rmse: 0.0822389\tvalid_1's rmse: 0.30733\n",
      "[11000]\ttraining's rmse: 0.075351\tvalid_1's rmse: 0.306636\n",
      "[12000]\ttraining's rmse: 0.0694356\tvalid_1's rmse: 0.306023\n",
      "[13000]\ttraining's rmse: 0.0643079\tvalid_1's rmse: 0.3056\n",
      "[14000]\ttraining's rmse: 0.0598785\tvalid_1's rmse: 0.305279\n",
      "[15000]\ttraining's rmse: 0.0559978\tvalid_1's rmse: 0.304983\n",
      "[16000]\ttraining's rmse: 0.0525521\tvalid_1's rmse: 0.304768\n",
      "[17000]\ttraining's rmse: 0.0494791\tvalid_1's rmse: 0.304507\n",
      "[18000]\ttraining's rmse: 0.0467359\tvalid_1's rmse: 0.304351\n",
      "[19000]\ttraining's rmse: 0.0442889\tvalid_1's rmse: 0.304197\n",
      "[20000]\ttraining's rmse: 0.0420827\tvalid_1's rmse: 0.304041\n",
      "[21000]\ttraining's rmse: 0.0400862\tvalid_1's rmse: 0.303927\n",
      "[22000]\ttraining's rmse: 0.0382656\tvalid_1's rmse: 0.303846\n",
      "[23000]\ttraining's rmse: 0.036592\tvalid_1's rmse: 0.303746\n",
      "[24000]\ttraining's rmse: 0.0350615\tvalid_1's rmse: 0.303666\n",
      "[25000]\ttraining's rmse: 0.0336666\tvalid_1's rmse: 0.30357\n",
      "[26000]\ttraining's rmse: 0.0323806\tvalid_1's rmse: 0.303488\n",
      "[27000]\ttraining's rmse: 0.0311907\tvalid_1's rmse: 0.30341\n",
      "[28000]\ttraining's rmse: 0.0300816\tvalid_1's rmse: 0.303339\n",
      "[29000]\ttraining's rmse: 0.0290605\tvalid_1's rmse: 0.303299\n",
      "[30000]\ttraining's rmse: 0.0281185\tvalid_1's rmse: 0.303264\n",
      "[31000]\ttraining's rmse: 0.0272251\tvalid_1's rmse: 0.30321\n",
      "[32000]\ttraining's rmse: 0.0264017\tvalid_1's rmse: 0.303177\n",
      "[33000]\ttraining's rmse: 0.0256279\tvalid_1's rmse: 0.303141\n",
      "[34000]\ttraining's rmse: 0.0249157\tvalid_1's rmse: 0.303108\n",
      "[35000]\ttraining's rmse: 0.0242541\tvalid_1's rmse: 0.303078\n",
      "[36000]\ttraining's rmse: 0.0236309\tvalid_1's rmse: 0.303053\n",
      "[37000]\ttraining's rmse: 0.0230462\tvalid_1's rmse: 0.303027\n",
      "[38000]\ttraining's rmse: 0.0225043\tvalid_1's rmse: 0.303011\n",
      "[39000]\ttraining's rmse: 0.0220004\tvalid_1's rmse: 0.302984\n",
      "[40000]\ttraining's rmse: 0.0215221\tvalid_1's rmse: 0.302958\n",
      "[41000]\ttraining's rmse: 0.0210753\tvalid_1's rmse: 0.302954\n",
      "[42000]\ttraining's rmse: 0.0206577\tvalid_1's rmse: 0.302939\n",
      "[43000]\ttraining's rmse: 0.0202715\tvalid_1's rmse: 0.302921\n",
      "[44000]\ttraining's rmse: 0.0199079\tvalid_1's rmse: 0.302904\n",
      "[45000]\ttraining's rmse: 0.0195687\tvalid_1's rmse: 0.302897\n",
      "[46000]\ttraining's rmse: 0.0192442\tvalid_1's rmse: 0.302885\n",
      "[47000]\ttraining's rmse: 0.0189354\tvalid_1's rmse: 0.302869\n",
      "[48000]\ttraining's rmse: 0.0186471\tvalid_1's rmse: 0.302861\n",
      "[49000]\ttraining's rmse: 0.0183853\tvalid_1's rmse: 0.302852\n",
      "[50000]\ttraining's rmse: 0.01813\tvalid_1's rmse: 0.302838\n",
      "[51000]\ttraining's rmse: 0.0178933\tvalid_1's rmse: 0.302833\n",
      "[52000]\ttraining's rmse: 0.0176623\tvalid_1's rmse: 0.30282\n",
      "[53000]\ttraining's rmse: 0.0174498\tvalid_1's rmse: 0.302812\n",
      "[54000]\ttraining's rmse: 0.0172504\tvalid_1's rmse: 0.302805\n",
      "[55000]\ttraining's rmse: 0.0170613\tvalid_1's rmse: 0.302793\n",
      "[56000]\ttraining's rmse: 0.0168812\tvalid_1's rmse: 0.302782\n",
      "[57000]\ttraining's rmse: 0.0167164\tvalid_1's rmse: 0.302776\n",
      "[58000]\ttraining's rmse: 0.0165512\tvalid_1's rmse: 0.302773\n",
      "[59000]\ttraining's rmse: 0.0165085\tvalid_1's rmse: 0.302771\n",
      "Early stopping, best iteration is:\n",
      "[57727]\ttraining's rmse: 0.0165924\tvalid_1's rmse: 0.30277\n",
      "train score : 0.0166 validation score : 0.3028\n",
      "################################################## 4 ##################################################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[1000]\ttraining's rmse: 0.286688\tvalid_1's rmse: 0.343773\n",
      "[2000]\ttraining's rmse: 0.2289\tvalid_1's rmse: 0.321781\n",
      "[3000]\ttraining's rmse: 0.191417\tvalid_1's rmse: 0.312333\n",
      "[4000]\ttraining's rmse: 0.16396\tvalid_1's rmse: 0.306737\n",
      "[5000]\ttraining's rmse: 0.142703\tvalid_1's rmse: 0.302961\n",
      "[6000]\ttraining's rmse: 0.125595\tvalid_1's rmse: 0.300401\n",
      "[7000]\ttraining's rmse: 0.111782\tvalid_1's rmse: 0.298662\n",
      "[8000]\ttraining's rmse: 0.100377\tvalid_1's rmse: 0.297406\n",
      "[9000]\ttraining's rmse: 0.0908571\tvalid_1's rmse: 0.296408\n",
      "[10000]\ttraining's rmse: 0.0828147\tvalid_1's rmse: 0.295626\n",
      "[11000]\ttraining's rmse: 0.0759841\tvalid_1's rmse: 0.294972\n",
      "[12000]\ttraining's rmse: 0.0700422\tvalid_1's rmse: 0.294536\n",
      "[13000]\ttraining's rmse: 0.0649309\tvalid_1's rmse: 0.294184\n",
      "[14000]\ttraining's rmse: 0.0605086\tvalid_1's rmse: 0.293868\n",
      "[15000]\ttraining's rmse: 0.056585\tvalid_1's rmse: 0.293652\n",
      "[16000]\ttraining's rmse: 0.0531152\tvalid_1's rmse: 0.293465\n",
      "[17000]\ttraining's rmse: 0.0500516\tvalid_1's rmse: 0.293227\n",
      "[18000]\ttraining's rmse: 0.0473117\tvalid_1's rmse: 0.293073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19000]\ttraining's rmse: 0.044852\tvalid_1's rmse: 0.29291\n",
      "[20000]\ttraining's rmse: 0.0426323\tvalid_1's rmse: 0.292792\n",
      "[21000]\ttraining's rmse: 0.0406357\tvalid_1's rmse: 0.29268\n",
      "[22000]\ttraining's rmse: 0.038817\tvalid_1's rmse: 0.292565\n",
      "[23000]\ttraining's rmse: 0.0371572\tvalid_1's rmse: 0.292485\n",
      "[24000]\ttraining's rmse: 0.035626\tvalid_1's rmse: 0.292398\n",
      "[25000]\ttraining's rmse: 0.0342342\tvalid_1's rmse: 0.292316\n",
      "[26000]\ttraining's rmse: 0.032952\tvalid_1's rmse: 0.292239\n",
      "[27000]\ttraining's rmse: 0.0317478\tvalid_1's rmse: 0.292173\n",
      "[28000]\ttraining's rmse: 0.0306346\tvalid_1's rmse: 0.29213\n",
      "[29000]\ttraining's rmse: 0.0296083\tvalid_1's rmse: 0.292064\n",
      "[30000]\ttraining's rmse: 0.0286499\tvalid_1's rmse: 0.292023\n",
      "[31000]\ttraining's rmse: 0.02776\tvalid_1's rmse: 0.291988\n",
      "[32000]\ttraining's rmse: 0.0269308\tvalid_1's rmse: 0.29194\n",
      "[33000]\ttraining's rmse: 0.0261561\tvalid_1's rmse: 0.291909\n",
      "[34000]\ttraining's rmse: 0.0254415\tvalid_1's rmse: 0.291886\n",
      "[35000]\ttraining's rmse: 0.024765\tvalid_1's rmse: 0.291851\n",
      "[36000]\ttraining's rmse: 0.0241185\tvalid_1's rmse: 0.291823\n",
      "[37000]\ttraining's rmse: 0.02353\tvalid_1's rmse: 0.291803\n",
      "[38000]\ttraining's rmse: 0.0229675\tvalid_1's rmse: 0.291776\n",
      "[39000]\ttraining's rmse: 0.0224436\tvalid_1's rmse: 0.291751\n",
      "[40000]\ttraining's rmse: 0.0219508\tvalid_1's rmse: 0.291734\n",
      "[41000]\ttraining's rmse: 0.0214928\tvalid_1's rmse: 0.291706\n",
      "[42000]\ttraining's rmse: 0.0210641\tvalid_1's rmse: 0.29169\n",
      "[43000]\ttraining's rmse: 0.0206625\tvalid_1's rmse: 0.291671\n",
      "[44000]\ttraining's rmse: 0.0202816\tvalid_1's rmse: 0.291661\n",
      "[45000]\ttraining's rmse: 0.0199213\tvalid_1's rmse: 0.291652\n",
      "[46000]\ttraining's rmse: 0.0195926\tvalid_1's rmse: 0.291651\n",
      "[47000]\ttraining's rmse: 0.0192846\tvalid_1's rmse: 0.291637\n",
      "[48000]\ttraining's rmse: 0.0189897\tvalid_1's rmse: 0.291625\n",
      "[49000]\ttraining's rmse: 0.0187124\tvalid_1's rmse: 0.291613\n",
      "[50000]\ttraining's rmse: 0.018449\tvalid_1's rmse: 0.291599\n",
      "[51000]\ttraining's rmse: 0.0182094\tvalid_1's rmse: 0.29159\n",
      "[52000]\ttraining's rmse: 0.0179784\tvalid_1's rmse: 0.291587\n",
      "[53000]\ttraining's rmse: 0.0177616\tvalid_1's rmse: 0.291581\n",
      "[54000]\ttraining's rmse: 0.0175527\tvalid_1's rmse: 0.291569\n",
      "[55000]\ttraining's rmse: 0.0173545\tvalid_1's rmse: 0.291567\n",
      "[56000]\ttraining's rmse: 0.0171633\tvalid_1's rmse: 0.291563\n",
      "[57000]\ttraining's rmse: 0.0169836\tvalid_1's rmse: 0.291566\n",
      "[58000]\ttraining's rmse: 0.0168153\tvalid_1's rmse: 0.291558\n",
      "[59000]\ttraining's rmse: 0.0166548\tvalid_1's rmse: 0.291555\n",
      "[60000]\ttraining's rmse: 0.0165072\tvalid_1's rmse: 0.291557\n",
      "[61000]\ttraining's rmse: 0.0163581\tvalid_1's rmse: 0.291554\n",
      "[62000]\ttraining's rmse: 0.0163264\tvalid_1's rmse: 0.291552\n",
      "[63000]\ttraining's rmse: 0.0163264\tvalid_1's rmse: 0.291552\n",
      "Early stopping, best iteration is:\n",
      "[61150]\ttraining's rmse: 0.0163353\tvalid_1's rmse: 0.291551\n",
      "train score : 0.0163 validation score : 0.2916\n",
      "final validation score : 0.2973115380062821\n"
     ]
    }
   ],
   "source": [
    "sub0,val0,score0,model0 = generate_model_two(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>log_target</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_test_0</td>\n",
       "      <td>4.859742</td>\n",
       "      <td>128.990955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_test_1</td>\n",
       "      <td>4.565645</td>\n",
       "      <td>96.124619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_test_10</td>\n",
       "      <td>3.175753</td>\n",
       "      <td>23.944840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_test_100</td>\n",
       "      <td>4.198282</td>\n",
       "      <td>66.571846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_test_1000</td>\n",
       "      <td>4.454754</td>\n",
       "      <td>86.034998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>ID_test_995</td>\n",
       "      <td>3.645542</td>\n",
       "      <td>38.303546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>ID_test_996</td>\n",
       "      <td>3.890532</td>\n",
       "      <td>48.936937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5032</th>\n",
       "      <td>ID_test_997</td>\n",
       "      <td>3.831232</td>\n",
       "      <td>46.119320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5033</th>\n",
       "      <td>ID_test_998</td>\n",
       "      <td>4.325032</td>\n",
       "      <td>75.567937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5034</th>\n",
       "      <td>ID_test_999</td>\n",
       "      <td>3.632469</td>\n",
       "      <td>37.806048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5035 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  log_target      target\n",
       "0        ID_test_0    4.859742  128.990955\n",
       "1        ID_test_1    4.565645   96.124619\n",
       "2       ID_test_10    3.175753   23.944840\n",
       "3      ID_test_100    4.198282   66.571846\n",
       "4     ID_test_1000    4.454754   86.034998\n",
       "...            ...         ...         ...\n",
       "5030   ID_test_995    3.645542   38.303546\n",
       "5031   ID_test_996    3.890532   48.936937\n",
       "5032   ID_test_997    3.831232   46.119320\n",
       "5033   ID_test_998    4.325032   75.567937\n",
       "5034   ID_test_999    3.632469   37.806048\n",
       "\n",
       "[5035 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val0.pred_log_target = np.exp(val0.pred_log_target)\n",
    "sub0['target'] = np.exp(sub0.log_target)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def metric(y,x):\n",
    "    return np.sqrt(mean_squared_error(x,y))\n",
    "def fun_w(w):\n",
    "    return metric(((1-w)*val0.pred_log_target + w*val1.pred_target),val1.target)\n",
    "import scipy\n",
    "op = scipy.optimize.minimize(fun_w , x0 = 0)\n",
    "sub3 = sub0.copy()\n",
    "sub3.target = sub0.target*(1-op.x) + sub1.target * op.x\n",
    "sub3.drop('log_target', axis = 1, inplace = True )\n",
    "sub3.to_csv(\"./final_submission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub3.to_csv(\"./final_submission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3.to_csv(\"./final_sub_{}.csv\".format(round(op.fun,6)),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub4 = pd.read_csv('final_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_test_0</td>\n",
       "      <td>139.071818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_test_1</td>\n",
       "      <td>104.565515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_test_10</td>\n",
       "      <td>31.424821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_test_100</td>\n",
       "      <td>66.871922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_test_1000</td>\n",
       "      <td>88.391856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>ID_test_995</td>\n",
       "      <td>43.498282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>ID_test_996</td>\n",
       "      <td>48.548816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5032</th>\n",
       "      <td>ID_test_997</td>\n",
       "      <td>37.492995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5033</th>\n",
       "      <td>ID_test_998</td>\n",
       "      <td>78.381421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5034</th>\n",
       "      <td>ID_test_999</td>\n",
       "      <td>39.982722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5035 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID      target\n",
       "0        ID_test_0  139.071818\n",
       "1        ID_test_1  104.565515\n",
       "2       ID_test_10   31.424821\n",
       "3      ID_test_100   66.871922\n",
       "4     ID_test_1000   88.391856\n",
       "...            ...         ...\n",
       "5030   ID_test_995   43.498282\n",
       "5031   ID_test_996   48.548816\n",
       "5032   ID_test_997   37.492995\n",
       "5033   ID_test_998   78.381421\n",
       "5034   ID_test_999   39.982722\n",
       "\n",
       "[5035 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       147.747783\n",
       "1       111.830063\n",
       "2        37.862370\n",
       "3        67.130178\n",
       "4        90.420255\n",
       "           ...    \n",
       "5030     47.969064\n",
       "5031     48.214785\n",
       "5032     30.068859\n",
       "5033     80.802809\n",
       "5034     41.856049\n",
       "Length: 5035, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub0.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
