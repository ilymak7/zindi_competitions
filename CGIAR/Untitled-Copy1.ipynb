{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from keras import Model\n",
    "from keras.layers import Dense , Flatten,Concatenate, Embedding,Input\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import tensorflow as tf\n",
    "seed = 2891  \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = pd.read_csv('Train.csv')\n",
    "band_names = [l.strip() for l in open('bandnames.txt', 'r').readlines()]\n",
    "ss = pd.read_csv('SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_im(fid, folder='image_arrays_train'):\n",
    "  fn = f'{folder}/{fid}.npy'\n",
    "  arr = np.load(fn)\n",
    "  bands_of_interest = ['S2_B1','S2_B2','S2_B3','S2_B4','S2_B5','S2_B6','S2_B7','S2_B8','S2_B8A','S2_B9','S2_B10',\n",
    "                       'S2_B11','S2_B12','S2_QA10','S2_QA20' ,'S2_QA60','CLIM_aet','CLIM_def','CLIM_pdsi','CLIM_pet',\n",
    "                       'CLIM_pr','CLIM_ro','CLIM_soil' ,'CLIM_srad' ,'CLIM_swe','CLIM_tmmn','CLIM_tmmx',\n",
    "                       'CLIM_vap','CLIM_vpd','CLIM_vs']\n",
    "  \n",
    "  values = {}\n",
    "  for month in range(12):\n",
    "    bns = [str(month) + '_' + b for b in bands_of_interest] # Bands of interest for this month \n",
    "    idxs = np.where(np.isin(band_names, bns)) # Index of these bands\n",
    "    vs = arr[idxs, 20, 20] # Sample the im at the center point\n",
    "    for bn, v in zip(bns, vs[0]):\n",
    "      values[bn] = v\n",
    "  return values\n",
    "def to_submit(pred_y,name_out):\n",
    "    y_predict = list(itertools.islice(pred_y, test.shape[0]))\n",
    "    y_predict = pd.DataFrame(prepro_y.inverse_transform(np.array(y_predict).reshape(len(y_predict),1)), columns = ['Yield'])\n",
    "    y_predict = y_predict.join(t)   #Field_ID\n",
    "    y_predic = y_predict[['Field_ID', 'Yield']]\n",
    "    y_predic.to_csv(name_out + '.csv',index=False)\n",
    "def input_fn_new(data_set, training = True):\n",
    "    continuous_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "    \n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(data_set[k].size)], values = data_set[k].values, dense_shape = [data_set[k].size, 1]) for k in FEATURES_CAT}\n",
    "\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n",
    "    \n",
    "    if training == True:\n",
    "        # Converts the label column into a constant Tensor.\n",
    "        label = tf.constant(data_set[LABEL].values)\n",
    "\n",
    "        # Returns the feature columns and the label.\n",
    "        return feature_cols, label\n",
    "    \n",
    "    return feature_cols\n",
    "train_sampled = pd.DataFrame([process_im(fid) for fid in train_new['Field_ID'].values])\n",
    "\n",
    "# Add in the field ID and yield\n",
    "train_sampled['Field_ID'] = train_new['Field_ID'].values\n",
    "train_sampled['Yield'] = train_new['Yield'].values\n",
    "# train_sampled.head()\n",
    "test_sampled = pd.DataFrame([process_im(fid, folder='image_arrays_test') for fid in ss['Field_ID'].values])\n",
    "# test_sampled['Field_ID'] = ss['Field_ID']\n",
    "# test_sampled['Yield'] = 0\n",
    "# Example\n",
    "# process_im('35AFSDD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_sampled.copy()\n",
    "test = test_sampled.copy()\n",
    "add_info = pd.read_csv('fields_w_additional_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_info.isna().sum()\n",
    "# A=add_info[add_info.columns[9]]\n",
    "# Anan=A[~np.isnan(A)] # Remove the NaNs\n",
    "# sns.distplot(Anan)\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# imputer = imputer.fit(add_info)\n",
    "# X= imputer.transform(add_info)\n",
    "add_info[add_info.columns.tolist()[1:]] = add_info[add_info.columns.tolist()[1:]].apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "prepo_add_info = MinMaxScaler()\n",
    "add_info[add_info.columns.tolist()[1:]] = prepo_add_info.fit_transform(add_info[add_info.columns.tolist()[1:]])\n",
    "# add_info.isna().sum()\n",
    "add_info_train = add_info.loc[add_info['Field_ID'].isin(train.Field_ID.values)]\n",
    "add_info_test =  add_info.loc[add_info['Field_ID'].isin(ss.Field_ID.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.Field_ID\n",
    "# add_info.where(add_info.Field_ID == train.Field_ID.values)\n",
    "# add_info_test\n",
    "add_info_cols = add_info.columns.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Import and split\n",
    "# # train = pd.read_csv('train_df_cust.csv')\n",
    "# train_numerical = train.select_dtypes(exclude=['object'])\n",
    "# train_categoric = train.select_dtypes(include=['object'])\n",
    "# train = train_numerical.merge(train_categoric, left_index = True, right_index = True) \n",
    "\n",
    "# # test = pd.read_csv('test_df_cust.csv')\n",
    "# Yield_ID = ss.Field_ID\n",
    "# test_numerical = test.select_dtypes(exclude=['object'])\n",
    "# test_categoric = test.select_dtypes(include=['object'])\n",
    "# test = test_numerical.merge(test_categoric, left_index = True, right_index = True) \n",
    "# # Removie the outliers\n",
    "\n",
    "# clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "# clf.fit(train_numerical)\n",
    "# y_noano = clf.predict(train_numerical)\n",
    "# y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "# y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "# train_numerical = train_numerical.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "# train_numerical.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# train_categoric = train_categoric.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "# train_categoric.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# train = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numerical = test.select_dtypes(exclude=['object'])\n",
    "test_categoric = test.select_dtypes(include=['object'])\n",
    "train_numerical = train.select_dtypes(exclude=['object'])\n",
    "train_categoric = train.select_dtypes(include=['object'])\n",
    "col_train = list(train.columns)\n",
    "col_train_bis = list(train.columns)\n",
    "col_train_bis.remove('Yield')\n",
    "\n",
    "col_train_num = list(train_numerical.columns)\n",
    "col_train_num_bis = list(train_numerical.columns)\n",
    "\n",
    "col_train_cat = list(train_categoric.columns)\n",
    "\n",
    "col_train_num_bis.remove('Yield')\n",
    "\n",
    "mat_train = np.matrix(train_numerical)\n",
    "mat_test  = np.matrix(test_numerical)\n",
    "mat_new = np.matrix(train_numerical.drop('Yield',axis = 1))\n",
    "mat_y = np.array(train.Yield)\n",
    "\n",
    "prepro_y = MinMaxScaler()\n",
    "prepro_y.fit(mat_y.reshape(2977,1))\n",
    "\n",
    "prepro = MinMaxScaler()\n",
    "prepro.fit(mat_train)\n",
    "\n",
    "prepro_test = MinMaxScaler()\n",
    "prepro_test.fit(mat_new)\n",
    "\n",
    "train_num_scale = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\n",
    "test_num_scale  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_num_bis)\n",
    "\n",
    "train[col_train_num] = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\n",
    "test[col_train_num_bis]  = test_num_scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of features\n",
    "# COLUMNS = col_train_num\n",
    "# FEATURES = col_train_num_bis\n",
    "# LABEL = \"Yield\"\n",
    "\n",
    "# FEATURES_CAT = col_train_cat\n",
    "\n",
    "# engineered_features = []\n",
    "\n",
    "# for continuous_feature in FEATURES:\n",
    "#     engineered_features.append(\n",
    "#         tf.feature_column.numeric_column(continuous_feature))\n",
    "\n",
    "# for categorical_feature in FEATURES_CAT:\n",
    "#     sparse_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "#         categorical_feature, hash_bucket_size=1000)\n",
    "\n",
    "#     engineered_features.append(tf.feature_column.embedding_column(categorical_column=sparse_column, dimension=16,combiner=\"sum\"))\n",
    "                                 \n",
    "# # Training set and Prediction set with the features to predict\n",
    "# training_set = train[FEATURES + FEATURES_CAT]\n",
    "# prediction_set = train.Yield\n",
    "\n",
    "# # Train and Test \n",
    "# x_train, x_test, y_train, y_test = train_test_split(training_set[FEATURES + FEATURES_CAT] ,\n",
    "#                                                     prediction_set, test_size=0.33, random_state=42)\n",
    "# y_train = pd.DataFrame(y_train, columns = [LABEL])\n",
    "# training_set = pd.DataFrame(x_train, columns = FEATURES + FEATURES_CAT).merge(y_train, left_index = True, right_index = True)\n",
    "\n",
    "# # Training for submission\n",
    "# training_sub = training_set[FEATURES] # + FEATURES_CAT]\n",
    "# testing_sub = test[FEATURES] # + FEATURES_CAT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a[0]['Field_ID']\n",
    "from keras.utils import to_categorical\n",
    "# to_categorical(train['Field_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = add_info['Field_ID'].values.copy()\n",
    "par = train['Field_ID'].values.copy()\n",
    "sar = ss.Field_ID.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = par.tolist()+sar.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(one_hot_encode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "for x in range(len(tot)):\n",
    "  mapping[tot[x]] = x\n",
    "\n",
    "# integer representation\n",
    "for x in range(len(tot)):\n",
    "  tot[x] = mapping[tot[x]]\n",
    "\n",
    "\n",
    "one_hot_encode = to_categorical(tot)\n",
    "print(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping[ss.Field_ID[55]]\n",
    "# one_hot_encode.shape\n",
    "# len(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_unique_cat  = len(tot)\n",
    "embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "embedding_size = int(embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = train[col_train_num_bis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_cat_data = Input(shape=(no_of_unique_cat,))\n",
    "inp_num_data = Input(shape=(num_data.shape[1],))\n",
    "input_add_data = Input(shape=(add_info_train[add_info_cols].shape[1],))\n",
    "# Bind nulti_hot to embedding layer\n",
    "emb = Embedding(input_dim=no_of_unique_cat, output_dim=embedding_size)(inp_cat_data)  \n",
    "# Also you need flatten embedded output of shape (?,3,2) to (?, 6) -\n",
    "# otherwise it's not possible to concatenate it with inp_num_data\n",
    "flatten = Flatten()(emb)\n",
    "# Concatenate two layers\n",
    "conc = Concatenate()([flatten, inp_num_data])\n",
    "conc2 = Concatenate()([conc, input_add_data])\n",
    "\n",
    "# x = BatchNormalization()(conc)\n",
    "dense1 = Dense(1148, activation='relu')(conc2)\n",
    "# x =  BatchNormalization()(dense1)\n",
    "dense2 = Dense(512,activation = 'relu')(dense1)\n",
    "dense3 = Dense(128,activation = 'relu')(dense2)\n",
    "# dense4 = Dense(128, activation='relu')(input_add_data)\n",
    "# dense5 = Dense(512,activation = 'relu')(dense4)\n",
    "# dense6 = Dense(128,activation = 'relu')(dense5)\n",
    "\n",
    "# dense7 = Dense(32,activation = 'relu')(dense3)\n",
    "\n",
    "out = Dense(1, activation='linear')(dense3)\n",
    "model = Model(inputs=[inp_cat_data, inp_num_data, input_add_data], outputs=out)\n",
    "from tensorflow import keras\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer='adam',loss='mse',metrics=['mean_squared_error'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min',restore_best_weights=True)\n",
    "save_best = ModelCheckpoint('checkpoint', save_best_only=True, \n",
    "                               monitor='val_loss', mode='min')\n",
    "model.compile(optimizer=opt,\n",
    "              loss=keras.losses.mean_squared_error,\n",
    "              metrics=[keras.metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000026516FE0AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000026516FE0AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "24/24 [==============================] - ETA: 0s - loss: 306860.0625 - mean_squared_error: 306860.0625WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000026519ED6318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000026519ED6318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EB050D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EB050D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 70s 3s/step - loss: 306860.0625 - mean_squared_error: 306860.0625 - val_loss: 18.4399 - val_mean_squared_error: 18.4399\n",
      "Epoch 2/30\n",
      "24/24 [==============================] - 51s 2s/step - loss: 92.9277 - mean_squared_error: 92.9277 - val_loss: 23.6741 - val_mean_squared_error: 23.6741\n",
      "Epoch 3/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 6.4785 - mean_squared_error: 6.4785WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EB05DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EB05DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 65s 3s/step - loss: 6.4785 - mean_squared_error: 6.4785 - val_loss: 0.5925 - val_mean_squared_error: 0.5925\n",
      "Epoch 4/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6548 - mean_squared_error: 0.6548WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000026523CAD708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000026523CAD708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 68s 3s/step - loss: 0.6548 - mean_squared_error: 0.6548 - val_loss: 0.1362 - val_mean_squared_error: 0.1362\n",
      "Epoch 5/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0758 - mean_squared_error: 0.0758WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000026518578D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000026518578D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 66s 3s/step - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
      "Epoch 6/30\n",
      "24/24 [==============================] - 51s 2s/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0324 - val_mean_squared_error: 0.0324\n",
      "Epoch 7/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EB05828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EB05828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 65s 3s/step - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0149 - val_mean_squared_error: 0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EB05B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EB05B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 75s 3s/step - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0147 - val_mean_squared_error: 0.0147\n",
      "Epoch 9/30\n",
      "24/24 [==============================] - 55s 2s/step - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0319 - val_mean_squared_error: 0.0319\n",
      "Epoch 10/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000026516FD45E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000026516FD45E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 65s 3s/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0139 - val_mean_squared_error: 0.0139\n",
      "Epoch 11/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EC0F3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002651EC0F3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 69s 3s/step - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0129 - val_mean_squared_error: 0.0129\n",
      "Epoch 12/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0140 - val_mean_squared_error: 0.0140\n",
      "Epoch 13/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0726 - mean_squared_error: 0.0726WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002652174B3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002652174B3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 75s 3s/step - loss: 0.0726 - mean_squared_error: 0.0726 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
      "Epoch 14/30\n",
      "24/24 [==============================] - 56s 2s/step - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0143 - val_mean_squared_error: 0.0143\n",
      "Epoch 15/30\n",
      "24/24 [==============================] - 54s 2s/step - loss: 0.0972 - mean_squared_error: 0.0972 - val_loss: 0.1085 - val_mean_squared_error: 0.1085\n",
      "Epoch 16/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 0.3026 - mean_squared_error: 0.3026 - val_loss: 1.5101 - val_mean_squared_error: 1.5101\n",
      "Epoch 17/30\n",
      "24/24 [==============================] - 54s 2s/step - loss: 0.4885 - mean_squared_error: 0.4885 - val_loss: 0.0701 - val_mean_squared_error: 0.0701\n",
      "Epoch 18/30\n",
      "24/24 [==============================] - 52s 2s/step - loss: 5.4645 - mean_squared_error: 5.4645 - val_loss: 0.1343 - val_mean_squared_error: 0.1343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2651f01cc48>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([one_hot_encode[:len(par)], train[col_train_num_bis], add_info_train[add_info_cols]] ,train['Yield'] , validation_split= 0.2,epochs=30,batch_size=100,verbose=1 , \n",
    "                 callbacks=[early_stopping,save_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_submit(pred_y,name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss.Field_ID\n",
    "# (train_sampled.Field_ID == 'E9UZCEA' ).any()\n",
    "# train_sampled\n",
    "# (train_new[train_new.Quality>1].Field_ID == 'E9UZCEA').any() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training_set1[0]\n",
    "# for x in range(len(sar)):\n",
    "#    sar[x] = mapping[sar[x]]\n",
    "# one_he = to_categorical(sar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_info.Field_ID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002652131D9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002652131D9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# ss.shape\n",
    "y_predict = model.predict([one_hot_encode[len(par):], test[col_train_num_bis],add_info_test[add_info_cols]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_categorical(sar)\n",
    "# y_predict\n",
    "ss = pd.read_csv('SampleSubmission.csv')\n",
    "t = ss.Field_ID\n",
    "to_submit(y_predict, \"submission_continuous16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "chec = pd.read_csv('submission_continuous14.csv')\n",
    "chec['Yield'] = abs(chec['Yield'])\n",
    "# (chec['Yield']<0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chec.to_csv('mod8_sub_abs.csv',index = False)\n",
    "(chec['Yield']<0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1022296 ],\n",
       "       [ 2.0850935 ],\n",
       "       [ 1.2661266 ],\n",
       "       ...,\n",
       "       [-0.32159072],\n",
       "       [ 1.5196413 ],\n",
       "       [-0.6216494 ]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_y1 = MinMaxScaler()\n",
    "prepro_y1.fit(mat_y.reshape(2642,1))\n",
    "prepro_y1.inverse_transform(np.array(y_predict).reshape(len(y_predict),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "(mat_y == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E9UZCEA</td>\n",
       "      <td>2.102230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1WGGS1Q</td>\n",
       "      <td>2.085094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EG2KXE2</td>\n",
       "      <td>1.266127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HC3GQXF</td>\n",
       "      <td>2.783577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7AK6GFK</td>\n",
       "      <td>2.277250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>3H89LWV</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>I6EYSGB</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>XOEIR44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>YB307JG</td>\n",
       "      <td>1.519641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>8TT86NF</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Field_ID     Yield\n",
       "0     E9UZCEA  2.102230\n",
       "1     1WGGS1Q  2.085094\n",
       "2     EG2KXE2  1.266127\n",
       "3     HC3GQXF  2.783577\n",
       "4     7AK6GFK  2.277250\n",
       "...       ...       ...\n",
       "1050  3H89LWV  0.000000\n",
       "1051  I6EYSGB  0.000000\n",
       "1052  XOEIR44  0.000000\n",
       "1053  YB307JG  1.519641\n",
       "1054  8TT86NF  0.000000\n",
       "\n",
       "[1055 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.Yield\n",
    "# chec = pd.read_csv('mod_sub_abs.csv')\n",
    "# chec\n",
    "chec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
