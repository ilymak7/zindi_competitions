{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from keras import Model\n",
    "from keras.layers import Dense , Flatten,Concatenate, Embedding,Input,Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "seed = 2891  \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = pd.read_csv('Train.csv')\n",
    "band_names = [l.strip() for l in open('bandnames.txt', 'r').readlines()]\n",
    "ss = pd.read_csv('SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_im(fid, folder='image_arrays_train'):\n",
    "  fn = f'{folder}/{fid}.npy'\n",
    "  arr = np.load(fn)\n",
    "  bands_of_interest = ['S2_B1','S2_B2','S2_B3','S2_B4','S2_B5','S2_B6','S2_B7','S2_B8','S2_B8A','S2_B9','S2_B10',\n",
    "                       'S2_B11','S2_B12','S2_QA10','S2_QA20' ,'S2_QA60','CLIM_aet','CLIM_def','CLIM_pdsi','CLIM_pet',\n",
    "                       'CLIM_pr','CLIM_ro','CLIM_soil' ,'CLIM_srad' ,'CLIM_swe','CLIM_tmmn','CLIM_tmmx',\n",
    "                       'CLIM_vap','CLIM_vpd','CLIM_vs']\n",
    "  \n",
    "  values = {}\n",
    "  for month in range(12):\n",
    "    bns = [str(month) + '_' + b for b in bands_of_interest] # Bands of interest for this month \n",
    "    idxs = np.where(np.isin(band_names, bns)) # Index of these bands\n",
    "    vs = arr[idxs, 20, 20] # Sample the im at the center point\n",
    "    for bn, v in zip(bns, vs[0]):\n",
    "      values[bn] = v\n",
    "  return values\n",
    "def to_submit(pred_y,name_out):\n",
    "    y_predict = list(itertools.islice(pred_y, test.shape[0]))\n",
    "    y_predict = pd.DataFrame(prepro_y.inverse_transform(np.array(y_predict).reshape(len(y_predict),1)), columns = ['Yield'])\n",
    "    y_predict = y_predict.join(t)   #Field_ID\n",
    "    y_predic = y_predict[['Field_ID', 'Yield']]\n",
    "    y_predic.to_csv(name_out + '.csv',index=False)\n",
    "def input_fn_new(data_set, training = True):\n",
    "    continuous_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "    \n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(data_set[k].size)], values = data_set[k].values, dense_shape = [data_set[k].size, 1]) for k in FEATURES_CAT}\n",
    "\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n",
    "    \n",
    "    if training == True:\n",
    "        # Converts the label column into a constant Tensor.\n",
    "        label = tf.constant(data_set[LABEL].values)\n",
    "\n",
    "        # Returns the feature columns and the label.\n",
    "        return feature_cols, label\n",
    "    \n",
    "    return feature_cols\n",
    "train_sampled = pd.DataFrame([process_im(fid) for fid in train_new['Field_ID'].values])\n",
    "\n",
    "# Add in the field ID and yield\n",
    "train_sampled['Field_ID'] = train_new['Field_ID'].values\n",
    "train_sampled['Yield'] = train_new['Yield'].values\n",
    "# train_sampled.head()\n",
    "test_sampled = pd.DataFrame([process_im(fid, folder='image_arrays_test') for fid in ss['Field_ID'].values])\n",
    "# test_sampled['Field_ID'] = ss['Field_ID']\n",
    "# test_sampled['Yield'] = 0\n",
    "# Example\n",
    "# process_im('35AFSDD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_sampled.copy()\n",
    "test = test_sampled.copy()\n",
    "add_info = pd.read_csv('fields_w_additional_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDVI(df):\n",
    "  bands_of_interest = ['S2_B4','S2_B8']\n",
    "  \n",
    "  values = {}\n",
    "  for month in range(12):\n",
    "    df[str(month) + '_'+'NDVI'] = (df[str(month) + '_'+bands_of_interest[1]] - df[str(month) + '_'+bands_of_interest[0]]) / (df[str(month) + '_'+bands_of_interest[1]] + df[str(month) + '_'+bands_of_interest[0]])  \n",
    "#     [str(month) + '_' + b for b in bands_of_interest] # Bands of interest for this month \n",
    "#     idxs = np.where(np.isin(band_names, bns)) # Index of these bands\n",
    "#     vs = arr[idxs, 20, 20] # Sample the im at the center point\n",
    "#     for bn, v in zip(bns, vs[0]):\n",
    "#       values[bn] = v\n",
    "    \n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = NDVI(train)\n",
    "test = NDVI(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train['Quality'] = train_new.Quality\n",
    "train = train[train.Quality>1]\n",
    "train.drop('Quality', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_info.isna().sum()\n",
    "# A=add_info[add_info.columns[9]]\n",
    "# Anan=A[~np.isnan(A)] # Remove the NaNs\n",
    "# sns.distplot(Anan)\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# imputer = imputer.fit(add_info)\n",
    "# X= imputer.transform(add_info)\n",
    "add_info[add_info.columns.tolist()[1:]] = add_info[add_info.columns.tolist()[1:]].apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "prepo_add_info = MinMaxScaler()\n",
    "add_info[add_info.columns.tolist()[1:]] = prepo_add_info.fit_transform(add_info[add_info.columns.tolist()[1:]])\n",
    "# add_info.isna().sum()\n",
    "add_info_train = add_info.loc[add_info['Field_ID'].isin(train.Field_ID.values)]\n",
    "add_info_test =  add_info.loc[add_info['Field_ID'].isin(ss.Field_ID.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.Field_ID\n",
    "# add_info.where(add_info.Field_ID == train.Field_ID.values)\n",
    "# add_info_test\n",
    "add_info_cols = add_info.columns.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Import and split\n",
    "# # train = pd.read_csv('train_df_cust.csv')\n",
    "# train_numerical = train.select_dtypes(exclude=['object'])\n",
    "# train_categoric = train.select_dtypes(include=['object'])\n",
    "# train = train_numerical.merge(train_categoric, left_index = True, right_index = True) \n",
    "\n",
    "# # test = pd.read_csv('test_df_cust.csv')\n",
    "# Yield_ID = ss.Field_ID\n",
    "# test_numerical = test.select_dtypes(exclude=['object'])\n",
    "# test_categoric = test.select_dtypes(include=['object'])\n",
    "# test = test_numerical.merge(test_categoric, left_index = True, right_index = True) \n",
    "# # Removie the outliers\n",
    "\n",
    "# clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "# clf.fit(train_numerical)\n",
    "# y_noano = clf.predict(train_numerical)\n",
    "# y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "# y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "# train_numerical = train_numerical.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "# train_numerical.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# train_categoric = train_categoric.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "# train_categoric.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# train = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numerical = test.select_dtypes(exclude=['object'])\n",
    "test_categoric = test.select_dtypes(include=['object'])\n",
    "train_numerical = train.select_dtypes(exclude=['object'])\n",
    "train_categoric = train.select_dtypes(include=['object'])\n",
    "col_train = list(train.columns)\n",
    "col_train_bis = list(train.columns)\n",
    "col_train_bis.remove('Yield')\n",
    "\n",
    "col_train_num = list(train_numerical.columns)\n",
    "col_train_num_bis = list(train_numerical.columns)\n",
    "\n",
    "col_train_cat = list(train_categoric.columns)\n",
    "\n",
    "col_train_num_bis.remove('Yield')\n",
    "\n",
    "mat_train = np.matrix(train_numerical)\n",
    "mat_test  = np.matrix(test_numerical)\n",
    "mat_new = np.matrix(train_numerical.drop('Yield',axis = 1))\n",
    "mat_y = np.array(train.Yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_test[2].shape\n",
    "# \n",
    "# (mat_new ==  ) .sum()\n",
    "# mat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_y = MinMaxScaler()\n",
    "prepro_y.fit(mat_y.reshape(2977,1))\n",
    "\n",
    "prepro = MinMaxScaler()\n",
    "prepro.fit(mat_train)\n",
    "\n",
    "prepro_test = MinMaxScaler()\n",
    "prepro_test.fit(mat_test)\n",
    "\n",
    "train_num_scale = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\n",
    "test_num_scale  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_num_bis)\n",
    "\n",
    "train[col_train_num] = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\n",
    "test[col_train_num_bis]  = test_num_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of features\n",
    "# COLUMNS = col_train_num\n",
    "# FEATURES = col_train_num_bis\n",
    "# LABEL = \"Yield\"\n",
    "\n",
    "# FEATURES_CAT = col_train_cat\n",
    "\n",
    "# engineered_features = []\n",
    "\n",
    "# for continuous_feature in FEATURES:\n",
    "#     engineered_features.append(\n",
    "#         tf.feature_column.numeric_column(continuous_feature))\n",
    "\n",
    "# for categorical_feature in FEATURES_CAT:\n",
    "#     sparse_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "#         categorical_feature, hash_bucket_size=1000)\n",
    "\n",
    "#     engineered_features.append(tf.feature_column.embedding_column(categorical_column=sparse_column, dimension=16,combiner=\"sum\"))\n",
    "                                 \n",
    "# # Training set and Prediction set with the features to predict\n",
    "# training_set = train[FEATURES + FEATURES_CAT]\n",
    "# prediction_set = train.Yield\n",
    "\n",
    "# # Train and Test \n",
    "# x_train, x_test, y_train, y_test = train_test_split(training_set[FEATURES + FEATURES_CAT] ,\n",
    "#                                                     prediction_set, test_size=0.33, random_state=42)\n",
    "# y_train = pd.DataFrame(y_train, columns = [LABEL])\n",
    "# training_set = pd.DataFrame(x_train, columns = FEATURES + FEATURES_CAT).merge(y_train, left_index = True, right_index = True)\n",
    "\n",
    "# # Training for submission\n",
    "# training_sub = training_set[FEATURES] # + FEATURES_CAT]\n",
    "# testing_sub = test[FEATURES] # + FEATURES_CAT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a[0]['Field_ID']\n",
    "from keras.utils import to_categorical\n",
    "# to_categorical(train['Field_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = add_info['Field_ID'].values.copy()\n",
    "par = train['Field_ID'].values.copy()\n",
    "sar = ss.Field_ID.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = par.tolist()+sar.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(one_hot_encode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "for x in range(len(tot)):\n",
    "  mapping[tot[x]] = x\n",
    "\n",
    "# integer representation\n",
    "for x in range(len(tot)):\n",
    "  tot[x] = mapping[tot[x]]\n",
    "\n",
    "\n",
    "one_hot_encode = to_categorical(tot)\n",
    "print(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping[ss.Field_ID[55]]\n",
    "# one_hot_encode.shape\n",
    "# len(par)\n",
    "bad = train_new[train_new.Quality == 1]['Field_ID'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train.Field_ID in  bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_unique_cat  = len(tot)\n",
    "embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "embedding_size = int(embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = train[col_train_num_bis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "inp_cat_data = Input(shape=(no_of_unique_cat,))\n",
    "inp_num_data = Input(shape=(num_data.shape[1],))\n",
    "input_add_data = Input(shape=(add_info_train[add_info_cols].shape[1],))\n",
    "# Bind nulti_hot to embedding layer\n",
    "emb = Embedding(input_dim=no_of_unique_cat, output_dim=embedding_size)(inp_cat_data)  \n",
    "# Also you need flatten embedded output of shape (?,3,2) to (?, 6) -\n",
    "# otherwise it's not possible to concatenate it with inp_num_data\n",
    "flatten = Flatten()(emb)\n",
    "# Concatenate two layers\n",
    "conc = Concatenate()([flatten, inp_num_data])\n",
    "conc2 = Concatenate()([conc, input_add_data])\n",
    "\n",
    "# x = BatchNormalization()(conc)\n",
    "dense1 = Dense(1148, activation='relu')(conc2)\n",
    "x = Dropout(0.2)(dense1)\n",
    "# x =  BatchNormalization()(dense1)\n",
    "dense2 = Dense(512,activation = 'relu')(x)\n",
    "x= Dropout(0.2)(dense2)\n",
    "# dense21 = Dense(512,activation = 'relu')(x)\n",
    "# x= Dropout(0.2)(dense21)\n",
    "dense3 = Dense(256,activation = 'relu')(x)\n",
    "x = Dropout(0.2)(dense3)\n",
    "dense4 = Dense(128,activation = 'relu')(x)\n",
    "# x = Dropout(0.2)(dense4)\n",
    "# dense5 = Dense(64,activation = 'relu')(x)\n",
    "\n",
    "# dense4 = Dense(128, activation='relu')(input_add_data)\n",
    "# dense5 = Dense(512,activation = 'relu')(dense4)\n",
    "# dense6 = Dense(128,activation = 'relu')(dense5)\n",
    "\n",
    "# dense7 = Dense(32,activation = 'relu')(dense3)\n",
    "\n",
    "out = Dense(1, activation='linear')(dense4)\n",
    "model = Model(inputs=[inp_cat_data, inp_num_data, input_add_data], outputs=out)\n",
    "from tensorflow import keras\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer='adam',loss='mse',metrics=['mean_squared_error'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min',restore_best_weights=True)\n",
    "save_best = ModelCheckpoint('checkpoint', save_best_only=True, \n",
    "                               monitor='val_loss', mode='min')\n",
    "model.compile(optimizer=opt,\n",
    "              loss=keras.losses.Huber(delta=1),\n",
    "              metrics=[keras.metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# try forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_submit(pred_y,name_out)\n",
    "# %load_ext tensorboard\n",
    "\n",
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001E27808A678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001E27808A678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "24/24 [==============================] - ETA: 0s - loss: 220.9701 - mean_squared_error: 1137919.6250WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001E2742E3288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001E2742E3288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001E27860EEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001E27860EEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 72s 3s/step - loss: 220.9701 - mean_squared_error: 1137919.6250 - val_loss: 0.0103 - val_mean_squared_error: 0.0206\n",
      "Epoch 2/30\n",
      "24/24 [==============================] - 71s 3s/step - loss: 0.0527 - mean_squared_error: 0.1473 - val_loss: 0.0150 - val_mean_squared_error: 0.0300\n",
      "Epoch 3/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0556WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001E277DFDEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001E277DFDEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 78s 3s/step - loss: 0.0258 - mean_squared_error: 0.0556 - val_loss: 0.0073 - val_mean_squared_error: 0.0146\n",
      "Epoch 4/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0416WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001E279FDCC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001E279FDCC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 77s 3s/step - loss: 0.0168 - mean_squared_error: 0.0416 - val_loss: 0.0072 - val_mean_squared_error: 0.0143\n",
      "Epoch 5/30\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.0121 - mean_squared_error: 0.0245 - val_loss: 0.0075 - val_mean_squared_error: 0.0151\n",
      "Epoch 6/30\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.0097 - mean_squared_error: 0.0198 - val_loss: 0.0075 - val_mean_squared_error: 0.0151\n",
      "Epoch 7/30\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.0089 - mean_squared_error: 0.0179 - val_loss: 0.0076 - val_mean_squared_error: 0.0153\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([one_hot_encode[:len(par)], train[col_train_num_bis], add_info_train[add_info_cols]] ,train['Yield'] , validation_split= 0.2,epochs=30,batch_size=100,verbose=1 , \n",
    "                 callbacks=[early_stopping,save_best, tensorboard_cb])\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./my_logs --port=6006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss.Field_ID\n",
    "# (train_sampled.Field_ID == 'E9UZCEA' ).any()\n",
    "# train_sampled\n",
    "# (train_new[train_new.Quality>1].Field_ID == 'E9UZCEA').any() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training_set1[0]\n",
    "# for x in range(len(sar)):\n",
    "#    sar[x] = mapping[sar[x]]\n",
    "# one_he = to_categorical(sar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_info.Field_ID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001E279EE70D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001E279EE70D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# ss.shape\n",
    "y_predict = model.predict([one_hot_encode[len(par):], test[col_train_num_bis],add_info_test[add_info_cols]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_categorical(sar)\n",
    "# y_predict\n",
    "ss = pd.read_csv('SampleSubmission.csv')\n",
    "t = ss.Field_ID\n",
    "to_submit(y_predict, \"submission_continuous31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "chec = pd.read_csv('submission_continuous31.csv')\n",
    "# chec['Yield'] = abs(chec['Yield'])\n",
    "# (chec['Yield']<0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chec.to_csv('mod8_sub_abs.csv',index = False)\n",
    "(chec['Yield']<0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_y1 = MinMaxScaler()\n",
    "prepro_y1.fit(mat_y.reshape(2642,1))\n",
    "prepro_y1.inverse_transform(np.array(y_predict).reshape(len(y_predict),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "(mat_y == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.Yield\n",
    "# chec = pd.read_csv('mod_sub_abs.csv')\n",
    "# chec\n",
    "chec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = train.merge(add_info_train, on = 'Field_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.drop(['Field_ID', 'Yield'], axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = pca.transform(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Field_ID'] = ss.Field_ID\n",
    "C = test.merge(add_info_test, on = 'Field_ID')\n",
    "C.drop(['Field_ID'], axis = 1 , inplace = True)\n",
    "D = pca.transform(C)\n",
    "D.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D[0].shape\n",
    "# test.shape\n",
    "229*6\n",
    "# train\n",
    "add_info_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
