{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from keras import Model\n",
    "from keras.layers import Dense , Flatten,Concatenate, Embedding,Input\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import tensorflow as tf\n",
    "seed = 2891  \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = pd.read_csv('Train.csv')\n",
    "band_names = [l.strip() for l in open('bandnames.txt', 'r').readlines()]\n",
    "ss = pd.read_csv('SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_im(fid, folder='image_arrays_train'):\n",
    "  fn = f'{folder}/{fid}.npy'\n",
    "  arr = np.load(fn)\n",
    "  bands_of_interest = ['S2_B1','S2_B2','S2_B3','S2_B4','S2_B5','S2_B6','S2_B7','S2_B8','S2_B8A','S2_B9','S2_B10',\n",
    "                       'S2_B11','S2_B12','S2_QA10','S2_QA20' ,'S2_QA60','CLIM_aet','CLIM_def','CLIM_pdsi','CLIM_pet',\n",
    "                       'CLIM_pr','CLIM_ro','CLIM_soil' ,'CLIM_srad' ,'CLIM_swe','CLIM_tmmn','CLIM_tmmx',\n",
    "                       'CLIM_vap','CLIM_vpd','CLIM_vs']\n",
    "  \n",
    "  values = {}\n",
    "  for month in range(12):\n",
    "    bns = [str(month) + '_' + b for b in bands_of_interest] # Bands of interest for this month \n",
    "    idxs = np.where(np.isin(band_names, bns)) # Index of these bands\n",
    "    vs = arr[idxs, 20, 20] # Sample the im at the center point\n",
    "    for bn, v in zip(bns, vs[0]):\n",
    "      values[bn] = v\n",
    "  return values\n",
    "def to_submit(pred_y,name_out):\n",
    "    y_predict = list(itertools.islice(pred_y, test.shape[0]))\n",
    "    y_predict = pd.DataFrame(prepro_y.inverse_transform(np.array(y_predict).reshape(len(y_predict),1)), columns = ['Yield'])\n",
    "    y_predict = y_predict.join(t)   #Field_ID\n",
    "    y_predic = y_predict[['Field_ID', 'Yield']]\n",
    "    y_predic.to_csv(name_out + '.csv',index=False)\n",
    "def input_fn_new(data_set, training = True):\n",
    "    continuous_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "    \n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(data_set[k].size)], values = data_set[k].values, dense_shape = [data_set[k].size, 1]) for k in FEATURES_CAT}\n",
    "\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n",
    "    \n",
    "    if training == True:\n",
    "        # Converts the label column into a constant Tensor.\n",
    "        label = tf.constant(data_set[LABEL].values)\n",
    "\n",
    "        # Returns the feature columns and the label.\n",
    "        return feature_cols, label\n",
    "    \n",
    "    return feature_cols\n",
    "train_sampled = pd.DataFrame([process_im(fid) for fid in train_new['Field_ID'].values])\n",
    "\n",
    "# Add in the field ID and yield\n",
    "train_sampled['Field_ID'] = train_new['Field_ID'].values\n",
    "train_sampled['Yield'] = train_new['Yield'].values\n",
    "# train_sampled.head()\n",
    "test_sampled = pd.DataFrame([process_im(fid, folder='image_arrays_test') for fid in ss['Field_ID'].values])\n",
    "# test_sampled['Field_ID'] = ss['Field_ID']\n",
    "# test_sampled['Yield'] = 0\n",
    "# Example\n",
    "# process_im('35AFSDD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_sampled.copy()\n",
    "test = test_sampled.copy()\n",
    "add_info = pd.read_csv('fields_w_additional_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_info.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Import and split\n",
    "# # train = pd.read_csv('train_df_cust.csv')\n",
    "# train_numerical = train.select_dtypes(exclude=['object'])\n",
    "# train_categoric = train.select_dtypes(include=['object'])\n",
    "# train = train_numerical.merge(train_categoric, left_index = True, right_index = True) \n",
    "\n",
    "# # test = pd.read_csv('test_df_cust.csv')\n",
    "# Yield_ID = ss.Field_ID\n",
    "# test_numerical = test.select_dtypes(exclude=['object'])\n",
    "# test_categoric = test.select_dtypes(include=['object'])\n",
    "# test = test_numerical.merge(test_categoric, left_index = True, right_index = True) \n",
    "# # Removie the outliers\n",
    "\n",
    "# clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "# clf.fit(train_numerical)\n",
    "# y_noano = clf.predict(train_numerical)\n",
    "# y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "# y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "# train_numerical = train_numerical.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "# train_numerical.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# train_categoric = train_categoric.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "# train_categoric.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# train = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numerical = test.select_dtypes(exclude=['object'])\n",
    "test_categoric = test.select_dtypes(include=['object'])\n",
    "train_numerical = train.select_dtypes(exclude=['object'])\n",
    "train_categoric = train.select_dtypes(include=['object'])\n",
    "col_train = list(train.columns)\n",
    "col_train_bis = list(train.columns)\n",
    "col_train_bis.remove('Yield')\n",
    "\n",
    "col_train_num = list(train_numerical.columns)\n",
    "col_train_num_bis = list(train_numerical.columns)\n",
    "\n",
    "col_train_cat = list(train_categoric.columns)\n",
    "\n",
    "col_train_num_bis.remove('Yield')\n",
    "\n",
    "mat_train = np.matrix(train_numerical)\n",
    "mat_test  = np.matrix(test_numerical)\n",
    "mat_new = np.matrix(train_numerical.drop('Yield',axis = 1))\n",
    "mat_y = np.array(train.Yield)\n",
    "\n",
    "prepro_y = MinMaxScaler()\n",
    "prepro_y.fit(mat_y.reshape(2977,1))\n",
    "\n",
    "prepro = MinMaxScaler()\n",
    "prepro.fit(mat_train)\n",
    "\n",
    "prepro_test = MinMaxScaler()\n",
    "prepro_test.fit(mat_new)\n",
    "\n",
    "train_num_scale = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\n",
    "test_num_scale  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_num_bis)\n",
    "\n",
    "train[col_train_num] = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\n",
    "test[col_train_num_bis]  = test_num_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of features\n",
    "# COLUMNS = col_train_num\n",
    "# FEATURES = col_train_num_bis\n",
    "# LABEL = \"Yield\"\n",
    "\n",
    "# FEATURES_CAT = col_train_cat\n",
    "\n",
    "# engineered_features = []\n",
    "\n",
    "# for continuous_feature in FEATURES:\n",
    "#     engineered_features.append(\n",
    "#         tf.feature_column.numeric_column(continuous_feature))\n",
    "\n",
    "# for categorical_feature in FEATURES_CAT:\n",
    "#     sparse_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "#         categorical_feature, hash_bucket_size=1000)\n",
    "\n",
    "#     engineered_features.append(tf.feature_column.embedding_column(categorical_column=sparse_column, dimension=16,combiner=\"sum\"))\n",
    "                                 \n",
    "# # Training set and Prediction set with the features to predict\n",
    "# training_set = train[FEATURES + FEATURES_CAT]\n",
    "# prediction_set = train.Yield\n",
    "\n",
    "# # Train and Test \n",
    "# x_train, x_test, y_train, y_test = train_test_split(training_set[FEATURES + FEATURES_CAT] ,\n",
    "#                                                     prediction_set, test_size=0.33, random_state=42)\n",
    "# y_train = pd.DataFrame(y_train, columns = [LABEL])\n",
    "# training_set = pd.DataFrame(x_train, columns = FEATURES + FEATURES_CAT).merge(y_train, left_index = True, right_index = True)\n",
    "\n",
    "# # Training for submission\n",
    "# training_sub = training_set[FEATURES] # + FEATURES_CAT]\n",
    "# testing_sub = test[FEATURES] # + FEATURES_CAT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a[0]['Field_ID']\n",
    "from keras.utils import to_categorical\n",
    "# to_categorical(train['Field_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = add_info['Field_ID'].values.copy()\n",
    "par = train['Field_ID'].values.copy()\n",
    "sar = ss.Field_ID.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = par.tolist()+sar.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(one_hot_encode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "for x in range(len(tot)):\n",
    "  mapping[tot[x]] = x\n",
    "\n",
    "# integer representation\n",
    "for x in range(len(tot)):\n",
    "  tot[x] = mapping[tot[x]]\n",
    "\n",
    "\n",
    "one_hot_encode = to_categorical(tot)\n",
    "print(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping[ss.Field_ID[55]]\n",
    "# one_hot_encode.shape\n",
    "# len(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_unique_cat  = len(tot)\n",
    "embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "embedding_size = int(embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = train[col_train_num_bis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_cat_data = Input(shape=(no_of_unique_cat,))\n",
    "inp_num_data = Input(shape=(num_data.shape[1],))\n",
    "# Bind nulti_hot to embedding layer\n",
    "emb = Embedding(input_dim=no_of_unique_cat, output_dim=embedding_size)(inp_cat_data)  \n",
    "# Also you need flatten embedded output of shape (?,3,2) to (?, 6) -\n",
    "# otherwise it's not possible to concatenate it with inp_num_data\n",
    "flatten = Flatten()(emb)\n",
    "# Concatenate two layers\n",
    "conc = Concatenate()([flatten, inp_num_data])\n",
    "# x = BatchNormalization()(conc)\n",
    "dense1 = Dense(1148, activation='relu')(conc)\n",
    "# x =  BatchNormalization()(dense1)\n",
    "dense2 = Dense(512,activation = 'relu')(dense1)\n",
    "dense3 = Dense(128,activation = 'relu')(dense2)\n",
    "out = Dense(1, activation='linear')(dense3)\n",
    "model = Model(inputs=[inp_cat_data, inp_num_data], outputs=out)\n",
    "from tensorflow import keras\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer='adam',loss='mse',metrics=['mean_squared_error'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min',restore_best_weights=True)\n",
    "save_best = ModelCheckpoint('checkpoint', save_best_only=True, \n",
    "                               monitor='val_loss', mode='min')\n",
    "model.compile(optimizer=opt,\n",
    "              loss=keras.losses.mean_squared_error,\n",
    "              metrics=[keras.metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002318DEBF288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002318DEBF288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "24/24 [==============================] - ETA: 0s - loss: 62289.7812 - mean_squared_error: 62289.7812WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002318E6A3798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002318E6A3798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\amakr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\amakr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318DF08828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318DF08828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 68s 3s/step - loss: 62289.7812 - mean_squared_error: 62289.7812 - val_loss: 17.2425 - val_mean_squared_error: 17.2425\n",
      "Epoch 2/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 37.8815 - mean_squared_error: 37.8815WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318DF414C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318DF414C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 66s 3s/step - loss: 37.8815 - mean_squared_error: 37.8815 - val_loss: 0.1032 - val_mean_squared_error: 0.1032\n",
      "Epoch 3/30\n",
      "24/24 [==============================] - 51s 2s/step - loss: 1.4086 - mean_squared_error: 1.4086 - val_loss: 0.7997 - val_mean_squared_error: 0.7997\n",
      "Epoch 4/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.1838 - mean_squared_error: 0.1838 - val_loss: 0.1394 - val_mean_squared_error: 0.1394\n",
      "Epoch 5/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318DF084C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318DF084C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 63s 3s/step - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0229 - val_mean_squared_error: 0.0229\n",
      "Epoch 6/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0236 - val_mean_squared_error: 0.0236\n",
      "Epoch 7/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318F4B11F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318F4B11F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 62s 3s/step - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0153 - val_mean_squared_error: 0.0153\n",
      "Epoch 8/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318EBF8678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318EBF8678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 63s 3s/step - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0147 - val_mean_squared_error: 0.0147\n",
      "Epoch 10/30\n",
      "24/24 [==============================] - 48s 2s/step - loss: 0.3178 - mean_squared_error: 0.3178 - val_loss: 0.1576 - val_mean_squared_error: 0.1576\n",
      "Epoch 11/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1228 - mean_squared_error: 0.1228WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318EBF8E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002318EBF8E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: checkpoint\\assets\n",
      "24/24 [==============================] - 79s 3s/step - loss: 0.1228 - mean_squared_error: 0.1228 - val_loss: 0.0129 - val_mean_squared_error: 0.0129\n",
      "Epoch 12/30\n",
      "24/24 [==============================] - 48s 2s/step - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 13/30\n",
      "24/24 [==============================] - 48s 2s/step - loss: 0.0662 - mean_squared_error: 0.0662 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 14/30\n",
      "24/24 [==============================] - 48s 2s/step - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0740 - val_mean_squared_error: 0.0740\n",
      "Epoch 15/30\n",
      "24/24 [==============================] - 51s 2s/step - loss: 0.1269 - mean_squared_error: 0.1269 - val_loss: 0.0139 - val_mean_squared_error: 0.0139\n",
      "Epoch 16/30\n",
      "24/24 [==============================] - 54s 2s/step - loss: 0.0720 - mean_squared_error: 0.0720 - val_loss: 0.0131 - val_mean_squared_error: 0.0131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2318cb579c8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([one_hot_encode[:len(par)], train[col_train_num_bis]] ,train['Yield'] , validation_split= 0.2,epochs=30,batch_size=100,verbose=1 , \n",
    "                 callbacks=[early_stopping,save_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_submit(pred_y,name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss.Field_ID\n",
    "# (train_sampled.Field_ID == 'E9UZCEA' ).any()\n",
    "# train_sampled\n",
    "# (train_new[train_new.Quality>1].Field_ID == 'E9UZCEA').any() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training_set1[0]\n",
    "# for x in range(len(sar)):\n",
    "#    sar[x] = mapping[sar[x]]\n",
    "# one_he = to_categorical(sar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_info.Field_ID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002318F0473A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002318F0473A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# ss.shape\n",
    "y_predict = model.predict([one_hot_encode[len(par):], test[col_train_num_bis]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_categorical(sar)\n",
    "# y_predict\n",
    "ss = pd.read_csv('SampleSubmission.csv')\n",
    "t = ss.Field_ID\n",
    "to_submit(y_predict, \"submission_continuous10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chec = pd.read_csv('submission_continuous10.csv')\n",
    "chec['Yield'] = (chec['Yield']).clip(lower = 0)\n",
    "# (chec['Yield']<0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chec.to_csv('mod4_sub_0.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1022296 ],\n",
       "       [ 2.0850935 ],\n",
       "       [ 1.2661266 ],\n",
       "       ...,\n",
       "       [-0.32159072],\n",
       "       [ 1.5196413 ],\n",
       "       [-0.6216494 ]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_y1 = MinMaxScaler()\n",
    "prepro_y1.fit(mat_y.reshape(2642,1))\n",
    "prepro_y1.inverse_transform(np.array(y_predict).reshape(len(y_predict),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "(mat_y == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E9UZCEA</td>\n",
       "      <td>2.102230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1WGGS1Q</td>\n",
       "      <td>2.085094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EG2KXE2</td>\n",
       "      <td>1.266127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HC3GQXF</td>\n",
       "      <td>2.783577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7AK6GFK</td>\n",
       "      <td>2.277250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>3H89LWV</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>I6EYSGB</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>XOEIR44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>YB307JG</td>\n",
       "      <td>1.519641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>8TT86NF</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Field_ID     Yield\n",
       "0     E9UZCEA  2.102230\n",
       "1     1WGGS1Q  2.085094\n",
       "2     EG2KXE2  1.266127\n",
       "3     HC3GQXF  2.783577\n",
       "4     7AK6GFK  2.277250\n",
       "...       ...       ...\n",
       "1050  3H89LWV  0.000000\n",
       "1051  I6EYSGB  0.000000\n",
       "1052  XOEIR44  0.000000\n",
       "1053  YB307JG  1.519641\n",
       "1054  8TT86NF  0.000000\n",
       "\n",
       "[1055 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.Yield\n",
    "# chec = pd.read_csv('mod_sub_abs.csv')\n",
    "# chec\n",
    "chec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
