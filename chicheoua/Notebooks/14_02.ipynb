{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "# import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = fasttext.train_supervised(input=TRAIN_FILEPATH, lr=1.0, epoch=100,\n",
    "# #                              wordNgrams=2, bucket=200000, dim=300, loss='hs',\n",
    "# #                              pretrainedVectors=VECTORS_FILEPATH)\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\",\", \" \", string) \n",
    "    string = re.sub(r\"!\", \" \", string) \n",
    "    string = re.sub(r\"\\(\", \" \", string) \n",
    "    string = re.sub(r\"\\)\", \" \", string) \n",
    "    string = re.sub(r\"\\?\", \" \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)   \n",
    "    string = re.sub(\" \\d+\", \" \", string)\n",
    "    return  string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "TRAIN_FILEPATH = \"../Translated/cleaned/train.csv\"\n",
    "TEST_FILEPATH = \"../Translated/cleaned/test.csv\"\n",
    "SS_FILEPATH = \"../data/SampleSubmission.csv\"\n",
    "VECTORS_FILEPATH = \"\"\n",
    "train = pd.read_csv(TRAIN_FILEPATH)\n",
    "test = pd.read_csv(TEST_FILEPATH)\n",
    "ss = pd.read_csv(SS_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train.Text.str.lower().str.split()\n",
    "# # from collections import Counter\n",
    "# # result = Counter(\" \".join(train['cleaned'].values.tolist()).split(\" \")).items()\n",
    "# # result\n",
    "train[\"Text\"] =train.Text.apply(lambda x: clean_str(x))\n",
    "test[\"Text\"] =test.Text.apply(lambda x: clean_str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.Text.str.lower().str.split().apply(len).max()\n",
    "# train\n",
    "# train.Text[0]\n",
    "corpus = pd.concat([train.Text, test.Text],axis = 0,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result)\n",
    "# test\n",
    "# result\n",
    "# corpus = \n",
    "# train.Text.values.dtype\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tes = mat.prod(axis = 0)\n",
    "tes['Label'] = train.Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tes.groupby('Label').aggregate('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.values.prod(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat.shape\n",
    "stand = StandardScaler()\n",
    "mat2 = stand.fit_transform(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=0.99)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "pca.fit(mat2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pca.transform(mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_new[:train.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_train['Label'] = train['Label']\n",
    "X_test = pd.DataFrame(data_new[train.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['ID'] = train['ID']\n",
    "X_test['ID'] = test['ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "lb = LabelBinarizer()\n",
    "y_train_b = lb.fit_transform(train['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -1.126357\n",
       "1       -0.860474\n",
       "2       -0.529630\n",
       "3       -0.765032\n",
       "4       -0.518144\n",
       "          ...    \n",
       "1431    -0.769060\n",
       "1432    -1.618650\n",
       "1433    18.525993\n",
       "1434    -1.273803\n",
       "1435    -1.536207\n",
       "Name: 0, Length: 1436, dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1924"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tes = pd.DataFrame(mat[:train.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amakr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# train.Text[0]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "\n",
    "DATA_PATH = '../data'\n",
    "EMBEDDING_DIR = '../Vectors'\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('loading word embeddings...')\n",
    "# embeddings_index = {}\n",
    "# f = codecs.open('../Vectors/wiki.ny.vec', encoding='utf-8')\n",
    "# for line in tqdm(f):\n",
    "#     values = line.rstrip().rsplit(' ')\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "# print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = fasttext.load_model()\n",
    "# train[train.Label == \"POLITICS\"].Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model)\n",
    "# print(\"num train: \", train.shape[0])\n",
    "# print(\"num test: \", test.shape[0])\n",
    "\n",
    "# label_names = train.Label.unique().tolist()\n",
    "# y_train = train.Label.values\n",
    "\n",
    "# # #visualize word distribution\n",
    "# train['doc_len'] = train['Text'].apply(lambda words: len(words.split(\" \")))\n",
    "# max_seq_len = np.round(train['doc_len'].mean() + train['doc_len'].std()).astype(int)\n",
    "# sns.distplot(train['doc_len'], hist=True, kde=True, color='b', label='doc len')\n",
    "# plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n",
    "# plt.title('article length'); plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # model.predict(train.cleaned[0])\n",
    "# # model = fasttext.train_supervised(input=TRAIN_FILEPATH, lr=1.0, epoch=100,\n",
    "# #                              wordNgrams=2, bucket=200000, dim=300, loss='hs')\n",
    "# # #                              pretrainedVectors=\"../Vectors/wiki.ny.vec\")\n",
    "# raw_docs_train = train['Text'].tolist()\n",
    "# raw_docs_test = test['Text'].tolist() \n",
    "# num_classes = len(label_names)\n",
    "\n",
    "# print(\"pre-processing train data...\")\n",
    "# processed_docs_train = []\n",
    "# for doc in tqdm(raw_docs_train):\n",
    "#     tokens = tokenizer.tokenize(doc)\n",
    "#     filtered = [word for word in tokens if word not in stop_words]\n",
    "#     processed_docs_train.append(\" \".join(filtered))\n",
    "# #end for\n",
    "\n",
    "# processed_docs_test = []\n",
    "# for doc in tqdm(raw_docs_test):\n",
    "#     tokens = tokenizer.tokenize(doc)\n",
    "#     filtered = [word for word in tokens if word not in stop_words]\n",
    "#     processed_docs_test.append(\" \".join(filtered))\n",
    "# #end for\n",
    "\n",
    "# print(\"tokenizing input data...\")\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS+1, lower=True, char_level=False)\n",
    "# tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\n",
    "# word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "# word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "# word_index = tokenizer.word_index\n",
    "# print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "# #pad sequences\n",
    "# word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.Text\n",
    "# i\n",
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_seq_train.shape\n",
    "#training params\n",
    "batch_size = 64 \n",
    "num_epochs =  50\n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('preparing embedding matrix...')\n",
    "# words_not_found = []\n",
    "# nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "# embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= nb_words:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "#     else:\n",
    "#         words_not_found.append(word)\n",
    "# print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -0.247346\n",
       "1      -0.816465\n",
       "2      -0.603540\n",
       "3       0.370478\n",
       "4      -0.201107\n",
       "          ...   \n",
       "1431   -0.830241\n",
       "1432   -4.398766\n",
       "1433    7.091032\n",
       "1434    1.153235\n",
       "1435   -4.859901\n",
       "Name: 56, Length: 1436, dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"sample words not found: \", np.random.choice(words_not_found, 100))\n",
    "# len(words_not_found)\n",
    "# lb = LabelBinarizer()\n",
    "# y_train_b = lb.fit_transform(y_train) \n",
    "X_train[features][56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_b[0]\n",
    "num_classes = y_train_b.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        81.021802\n",
       "1        89.284926\n",
       "2       143.630518\n",
       "3        11.769739\n",
       "4       375.905040\n",
       "           ...    \n",
       "1919     46.571552\n",
       "1920     51.781500\n",
       "1921     58.180361\n",
       "1922     60.925842\n",
       "1923     38.065951\n",
       "Length: 1924, dtype: float64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[features].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 1924, 64)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 962, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 962, 64)           28736     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 481, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 481, 64)           28736     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 240, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 240, 64)           28736     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_19 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 20)                660       \n",
      "=================================================================\n",
      "Total params: 89,460\n",
      "Trainable params: 89,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "model.add(keras.Input(shape=(X_train.shape[1]-2,1)))\n",
    "# model.add(Embedding(63440, embed_dim, trainable=False))\n",
    "# Conv1D(filters=1, kernel_size=10 ,strides=10,     \n",
    "#                   ,kernel_initializer= 'uniform',      \n",
    "#                   activation= 'relu')\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=6, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "# y_train = np.asarray(y_train).reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 - 7s - loss: 0.3716 - accuracy: 0.0921 - val_loss: 0.2726 - val_accuracy: 0.0972\n",
      "Epoch 2/50\n",
      "21/21 - 7s - loss: 0.2809 - accuracy: 0.1385 - val_loss: 0.2608 - val_accuracy: 0.2014\n",
      "Epoch 3/50\n",
      "21/21 - 7s - loss: 0.2666 - accuracy: 0.1486 - val_loss: 0.2526 - val_accuracy: 0.1944\n",
      "Epoch 4/50\n",
      "21/21 - 7s - loss: 0.2535 - accuracy: 0.1827 - val_loss: 0.2453 - val_accuracy: 0.2014\n",
      "Epoch 5/50\n",
      "21/21 - 7s - loss: 0.2457 - accuracy: 0.1587 - val_loss: 0.2369 - val_accuracy: 0.2014\n",
      "Epoch 6/50\n",
      "21/21 - 7s - loss: 0.2349 - accuracy: 0.1950 - val_loss: 0.2323 - val_accuracy: 0.1944\n",
      "Epoch 7/50\n",
      "21/21 - 7s - loss: 0.2254 - accuracy: 0.1803 - val_loss: 0.2194 - val_accuracy: 0.2014\n",
      "Epoch 8/50\n",
      "21/21 - 7s - loss: 0.2131 - accuracy: 0.1749 - val_loss: 0.2080 - val_accuracy: 0.2014\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train[features].values.reshape(X_train[features].shape[0],X_train[features].shape[1],1 ), y_train_b, batch_size=batch_size, epochs=num_epochs,shuffle = True, callbacks=callbacks_list, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train.Label.unique().tolist())\n",
    "# word_seq_train.shape\n",
    "# labels\n",
    "a = model.predict(word_seq_test)\n",
    "# word_seq_train.max()\n",
    "# word_seq_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_seq_train.shape\n",
    "# a[0]\n",
    "word_seq_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits\n",
    "y_train_b[0]*a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "# train.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator.translate('안녕하세요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_train = pd.read_csv('../Translated/cleaned/train.csv')\n",
    "# test.char_len.max()\n",
    "# train.Text.to_csv('../Translated/train1.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_train.Text.to_csv('../Translated/train1.csv',index = False)\n",
    "# plt.figure()\n",
    "# plt.hist(test['char_len'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1436*2\n",
    "train['char_len'] = train['Text'].apply(lambda x: len(str(x)))\n",
    "test['char_len'] = test['Text'].apply(lambda x: len(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=5000, stop_words='english')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(max_features=5000,stop_words='english')\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dtm = vect.fit_transform(train.Text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_dtm\n",
    "test_X_dtm = vect.transform(test.Text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.texts_to_sequences(train.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49694"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.texts_to_sequences(test.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X_train[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-225-4a5761a4f421>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "X_train[features].reshape(-1,X_train[features].shape[0],X_train[features].shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
