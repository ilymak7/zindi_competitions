{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "5afdb577-5e12-4aae-ab95-697ce2fcb3b2",
    "_uuid": "a6e926edfd31362ac957d8a2a76f57fc78ebc433"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import functools\n",
    "import operator\n",
    "import fasttext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, InputSpec, GlobalMaxPool1D, GlobalAvgPool1D, Masking\n",
    "from keras.layers import LSTM, GRU, Bidirectional, Dropout, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers import concatenate\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras import initializers, regularizers, constraints\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# from textacy.preprocess import preprocess_text\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textacy\n",
    "# import textacy\n",
    "# textacy\n",
    "# textacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(textacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5aa68170-5233-4fdc-b530-b920151b35e1",
    "_uuid": "e0b1d5a7bf690a70c1a7de1cfbbf0b69780843c9"
   },
   "source": [
    "# Embedding Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "1a665f5c-6166-400b-bddb-4d08e57b2738",
    "_uuid": "03ca2d3e503ba8d3f46a67849a1b94578ae938bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Vectors\\glove.840B.300d.pkl\n",
      "../Vectors\\wiki.ny.bin\n",
      "../Vectors\\en-ny.xml.gz\n",
      "../Vectors\\wiki.ny.vec\n",
      "../Vectors\\en-nya.xml.gz\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(os.scandir('../Vectors'), key=lambda x: x.stat().st_size, reverse=True):\n",
    "    print(i.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Translated/cleaned/train.csv')\n",
    "test = pd.read_csv('../Translated/cleaned/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "72483c23-8145-4860-b8d1-b9d5cf758404",
    "_uuid": "5dbc6c0165cba74a63b9b31906ab832b0d4a1779"
   },
   "outputs": [],
   "source": [
    "max_features = 60000\n",
    "maxlen = 5000\n",
    "embed_size = 300\n",
    "\n",
    "file_path = \"weights_base.best.hdf5\"\n",
    "emb_file = '../Vectors/wiki.ny.bin'\n",
    "unused = set([])\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(reduce_len=True)\n",
    "lem = WordNetLemmatizer()\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "other_stop_w = pd.read_csv('words_shared_by_all.csv')\n",
    "list_classes = train.Label.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "4d198ec4-6faf-4960-aa09-3d9013ca4a19",
    "_uuid": "1a1cc2a5a9c0fb8cb7125f450882ccddf87b8ea2"
   },
   "outputs": [],
   "source": [
    "CONTEXT_DIM = 100\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, regularizer=regularizers.l2(1e-10), **kwargs):\n",
    "        self.regularizer = regularizer\n",
    "        self.supports_masking = True\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3        \n",
    "        self.W = self.add_weight(name='W',\n",
    "                                 shape=(input_shape[-1], CONTEXT_DIM),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.b = self.add_weight(name='b',\n",
    "                                 shape=(CONTEXT_DIM,),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.u = self.add_weight(name='u',\n",
    "                                 shape=(CONTEXT_DIM,),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True, \n",
    "                                 regularizer=self.regularizer)        \n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x, dim):\n",
    "        \"\"\"Computes softmax along a specified dim. Keras currently lacks this feature.\n",
    "        \"\"\"\n",
    "        if K.backend() == 'tensorflow':\n",
    "            import tensorflow as tf\n",
    "            return tf.nn.softmax(x, dim)\n",
    "        elif K.backend() == 'theano':\n",
    "            # Theano cannot softmax along an arbitrary dim.\n",
    "            # So, we will shuffle `dim` to -1 and un-shuffle after softmax.\n",
    "            perm = np.arange(K.ndim(x))\n",
    "            perm[dim], perm[-1] = perm[-1], perm[dim]\n",
    "            x_perm = K.permute_dimensions(x, perm)\n",
    "            output = K.softmax(x_perm)\n",
    "\n",
    "            # Permute back\n",
    "            perm[dim], perm[-1] = perm[-1], perm[dim]\n",
    "            output = K.permute_dimensions(x, output)\n",
    "            return output\n",
    "        else:\n",
    "            raise ValueError(\"Backend '{}' not supported\".format(K.backend()))\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        ut = K.tanh(K.bias_add(K.dot(x, self.W), self.b)) * self.u\n",
    "\n",
    "        # Collapse `attention_dims` to 1. This indicates the weight for each time_step.\n",
    "        ut = K.sum(ut, axis=-1, keepdims=True)\n",
    "\n",
    "        # Convert those weights into a distribution but along time axis.\n",
    "        # i.e., sum of alphas along `time_steps` axis should be 1.\n",
    "        self.at = self.softmax(ut, dim=1)\n",
    "        if mask is not None:\n",
    "            self.at *= K.cast(K.expand_dims(mask, -1), K.floatx())\n",
    "\n",
    "        # Weighted sum along `time_steps` axis.\n",
    "        return K.sum(x * self.at, axis=-2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "fffffe3c-8df6-4d6e-9f7f-ca176ee22cd0",
    "_uuid": "fae26f773b7699a6c5347635a42b793f78aaed43"
   },
   "outputs": [],
   "source": [
    "def create_embedding(emb_file, word_index):\n",
    "    if emb_file.endswith('bin'):\n",
    "        embeddings_index = fasttext.load_model(emb_file)\n",
    "    else:\n",
    "        embeddings_index = pd.read_table(emb_file,\n",
    "                                         sep=\" \",\n",
    "                                         index_col=0,\n",
    "                                         header=None,\n",
    "                                         quoting=csv.QUOTE_NONE,\n",
    "                                         usecols=range(embed_size + 1),\n",
    "                                         dtype={h: np.float32 for h in range(1, embed_size + 1)},\n",
    "                                         engine='c',\n",
    "        )\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    # Initialize Random Matrix\n",
    "    if emb_file.endswith('bin'):\n",
    "        mean, std = 0.007565171, 0.29283202\n",
    "    else:\n",
    "        mean, std = embeddings_index.values.mean(), embeddings_index.values.std()\n",
    "\n",
    "    embedding_matrix = np.random.normal(mean, std, (nb_words, embed_size))\n",
    "\n",
    "    with tqdm(total=nb_words, desc='Embeddings', unit=' words') as pbar:\n",
    "        for word, i in word_index.items():\n",
    "            if i >= nb_words:\n",
    "                continue\n",
    "            if emb_file.endswith('bin'):\n",
    "                if embeddings_index.get_word_id(word) != -1:\n",
    "                    embedding_matrix[i] = embeddings_index.get_word_vector(word).astype(np.float32)\n",
    "                    pbar.update()\n",
    "            else:\n",
    "                if word in embeddings_index.index:\n",
    "                    embedding_matrix[i] = embeddings_index.loc[word].values\n",
    "                    pbar.update()\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_embedding(emb_file):\n",
    "    return Embedding(min(max_features, len(tokenizer.word_index)), embed_size,\n",
    "                     weights=[create_embedding(emb_file, tokenizer.word_index)],\n",
    "                     input_length=maxlen,\n",
    "                     trainable=False\n",
    "    )\n",
    "\n",
    "def tokenize(s):\n",
    "    return re.sub('([{}“”¨«»®´·º½¾¿¡§£₤‘’])'.format(string.punctuation), r' \\1 ', s).split()\n",
    "\n",
    "def replace_numbers(s):\n",
    "    dictionary = {\n",
    "        '&': ' and ',\n",
    "        '@': ' at ',\n",
    "        '0': ' zero ',\n",
    "        '1': ' one ',\n",
    "        '2': ' two ',\n",
    "        '3': ' three ',\n",
    "        '4': ' four ',\n",
    "        '5': ' five ',\n",
    "        '6': ' six ',\n",
    "        '7': ' seven ',\n",
    "        '8': ' eight ',\n",
    "        '9': ' nine ',\n",
    "    }\n",
    "    for k, v in dictionary.items():\n",
    "        s = s.replace(k, v)\n",
    "    return s\n",
    "\n",
    "def text_cleanup(s, remove_unused=True):\n",
    "    \"\"\"\n",
    "    This function receives ss and returns clean word-list\n",
    "    \"\"\"\n",
    "    # Remove leaky elements like ip, user, numbers, newlines\n",
    "    s = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"_ip_\", s)\n",
    "    s = re.sub(\"\\[\\[.*\\]\", \"\", s)\n",
    "    s = re.sub('\\n', ' ', s)\n",
    "    s = replace_numbers(s)\n",
    "\n",
    "    # Split the sentences into words\n",
    "    s = tweet_tokenizer.tokenize(s)\n",
    "\n",
    "    # Lemmatize\n",
    "    s = [lem.lemmatize(word, \"v\") for word in s]\n",
    "\n",
    "    # Remove Stopwords\n",
    "    s = ' '.join([w for w in s if not w in eng_stopwords])\n",
    "    \n",
    "#     s = preprocess_text(s, fix_unicode=True,\n",
    "#                            lowercase=True,\n",
    "#                            no_currency_symbols=True,\n",
    "#                            transliterate=True,\n",
    "#                            no_urls=True,\n",
    "#                            no_emails=True,\n",
    "#                            no_contractions=True,\n",
    "#                            no_phone_numbers=True,\n",
    "#                            no_punct=True).strip()\n",
    "    \n",
    "    if remove_unused:\n",
    "        s = ' '.join([i for i in s.split() if i not in unused])\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train.Label)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ARTS AND CRAFTS', 'CULTURE', 'ECONOMY', 'EDUCATION', 'FARMING',\n",
       "       'FLOODING', 'HEALTH', 'LAW/ORDER', 'LOCALCHIEFS', 'MUSIC',\n",
       "       'OPINION/ESSAY', 'POLITICS', 'RELATIONSHIPS', 'RELIGION', 'SOCIAL',\n",
       "       'SOCIAL ISSUES', 'SPORTS', 'TRANSPORT', 'WILDLIFE/ENVIRONMENT',\n",
       "       'WITCHCRAFT'], dtype='<U20')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(lb)\n",
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "d1c534c1-446d-4e2e-82b6-490f01cbeb1d",
    "_uuid": "68cf14532fb4887f7ca908722f423a1befc51d63"
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv').sample(frac=1)\n",
    "# test  = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n",
    "\n",
    "train['Text'] = train.Text.fillna(\"_na_\").apply(text_cleanup)\n",
    "test['Text']  = test.Text.fillna(\"_na_\").apply(text_cleanup)\n",
    "\n",
    "list_sentences_train = train.Text.tolist()\n",
    "list_sentences_test  = test.Text.tolist()\n",
    "\n",
    "y = y_train\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
    "tokenizer.fit_on_texts(list_sentences_train + list_sentences_test)\n",
    "\n",
    "X_t  = sequence.pad_sequences(tokenizer.texts_to_sequences(list_sentences_train), maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(tokenizer.texts_to_sequences(list_sentences_test),  maxlen=maxlen)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_t, y, test_size=0.1, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "375b51da-1e7e-4ded-9cac-268477c7e7ef",
    "_uuid": "c961648e9c1fd511fdc7ce34fcb9465300bc6581"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings:   2%|█                                                          | 1076/60000 [00:00<00:09, 5935.68 words/s]\n"
     ]
    }
   ],
   "source": [
    "embedding = get_embedding(emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "9d6fa6c1-47db-45d7-ad20-324c9c1adaf9",
    "_uuid": "563bce691e753f2f40d8692f68412b5844548ec0"
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "\n",
    "    def __init__(self, verbose=True):\n",
    "        super(RocAucEvaluation, self).__init__()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs   = logs or {}\n",
    "        x_val  = self.validation_data[0]\n",
    "        y_val  = self.validation_data[1]\n",
    "        y_pred = self.model.predict(x_val, verbose=0)\n",
    "        try:\n",
    "            current  = roc_auc_score(y_val, y_pred)\n",
    "        except ValueError:\n",
    "            # Bug in AUC metric when TP = 100%\n",
    "            # https://github.com/scikit-learn/scikit-learn/issues/1257\n",
    "            current = 1.0\n",
    "\n",
    "        logs['roc_auc'] = current\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"val_roc_auc: {:.6f}\".format(current))\n",
    "\n",
    "def create_model(embedding=None):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "\n",
    "    x = embedding(inp)\n",
    "    x = Bidirectional(GRU(512, return_sequences=True))(x)\n",
    "    x = Attention()(x)\n",
    "    x = Dense(20, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3, clipnorm=4), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "ecadb92c-20f6-4726-82e3-c11bd7dff535",
    "_uuid": "b83fb812dc4c15f3e818e5b5fb0a38dcb8b0c36e"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "model = create_model(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "6955df70-b504-48d2-81b2-393d56d9a49b",
    "_uuid": "ebed2370d2a3f909d71a99ed243d7ddbc72399f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 5000, 300)         18000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 5000, 1024)        2500608   \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 1024)              102600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                20500     \n",
      "=================================================================\n",
      "Total params: 20,623,708\n",
      "Trainable params: 2,623,708\n",
      "Non-trainable params: 18,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f5ebda59-5eaf-4ae3-a655-debacce9c73d",
    "_uuid": "ffca917b8c73557477a27533cb3a1ca75c4f428d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 7\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=[\n",
    "#               RocAucEvaluation(verbose=True),\n",
    "              ModelCheckpoint(file_path,    monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "              EarlyStopping(patience=10,    monitor=\"val_accuracy\", mode=\"max\"),\n",
    "              ReduceLROnPlateau(patience=0, monitor='val_accuracy', mode='max', cooldown=2, min_lr=1e-7, factor=0.3)\n",
    "          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1292, 250)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_val\n",
    "# X_train.shape\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(max_features=5000,stop_words=stopw)\n",
    "vect\n",
    "vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bec86d64-6479-41a0-878f-d278149a7424",
    "_uuid": "ec45d87d15ec16f126fdae0dd249664e4638b74a"
   },
   "outputs": [],
   "source": [
    "# model.load_weights(file_path)\n",
    "\n",
    "# sample_submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n",
    "# sample_submission[list_classes] = model.predict(X_te, verbose=True)\n",
    "# sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01a68f6c-bd45-4026-b0a4-3d7425d2119e",
    "_uuid": "f1340e04736c2ec59b0eeb22759571334a2b18f5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
