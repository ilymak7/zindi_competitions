{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d5221cf58859cd6496cf4294414a3bc37d4c95f"
   },
   "source": [
    "In this kernel, I have implemented the encoder part of the transformer architecture as mentioned in the famous paper: Attention is all you need.(https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "Many of other codes are adopted from other kernels. For example, loading the embeddings,  load the training and test data and preprocessing, etc. I really appreciate their contributions.\n",
    "\n",
    "p.s. When I run this locally, I get validation f1-score around 0.688.\n",
    "\n",
    "Happy transforming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a947373c706a15ed71a686d92703b9677561894"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.layers import BatchNormalization, InputSpec, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, activations\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b37e0d09f42f5bc5ad73a366580f6b778c9aad5a"
   },
   "source": [
    "## Some pre-configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "c7498e1cc6e3dd7e7c58f24e10fd5ad1b06b4489"
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 45000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 400 # max number of words in a question to use\n",
    "n_heads = 4 # Number of heads as in Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "05385c91a5e5603c346bfe53010aa4cb0f3ddd4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1436, 3)\n",
      "Test shape :  (620, 2)\n"
     ]
    }
   ],
   "source": [
    "# def load_and_prec():\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train_df['Label'])\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= lb.classes_)\n",
    "train_df = pd.concat([train_df, y_train], axis = 1)\n",
    "cols_target = train_df.Label.unique().tolist()\n",
    "\n",
    "## split to train and val\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=0,shuffle = True) # hahaha\n",
    "# train_X, val_X, train_y , val_y = train_test_split(train_df, train_df[cols_target], test_size=0.1, random_state = 0,stratify = train_df['Label'])\n",
    "\n",
    "# trn_idx = train_y.index.tolist()\n",
    "# val_idx = val_y.index.tolist()\n",
    "\n",
    "\n",
    "## fill up the missing values\n",
    "# train_X = train_X[\"Text\"].fillna(\"_##_\").values\n",
    "# val_X = val_X[\"Text\"].fillna(\"_##_\").values\n",
    "# test_X = test_df[\"Text\"].fillna(\"_##_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_df.Text)\n",
    "# train_X = tokenizer.texts_to_sequences(train_X)\n",
    "# val_X = tokenizer.texts_to_sequences(val_X)\n",
    "# test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "# ## Pad the sentences \n",
    "# train_X = pad_sequences(train_X, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "# val_X = pad_sequences(val_X, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "# test_X = pad_sequences(test_X, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "\n",
    "# ## Get the target values\n",
    "# train_y = train_y.values\n",
    "# val_y = val_y.values  \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "#shuffling the data\n",
    "# np.random.seed(2018)\n",
    "# trn_idx = np.random.permutation(len(train_X))\n",
    "# val_idx = np.random.permutation(len(val_X))\n",
    "\n",
    "# train_X = train_X[trn_idx]\n",
    "# val_X = val_X[val_idx]\n",
    "# train_y = train_y[trn_idx]\n",
    "# val_y = val_y[val_idx]    \n",
    "\n",
    "#     return train_X, val_X, test_X, train_y, val_y, tokenizer.word_index,val_idx , trn_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20682431e22eab3cbf634777cb0d2bc2730ab754"
   },
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "cf6dc22b3b2a9f2ec94f70e2aa5dfe36f6f142d3"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
    "    embed_size = all_embs.shape[1]\n",
    "    print(emb_mean,emb_std,\"para\")\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a35ee76c0f926bcdf817fcb91dbd20ee90007f06"
   },
   "source": [
    "## Scaled Dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "92c050cb313508d5c88b288dd1561493bcfacbed"
   },
   "outputs": [],
   "source": [
    "class DotProdSelfAttention(Layer):\n",
    "    \"\"\"The self-attention layer as in 'Attention is all you need'.\n",
    "    paper reference: https://arxiv.org/abs/1706.03762\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=False,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(DotProdSelfAttention, self).__init__(*kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        input_dim = input_shape[-1]\n",
    "        # We assume the output-dim of Q, K, V are the same\n",
    "        self.kernels = dict.fromkeys(['Q', 'K', 'V'])\n",
    "        for key, _ in self.kernels.items():\n",
    "            self.kernels[key] = self.add_weight(shape=(input_dim, self.units),\n",
    "                                                initializer=self.kernel_initializer,\n",
    "                                                name='kernel_{}'.format(key),\n",
    "                                                regularizer=self.kernel_regularizer,\n",
    "                                                constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            raise NotImplementedError\n",
    "        super(DotProdSelfAttention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        Q = K.dot(x, self.kernels['Q'])\n",
    "        K_mat = K.dot(x, self.kernels['K'])\n",
    "        V = K.dot(x, self.kernels['V'])\n",
    "        attention = K.batch_dot(Q, K.permute_dimensions(K_mat, [0, 2, 1]))\n",
    "        d_k = K.constant(self.units, dtype=K.floatx())\n",
    "        attention = attention / K.sqrt(d_k)\n",
    "        attention = K.batch_dot(K.softmax(attention, axis=-1), V)\n",
    "        return attention\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        assert input_shape[-1]\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "      \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d931bd69e53756c2f8aa8cf63d07d4c7eb02a8d9"
   },
   "source": [
    "## The Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "399ddb0b7367ac0f6bd4121396b24ac3a6d1edfe"
   },
   "outputs": [],
   "source": [
    "def encoder(input_tensor):\n",
    "    \"\"\"One encoder as in Attention Is All You Need\n",
    "    \"\"\"\n",
    "    # Sub-layer 1\n",
    "    # Multi-Head Attention\n",
    "    multiheads = []\n",
    "    d_v = embed_size // n_heads\n",
    "    for i in range(n_heads):\n",
    "        multiheads.append(DotProdSelfAttention(d_v)(input_tensor))\n",
    "    multiheads = concatenate(multiheads, axis=-1)\n",
    "    multiheads = Dense(embed_size)(multiheads)\n",
    "    multiheads = Dropout(0.1)(multiheads)\n",
    "    \n",
    "    # Residual Connection\n",
    "    res_con = add([input_tensor, multiheads])\n",
    "    # Didn't use layer normalization, use Batch Normalization instead here\n",
    "    res_con = BatchNormalization(axis=-1)(res_con)\n",
    "    \n",
    "    # Sub-layer 2\n",
    "    # 2 Feed forward layer\n",
    "    ff1 = Dense(64, activation='relu')(res_con)\n",
    "    ff2 = Dense(embed_size)(ff1)\n",
    "    output = add([res_con, ff2])\n",
    "    output = BatchNormalization(axis=-1)(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8740be2622e8195ed3f0c7d9568a06ddb64cd98b"
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "8bd0cd11b96080f965195da697ba34c865f5b7e2"
   },
   "outputs": [],
   "source": [
    "# https://github.com/kpot/keras-transformer/blob/master/keras_transformer/position.py\n",
    "def positional_signal(hidden_size: int, length: int,\n",
    "                      min_timescale: float = 1.0, max_timescale: float = 1e4):\n",
    "    \"\"\"\n",
    "    Helper function, constructing basic positional encoding.\n",
    "    The code is partially based on implementation from Tensor2Tensor library\n",
    "    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n",
    "    \"\"\"\n",
    "\n",
    "    if hidden_size % 2 != 0:\n",
    "        raise ValueError(\n",
    "            f\"The hidden dimension of the model must be divisible by 2.\"\n",
    "            f\"Currently it is {hidden_size}\")\n",
    "    position = K.arange(0, length, dtype=K.floatx())\n",
    "    num_timescales = hidden_size // 2\n",
    "    log_timescale_increment = K.constant(\n",
    "        (np.log(float(max_timescale) / float(min_timescale)) /\n",
    "         (num_timescales - 1)),\n",
    "        dtype=K.floatx())\n",
    "    inv_timescales = (\n",
    "            min_timescale *\n",
    "            K.exp(K.arange(num_timescales, dtype=K.floatx()) *\n",
    "                  -log_timescale_increment))\n",
    "    scaled_time = K.expand_dims(position, 1) * K.expand_dims(inv_timescales, 0)\n",
    "    signal = K.concatenate([K.sin(scaled_time), K.cos(scaled_time)], axis=1)\n",
    "    return K.expand_dims(signal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "309135be6416acde8d5088af910eee3597e25d4e"
   },
   "outputs": [],
   "source": [
    "# https://github.com/kpot/keras-transformer/blob/master/keras_transformer/position.py\n",
    "class AddPositionalEncoding(Layer):\n",
    "    \"\"\"\n",
    "    Injects positional encoding signal described in section 3.5 of the original\n",
    "    paper \"Attention is all you need\". Also a base class for more complex\n",
    "    coordinate encoding described in \"Universal Transformers\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4, **kwargs):\n",
    "        self.min_timescale = min_timescale\n",
    "        self.max_timescale = max_timescale\n",
    "        self.signal = None\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['min_timescale'] = self.min_timescale\n",
    "        config['max_timescale'] = self.max_timescale\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, length, hidden_size = input_shape\n",
    "        self.signal = positional_signal(\n",
    "            hidden_size, length, self.min_timescale, self.max_timescale)\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return inputs + self.signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "312bd7b8472de749134273fead7f939a234c551c"
   },
   "source": [
    "## Transformer Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "235ae9ecb6873475900b1ad88e3a07a161195370"
   },
   "outputs": [],
   "source": [
    "def model_transformer( n_encoder=3):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, trainable=True)(inp)\n",
    "    # Add positional encoding\n",
    "    x = AddPositionalEncoding()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    for i in range(n_encoder):\n",
    "        x = encoder(x)\n",
    "    # These are my own experiments\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "#     conc = Dense(512, activation=\"relu\")(conc)\n",
    "#     conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(20, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.layers.recurrent_v2.LSTM"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "e2f42600ff37e6c286215562afc11a1be28f0981"
   },
   "outputs": [],
   "source": [
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class BaseDataGenerator(Sequence):\n",
    "    \"\"\"A data generator\"\"\"\n",
    "    def __init__(self, list_IDs, batch_size=64, shuffle=True):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"number of steps in one epoch\"\"\"\n",
    "        # Here is the trick\n",
    "        return len(self.list_IDs) // (self.batch_size * 2**2)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size: (index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        X = train_X[list_IDs_temp, :]\n",
    "        y = train_y[list_IDs_temp]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee0611a48264188ec3063bc6c6ce221eb3724d8e"
   },
   "source": [
    "### Train and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c9d6121293c82701f3c07ae00f396564d8aa2c9"
   },
   "source": [
    "Here I used early stopping and model checkpoint to load the best_val model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "5f90446889d864f0a318d305eca2d3a04d1baa55"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "def train_pred(n_encoder = 1, epochs=2):\n",
    "    # learning schedule callback\n",
    "#     loss_history = LossHistory()\n",
    "#     lrate = BatchLRScheduler(step_decay)\n",
    "#     callbacks_list = [loss_history, lrate]\n",
    "#     es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "#     model_path = 'keras_models.h5'\n",
    "#     mc = ModelCheckpoint(filepath=model_path, monitor='val_loss', save_best_only=True)\n",
    "#     callbacks = [es, mc]\n",
    "#     train_generator = BaseDataGenerator(list(np.arange(train_X.shape[0])), batch_size=512)\n",
    "#     model.fit_generator(train_generator,\n",
    "#                         epochs=epochs,\n",
    "#                         validation_data=(val_X, val_y),)\n",
    "#                         callbacks=callbacks)\n",
    "#     model = load_model(model_path)\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=0)\n",
    "    models, preds, scores = [], [],[]\n",
    "#     vectorizer = vect(max_df = 0.5)\n",
    "    for train, test in skf.split(train_df.Text, train_df.Label):\n",
    "#     print(train, test)\n",
    "#     clf = LogisticRegression(penalty='l1')\n",
    "#         clf.fit(vectorizer.transform(), data_train.Label.loc[data_train.index.intersection(train)])\n",
    "#         K.clear_session()\n",
    "#         clf = build_base_model()\n",
    "        model = model_transformer(n_encoder=n_encoder)\n",
    "        X_train = train_df.Text.loc[train_df.index.intersection(train)]\n",
    "        X_val = train_df.Text.loc[train_df.index.intersection(test)]\n",
    "        y_train = train_df[cols_target].loc[train_df.index.intersection(train)]\n",
    "        y_val = train_df[cols_target].loc[train_df.index.intersection(test)]\n",
    "        X_train = tokenizer.texts_to_sequences(X_train)\n",
    "        X_val = tokenizer.texts_to_sequences(X_val)\n",
    "        X_test = tokenizer.texts_to_sequences(test_df.Text)\n",
    "\n",
    "        ## Pad the sentences \n",
    "        X_train = pad_sequences(X_train, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        X_val = pad_sequences(X_val, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        X_test = pad_sequences(X_test, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "\n",
    "#         X_test = vect.transform(test_df.Text).toarray()\n",
    "        model.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                   validation_data = (X_val,y_val),\n",
    "                   callbacks=[\n",
    "#               RocAucEvaluation(verbose=True),\n",
    "              ModelCheckpoint(file_path,    monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "              EarlyStopping(patience=10,    monitor=\"val_accuracy\", mode=\"max\"),\n",
    "              ReduceLROnPlateau(patience=4, monitor='val_accuracy', mode='max', cooldown=2, min_lr=1e-7, factor=0.3)])\n",
    "        preds.append(model.predict(X_test))\n",
    "        models.append(model)\n",
    "        scores.append(model.evaluate(X_val,y_val))\n",
    "#         coefs.append(clf.coef_[0])\n",
    "#         clf.fit(X_train, y_train)\n",
    "#     train_time = time() - t0\n",
    "#     print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "#     t0 = time()\n",
    "#     pred = clf.predict(X_test)\n",
    "#     test_time = time() - t0\n",
    "#     print(\"test time:  %0.3fs\" % test_time)\n",
    "    pred = np.mean(preds,axis = 0)\n",
    "#     model.fit(train_X, train_y, batch_size=64,\n",
    "#               epochs=epochs,\n",
    "#               validation_data=(val_X, val_y),)\n",
    "\n",
    "#     pred_val_y = model.predict([val_X], batch_size=64, verbose=0)\n",
    "#     pred_test_y = model.predict([test_X], batch_size=64, verbose=0)\n",
    "    return models, preds, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9b632c9bea6c5e8c7e111fc97474fc9d8b099c4"
   },
   "source": [
    "### Main part: load, train, pred and blend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "6ddf2ae0c374759ef040db80346faaf609850e58"
   },
   "outputs": [],
   "source": [
    "# train_X, val_X, test_X, train_y, val_y, word_index,val_idx,trn_idx = load_and_prec()\n",
    "vocab = []\n",
    "for w,k in word_index.items():\n",
    "    vocab.append(w)\n",
    "    if k >= max_features:\n",
    "        break\n",
    "# embedding_matrix_1 = load_glove(word_index)\n",
    "# embedding_matrix_2 = load_fasttext(word_index)\n",
    "# embedding_matrix_3 = load_para(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ffc83124a9f220b31e698b922cad56d59f8c83e5"
   },
   "source": [
    "### Create New Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "622e0a2f777d15a9f74b3d1d56a0e5ad60ee90bc"
   },
   "outputs": [],
   "source": [
    "## Simple average: http://aclweb.org/anthology/N18-2031\n",
    "\n",
    "# We have presented an argument for averaging as\n",
    "# a valid meta-embedding technique, and found experimental\n",
    "# performance to be close to, or in some cases \n",
    "# better than that of concatenation, with the\n",
    "# additional benefit of reduced dimensionality  \n",
    "\n",
    "\n",
    "## Unweighted DME in https://arxiv.org/pdf/1804.07983.pdf\n",
    "\n",
    "# “The downside of concatenating embeddings and \n",
    "#  giving that as input to an RNN encoder, however,\n",
    "#  is that the network then quickly becomes inefficient\n",
    "#  as we combine more and more embeddings.”\n",
    "  \n",
    "# embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2, embedding_matrix_3], axis = 0)\n",
    "# embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_3], axis = 0)\n",
    "# np.shape(embedding_matrix)\n",
    "# model.evaluate(val_X,val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52627f8490364ed29f81e574c49af644726701e1"
   },
   "source": [
    "## Train and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3270c045c747a7c5da32704e6db44c18f470646"
   },
   "source": [
    "Here I am experimenting with 2 encoders, it's not guaranteed to be optimal, you can try out other numbers. Notice that I used epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "573d8e65c007aefd1fb4a0deef16e73894327486"
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "# outputs[0][1]\n",
    "# type(train_X[0][0])\n",
    "# val_X\n",
    "# train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(train_X,train_y)\n",
    "# train_X.shape\n",
    "# train_idx\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback, ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,cross_val_score,train_test_split,StratifiedShuffleSplit\n",
    "\n",
    "file_path = \"weights_trans.best.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "091d7915c8c00d088d60c09c306298950cd2afe6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/42\n",
      "36/36 [==============================] - 13s 362ms/step - loss: 2.8399 - accuracy: 0.1603 - val_loss: 2.6853 - val_accuracy: 0.1944\n",
      "Epoch 2/42\n",
      "36/36 [==============================] - 13s 372ms/step - loss: 2.5870 - accuracy: 0.2125 - val_loss: 2.5900 - val_accuracy: 0.2465\n",
      "Epoch 3/42\n",
      "36/36 [==============================] - 13s 352ms/step - loss: 2.4749 - accuracy: 0.2474 - val_loss: 2.5624 - val_accuracy: 0.1736\n",
      "Epoch 4/42\n",
      "36/36 [==============================] - 14s 378ms/step - loss: 2.3890 - accuracy: 0.2735 - val_loss: 2.4810 - val_accuracy: 0.2188\n",
      "Epoch 5/42\n",
      "36/36 [==============================] - 13s 360ms/step - loss: 2.2646 - accuracy: 0.2770 - val_loss: 2.4355 - val_accuracy: 0.2743\n",
      "Epoch 6/42\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 2.1754 - accuracy: 0.3754 - val_loss: 2.3645 - val_accuracy: 0.2535\n",
      "Epoch 7/42\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 2.0468 - accuracy: 0.4094 - val_loss: 2.2982 - val_accuracy: 0.3438\n",
      "Epoch 8/42\n",
      "36/36 [==============================] - 12s 329ms/step - loss: 1.9034 - accuracy: 0.5017 - val_loss: 2.2187 - val_accuracy: 0.3299\n",
      "Epoch 9/42\n",
      "36/36 [==============================] - 13s 357ms/step - loss: 1.7869 - accuracy: 0.5557 - val_loss: 2.1750 - val_accuracy: 0.4306\n",
      "Epoch 10/42\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 1.6566 - accuracy: 0.5897 - val_loss: 2.1268 - val_accuracy: 0.3264\n",
      "Epoch 11/42\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 1.5296 - accuracy: 0.6655 - val_loss: 2.0415 - val_accuracy: 0.4410\n",
      "Epoch 12/42\n",
      "36/36 [==============================] - 13s 365ms/step - loss: 1.4010 - accuracy: 0.7021 - val_loss: 2.0207 - val_accuracy: 0.4757\n",
      "Epoch 13/42\n",
      "36/36 [==============================] - 13s 362ms/step - loss: 1.2812 - accuracy: 0.7474 - val_loss: 1.9669 - val_accuracy: 0.4896\n",
      "Epoch 14/42\n",
      "36/36 [==============================] - 12s 334ms/step - loss: 1.1866 - accuracy: 0.7605 - val_loss: 1.9225 - val_accuracy: 0.4757\n",
      "Epoch 15/42\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 1.0745 - accuracy: 0.7901 - val_loss: 1.8537 - val_accuracy: 0.5556\n",
      "Epoch 16/42\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.9469 - accuracy: 0.8240 - val_loss: 1.8330 - val_accuracy: 0.5417\n",
      "Epoch 17/42\n",
      "36/36 [==============================] - 12s 330ms/step - loss: 0.8724 - accuracy: 0.8502 - val_loss: 1.8050 - val_accuracy: 0.5174\n",
      "Epoch 18/42\n",
      "36/36 [==============================] - 12s 332ms/step - loss: 0.7848 - accuracy: 0.8589 - val_loss: 1.7766 - val_accuracy: 0.5556\n",
      "Epoch 19/42\n",
      "36/36 [==============================] - 12s 334ms/step - loss: 0.7043 - accuracy: 0.8972 - val_loss: 1.7382 - val_accuracy: 0.5208\n",
      "Epoch 20/42\n",
      "36/36 [==============================] - 13s 353ms/step - loss: 0.6267 - accuracy: 0.9059 - val_loss: 1.7153 - val_accuracy: 0.5764\n",
      "Epoch 21/42\n",
      "36/36 [==============================] - 12s 332ms/step - loss: 0.5999 - accuracy: 0.9199 - val_loss: 1.7236 - val_accuracy: 0.5556\n",
      "Epoch 22/42\n",
      "36/36 [==============================] - 12s 334ms/step - loss: 0.5758 - accuracy: 0.9233 - val_loss: 1.6926 - val_accuracy: 0.5660\n",
      "Epoch 23/42\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.5567 - accuracy: 0.9251 - val_loss: 1.6961 - val_accuracy: 0.5625\n",
      "Epoch 24/42\n",
      "36/36 [==============================] - 13s 361ms/step - loss: 0.5344 - accuracy: 0.9355 - val_loss: 1.6882 - val_accuracy: 0.5833\n",
      "Epoch 25/42\n",
      "36/36 [==============================] - 12s 332ms/step - loss: 0.5136 - accuracy: 0.9347 - val_loss: 1.6755 - val_accuracy: 0.5764\n",
      "Epoch 26/42\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 0.5020 - accuracy: 0.9416 - val_loss: 1.6749 - val_accuracy: 0.5660\n",
      "Epoch 27/42\n",
      "36/36 [==============================] - 12s 330ms/step - loss: 0.4805 - accuracy: 0.9347 - val_loss: 1.6736 - val_accuracy: 0.5729\n",
      "Epoch 28/42\n",
      "36/36 [==============================] - 12s 324ms/step - loss: 0.4660 - accuracy: 0.9477 - val_loss: 1.6746 - val_accuracy: 0.5764\n",
      "Epoch 29/42\n",
      "36/36 [==============================] - 11s 312ms/step - loss: 0.4485 - accuracy: 0.9477 - val_loss: 1.6547 - val_accuracy: 0.5833\n",
      "Epoch 30/42\n",
      "36/36 [==============================] - 11s 308ms/step - loss: 0.4417 - accuracy: 0.9469 - val_loss: 1.6518 - val_accuracy: 0.5799\n",
      "Epoch 31/42\n",
      "36/36 [==============================] - 11s 308ms/step - loss: 0.4362 - accuracy: 0.9512 - val_loss: 1.6580 - val_accuracy: 0.5833\n",
      "Epoch 32/42\n",
      "36/36 [==============================] - 11s 315ms/step - loss: 0.4310 - accuracy: 0.9512 - val_loss: 1.6528 - val_accuracy: 0.5833\n",
      "Epoch 33/42\n",
      "36/36 [==============================] - 11s 312ms/step - loss: 0.4286 - accuracy: 0.9503 - val_loss: 1.6469 - val_accuracy: 0.5764\n",
      "Epoch 34/42\n",
      "36/36 [==============================] - 11s 309ms/step - loss: 0.4204 - accuracy: 0.9556 - val_loss: 1.6471 - val_accuracy: 0.5799\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.6471 - accuracy: 0.5799\n",
      "Epoch 1/42\n",
      "36/36 [==============================] - 12s 343ms/step - loss: 2.8897 - accuracy: 0.1375 - val_loss: 2.5689 - val_accuracy: 0.1951\n",
      "Epoch 2/42\n",
      "36/36 [==============================] - 12s 339ms/step - loss: 2.5689 - accuracy: 0.2019 - val_loss: 2.5628 - val_accuracy: 0.2300\n",
      "Epoch 3/42\n",
      "36/36 [==============================] - 11s 314ms/step - loss: 2.5084 - accuracy: 0.2254 - val_loss: 2.5484 - val_accuracy: 0.1638\n",
      "Epoch 4/42\n",
      "36/36 [==============================] - 11s 317ms/step - loss: 2.3915 - accuracy: 0.2550 - val_loss: 2.4785 - val_accuracy: 0.1812\n",
      "Epoch 5/42\n",
      "36/36 [==============================] - 12s 338ms/step - loss: 2.2946 - accuracy: 0.3107 - val_loss: 2.4059 - val_accuracy: 0.2509\n",
      "Epoch 6/42\n",
      "36/36 [==============================] - 12s 344ms/step - loss: 2.1789 - accuracy: 0.3568 - val_loss: 2.3173 - val_accuracy: 0.2962\n",
      "Epoch 7/42\n",
      "36/36 [==============================] - 12s 345ms/step - loss: 2.0710 - accuracy: 0.4064 - val_loss: 2.2552 - val_accuracy: 0.3380\n",
      "Epoch 8/42\n",
      "36/36 [==============================] - 12s 345ms/step - loss: 1.9377 - accuracy: 0.4909 - val_loss: 2.2173 - val_accuracy: 0.3833\n",
      "Epoch 9/42\n",
      "36/36 [==============================] - 12s 340ms/step - loss: 1.8124 - accuracy: 0.5344 - val_loss: 2.1283 - val_accuracy: 0.4077\n",
      "Epoch 10/42\n",
      "36/36 [==============================] - 12s 341ms/step - loss: 1.6972 - accuracy: 0.6092 - val_loss: 2.0648 - val_accuracy: 0.4495\n",
      "Epoch 11/42\n",
      "36/36 [==============================] - 12s 338ms/step - loss: 1.5810 - accuracy: 0.6084 - val_loss: 2.0400 - val_accuracy: 0.4530\n",
      "Epoch 12/42\n",
      "36/36 [==============================] - 12s 342ms/step - loss: 1.4367 - accuracy: 0.7102 - val_loss: 2.0031 - val_accuracy: 0.4564\n",
      "Epoch 13/42\n",
      "36/36 [==============================] - 11s 317ms/step - loss: 1.3196 - accuracy: 0.7406 - val_loss: 1.9486 - val_accuracy: 0.4564\n",
      "Epoch 14/42\n",
      "36/36 [==============================] - 12s 341ms/step - loss: 1.2050 - accuracy: 0.7702 - val_loss: 1.8540 - val_accuracy: 0.4669\n",
      "Epoch 15/42\n",
      "36/36 [==============================] - 12s 341ms/step - loss: 1.0818 - accuracy: 0.7850 - val_loss: 1.8367 - val_accuracy: 0.5331\n",
      "Epoch 16/42\n",
      "36/36 [==============================] - 11s 319ms/step - loss: 0.9884 - accuracy: 0.8190 - val_loss: 1.8202 - val_accuracy: 0.5226\n",
      "Epoch 17/42\n",
      "36/36 [==============================] - 11s 317ms/step - loss: 0.8998 - accuracy: 0.8372 - val_loss: 1.7470 - val_accuracy: 0.4948\n",
      "Epoch 18/42\n",
      "36/36 [==============================] - 12s 341ms/step - loss: 0.7966 - accuracy: 0.8695 - val_loss: 1.7064 - val_accuracy: 0.5540\n",
      "Epoch 19/42\n",
      "36/36 [==============================] - 12s 320ms/step - loss: 0.7254 - accuracy: 0.8851 - val_loss: 1.6876 - val_accuracy: 0.5401\n",
      "Epoch 20/42\n",
      "36/36 [==============================] - 11s 315ms/step - loss: 0.6391 - accuracy: 0.8973 - val_loss: 1.6633 - val_accuracy: 0.5401\n",
      "Epoch 21/42\n",
      "36/36 [==============================] - 12s 339ms/step - loss: 0.5752 - accuracy: 0.9147 - val_loss: 1.6155 - val_accuracy: 0.5679\n",
      "Epoch 22/42\n",
      "36/36 [==============================] - 12s 341ms/step - loss: 0.5002 - accuracy: 0.9295 - val_loss: 1.6214 - val_accuracy: 0.5854\n",
      "Epoch 23/42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 11s 319ms/step - loss: 0.4491 - accuracy: 0.9460 - val_loss: 1.6001 - val_accuracy: 0.5854\n",
      "Epoch 24/42\n",
      "36/36 [==============================] - 11s 315ms/step - loss: 0.4021 - accuracy: 0.9521 - val_loss: 1.5539 - val_accuracy: 0.5610\n",
      "Epoch 25/42\n",
      "36/36 [==============================] - 12s 338ms/step - loss: 0.3557 - accuracy: 0.9617 - val_loss: 1.5384 - val_accuracy: 0.5993\n",
      "Epoch 26/42\n",
      "36/36 [==============================] - 12s 337ms/step - loss: 0.3167 - accuracy: 0.9695 - val_loss: 1.5331 - val_accuracy: 0.6063\n",
      "Epoch 27/42\n",
      "36/36 [==============================] - 11s 313ms/step - loss: 0.2800 - accuracy: 0.9800 - val_loss: 1.5403 - val_accuracy: 0.5679\n",
      "Epoch 28/42\n",
      "36/36 [==============================] - 11s 316ms/step - loss: 0.2452 - accuracy: 0.9843 - val_loss: 1.4939 - val_accuracy: 0.5993\n",
      "Epoch 29/42\n",
      "36/36 [==============================] - 11s 313ms/step - loss: 0.2153 - accuracy: 0.9904 - val_loss: 1.4989 - val_accuracy: 0.6063\n",
      "Epoch 30/42\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.1947 - accuracy: 0.9922 - val_loss: 1.4838 - val_accuracy: 0.6132\n",
      "Epoch 31/42\n",
      "36/36 [==============================] - 11s 318ms/step - loss: 0.1733 - accuracy: 0.9930 - val_loss: 1.4825 - val_accuracy: 0.5993\n",
      "Epoch 32/42\n",
      "36/36 [==============================] - 11s 314ms/step - loss: 0.1550 - accuracy: 0.9939 - val_loss: 1.4669 - val_accuracy: 0.6132\n",
      "Epoch 33/42\n",
      "36/36 [==============================] - 12s 341ms/step - loss: 0.1357 - accuracy: 0.9965 - val_loss: 1.4322 - val_accuracy: 0.6167\n",
      "Epoch 34/42\n",
      "36/36 [==============================] - 11s 316ms/step - loss: 0.1244 - accuracy: 0.9974 - val_loss: 1.4435 - val_accuracy: 0.6098\n",
      "Epoch 35/42\n",
      "36/36 [==============================] - 11s 316ms/step - loss: 0.1115 - accuracy: 0.9983 - val_loss: 1.4438 - val_accuracy: 0.6028\n",
      "Epoch 36/42\n",
      "36/36 [==============================] - 11s 315ms/step - loss: 0.0996 - accuracy: 0.9983 - val_loss: 1.4300 - val_accuracy: 0.6132\n",
      "Epoch 37/42\n",
      "36/36 [==============================] - 12s 340ms/step - loss: 0.0909 - accuracy: 0.9991 - val_loss: 1.4117 - val_accuracy: 0.6237\n",
      "Epoch 38/42\n",
      "36/36 [==============================] - 11s 317ms/step - loss: 0.0823 - accuracy: 1.0000 - val_loss: 1.4189 - val_accuracy: 0.6167\n",
      "Epoch 39/42\n",
      "36/36 [==============================] - 12s 339ms/step - loss: 0.0743 - accuracy: 1.0000 - val_loss: 1.4023 - val_accuracy: 0.6132\n",
      "Epoch 40/42\n",
      "36/36 [==============================] - 12s 329ms/step - loss: 0.0672 - accuracy: 1.0000 - val_loss: 1.4017 - val_accuracy: 0.6237\n",
      "Epoch 41/42\n",
      "36/36 [==============================] - 12s 331ms/step - loss: 0.0612 - accuracy: 1.0000 - val_loss: 1.3897 - val_accuracy: 0.6202\n",
      "Epoch 42/42\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 0.0565 - accuracy: 1.0000 - val_loss: 1.3854 - val_accuracy: 0.6202\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1.3854 - accuracy: 0.6202\n",
      "Epoch 1/42\n",
      "36/36 [==============================] - 13s 358ms/step - loss: 2.6924 - accuracy: 0.1506 - val_loss: 2.5959 - val_accuracy: 0.1951\n",
      "Epoch 2/42\n",
      "36/36 [==============================] - 12s 330ms/step - loss: 2.5703 - accuracy: 0.2037 - val_loss: 2.5201 - val_accuracy: 0.1951\n",
      "Epoch 3/42\n",
      "36/36 [==============================] - 13s 356ms/step - loss: 2.4877 - accuracy: 0.2585 - val_loss: 2.5032 - val_accuracy: 0.2613\n",
      "Epoch 4/42\n",
      "36/36 [==============================] - 12s 335ms/step - loss: 2.3901 - accuracy: 0.2472 - val_loss: 2.4421 - val_accuracy: 0.2300\n",
      "Epoch 5/42\n",
      "36/36 [==============================] - 13s 360ms/step - loss: 2.3217 - accuracy: 0.2863 - val_loss: 2.3713 - val_accuracy: 0.2787\n",
      "Epoch 6/42\n",
      "36/36 [==============================] - 13s 361ms/step - loss: 2.1942 - accuracy: 0.3629 - val_loss: 2.2876 - val_accuracy: 0.3275\n",
      "Epoch 7/42\n",
      "36/36 [==============================] - 13s 356ms/step - loss: 2.0740 - accuracy: 0.4186 - val_loss: 2.2373 - val_accuracy: 0.3798\n",
      "Epoch 8/42\n",
      "36/36 [==============================] - 13s 354ms/step - loss: 1.9349 - accuracy: 0.5022 - val_loss: 2.1658 - val_accuracy: 0.4564\n",
      "Epoch 9/42\n",
      "36/36 [==============================] - 12s 330ms/step - loss: 1.8174 - accuracy: 0.5527 - val_loss: 2.0700 - val_accuracy: 0.4564\n",
      "Epoch 10/42\n",
      "36/36 [==============================] - 13s 351ms/step - loss: 1.6887 - accuracy: 0.6144 - val_loss: 2.0201 - val_accuracy: 0.4669\n",
      "Epoch 11/42\n",
      "36/36 [==============================] - 13s 352ms/step - loss: 1.5580 - accuracy: 0.6440 - val_loss: 1.9500 - val_accuracy: 0.5401\n",
      "Epoch 12/42\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 1.4287 - accuracy: 0.6893 - val_loss: 1.9125 - val_accuracy: 0.5470\n",
      "Epoch 13/42\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 1.2850 - accuracy: 0.7668 - val_loss: 1.8254 - val_accuracy: 0.5296\n",
      "Epoch 14/42\n",
      "36/36 [==============================] - 13s 360ms/step - loss: 1.1815 - accuracy: 0.7824 - val_loss: 1.7935 - val_accuracy: 0.5610\n",
      "Epoch 15/42\n",
      "36/36 [==============================] - 13s 361ms/step - loss: 1.0690 - accuracy: 0.7998 - val_loss: 1.7535 - val_accuracy: 0.5854\n",
      "Epoch 16/42\n",
      "36/36 [==============================] - 13s 361ms/step - loss: 0.9847 - accuracy: 0.8259 - val_loss: 1.7290 - val_accuracy: 0.5993\n",
      "Epoch 17/42\n",
      "36/36 [==============================] - 12s 335ms/step - loss: 0.9017 - accuracy: 0.8364 - val_loss: 1.6504 - val_accuracy: 0.5679\n",
      "Epoch 18/42\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 0.7987 - accuracy: 0.8642 - val_loss: 1.6312 - val_accuracy: 0.6202\n",
      "Epoch 19/42\n",
      "36/36 [==============================] - 12s 336ms/step - loss: 0.7055 - accuracy: 0.8903 - val_loss: 1.5892 - val_accuracy: 0.5854\n",
      "Epoch 20/42\n",
      "36/36 [==============================] - 13s 353ms/step - loss: 0.6276 - accuracy: 0.9025 - val_loss: 1.5564 - val_accuracy: 0.6272\n",
      "Epoch 21/42\n",
      "36/36 [==============================] - 13s 357ms/step - loss: 0.5759 - accuracy: 0.9164 - val_loss: 1.5346 - val_accuracy: 0.6376\n",
      "Epoch 22/42\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.5037 - accuracy: 0.9295 - val_loss: 1.5667 - val_accuracy: 0.6098\n",
      "Epoch 23/42\n",
      "36/36 [==============================] - 13s 356ms/step - loss: 0.4443 - accuracy: 0.9521 - val_loss: 1.5561 - val_accuracy: 0.6411\n",
      "Epoch 24/42\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 0.4035 - accuracy: 0.9600 - val_loss: 1.4738 - val_accuracy: 0.6551\n",
      "Epoch 25/42\n",
      "36/36 [==============================] - 12s 329ms/step - loss: 0.3477 - accuracy: 0.9687 - val_loss: 1.4528 - val_accuracy: 0.6376\n",
      "Epoch 26/42\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.3067 - accuracy: 0.9774 - val_loss: 1.4559 - val_accuracy: 0.6411\n",
      "Epoch 27/42\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.2728 - accuracy: 0.9774 - val_loss: 1.4332 - val_accuracy: 0.6272\n",
      "Epoch 28/42\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.2378 - accuracy: 0.9852 - val_loss: 1.4135 - val_accuracy: 0.6481\n",
      "Epoch 29/42\n",
      "36/36 [==============================] - 12s 334ms/step - loss: 0.2148 - accuracy: 0.9869 - val_loss: 1.4088 - val_accuracy: 0.6551\n",
      "Epoch 30/42\n",
      "36/36 [==============================] - 13s 353ms/step - loss: 0.2034 - accuracy: 0.9930 - val_loss: 1.4087 - val_accuracy: 0.6585\n",
      "Epoch 31/42\n",
      "36/36 [==============================] - 12s 332ms/step - loss: 0.1933 - accuracy: 0.9948 - val_loss: 1.4051 - val_accuracy: 0.6585\n",
      "Epoch 32/42\n",
      "36/36 [==============================] - 12s 332ms/step - loss: 0.1877 - accuracy: 0.9922 - val_loss: 1.3959 - val_accuracy: 0.6585\n",
      "Epoch 33/42\n",
      "36/36 [==============================] - 13s 358ms/step - loss: 0.1828 - accuracy: 0.9939 - val_loss: 1.3990 - val_accuracy: 0.6620\n",
      "Epoch 34/42\n",
      "36/36 [==============================] - 12s 330ms/step - loss: 0.1747 - accuracy: 0.9956 - val_loss: 1.3932 - val_accuracy: 0.6620\n",
      "Epoch 35/42\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 0.1693 - accuracy: 0.9965 - val_loss: 1.3935 - val_accuracy: 0.6655\n",
      "Epoch 36/42\n",
      "36/36 [==============================] - 12s 331ms/step - loss: 0.1620 - accuracy: 0.9956 - val_loss: 1.3852 - val_accuracy: 0.6620\n",
      "Epoch 37/42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 12s 324ms/step - loss: 0.1564 - accuracy: 0.9965 - val_loss: 1.3819 - val_accuracy: 0.6620\n",
      "Epoch 38/42\n",
      "36/36 [==============================] - 12s 324ms/step - loss: 0.1515 - accuracy: 0.9956 - val_loss: 1.3839 - val_accuracy: 0.6620\n",
      "Epoch 39/42\n",
      "36/36 [==============================] - 12s 325ms/step - loss: 0.1467 - accuracy: 0.9965 - val_loss: 1.3698 - val_accuracy: 0.6655\n",
      "Epoch 40/42\n",
      "36/36 [==============================] - 12s 325ms/step - loss: 0.1403 - accuracy: 0.9974 - val_loss: 1.3716 - val_accuracy: 0.6655\n",
      "Epoch 41/42\n",
      "36/36 [==============================] - 12s 324ms/step - loss: 0.1384 - accuracy: 0.9974 - val_loss: 1.3726 - val_accuracy: 0.6655\n",
      "Epoch 42/42\n",
      "36/36 [==============================] - 12s 322ms/step - loss: 0.1387 - accuracy: 0.9965 - val_loss: 1.3755 - val_accuracy: 0.6620\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1.3755 - accuracy: 0.6620\n",
      "Epoch 1/42\n",
      "36/36 [==============================] - 13s 352ms/step - loss: 2.8369 - accuracy: 0.1601 - val_loss: 2.6148 - val_accuracy: 0.2021\n",
      "Epoch 2/42\n",
      "36/36 [==============================] - 12s 326ms/step - loss: 2.5574 - accuracy: 0.2019 - val_loss: 2.5274 - val_accuracy: 0.1951\n",
      "Epoch 3/42\n",
      "36/36 [==============================] - 13s 348ms/step - loss: 2.4818 - accuracy: 0.2237 - val_loss: 2.4758 - val_accuracy: 0.2369\n",
      "Epoch 4/42\n",
      "36/36 [==============================] - 13s 348ms/step - loss: 2.3838 - accuracy: 0.2689 - val_loss: 2.4354 - val_accuracy: 0.3206\n",
      "Epoch 5/42\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 2.3104 - accuracy: 0.3211 - val_loss: 2.3546 - val_accuracy: 0.2822\n",
      "Epoch 6/42\n",
      "36/36 [==============================] - 12s 325ms/step - loss: 2.1989 - accuracy: 0.3951 - val_loss: 2.2959 - val_accuracy: 0.2927\n",
      "Epoch 7/42\n",
      "36/36 [==============================] - 13s 353ms/step - loss: 2.0474 - accuracy: 0.4334 - val_loss: 2.2244 - val_accuracy: 0.3833\n",
      "Epoch 8/42\n",
      "36/36 [==============================] - 12s 346ms/step - loss: 1.9147 - accuracy: 0.5048 - val_loss: 2.1541 - val_accuracy: 0.4007\n",
      "Epoch 9/42\n",
      "36/36 [==============================] - 13s 352ms/step - loss: 1.7990 - accuracy: 0.5640 - val_loss: 2.1054 - val_accuracy: 0.4286\n",
      "Epoch 10/42\n",
      "36/36 [==============================] - 13s 351ms/step - loss: 1.6724 - accuracy: 0.6057 - val_loss: 2.0261 - val_accuracy: 0.4530\n",
      "Epoch 11/42\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 1.5623 - accuracy: 0.6501 - val_loss: 1.9704 - val_accuracy: 0.4286\n",
      "Epoch 12/42\n",
      "36/36 [==============================] - 12s 325ms/step - loss: 1.4363 - accuracy: 0.6632 - val_loss: 1.9579 - val_accuracy: 0.4216\n",
      "Epoch 13/42\n",
      "36/36 [==============================] - 13s 351ms/step - loss: 1.3052 - accuracy: 0.7424 - val_loss: 1.8515 - val_accuracy: 0.5087\n",
      "Epoch 14/42\n",
      "36/36 [==============================] - 12s 321ms/step - loss: 1.1848 - accuracy: 0.7676 - val_loss: 1.8108 - val_accuracy: 0.4564\n",
      "Epoch 15/42\n",
      "36/36 [==============================] - 12s 328ms/step - loss: 1.0749 - accuracy: 0.7929 - val_loss: 1.7776 - val_accuracy: 0.5017\n",
      "Epoch 16/42\n",
      "36/36 [==============================] - 13s 347ms/step - loss: 0.9668 - accuracy: 0.8251 - val_loss: 1.7407 - val_accuracy: 0.5296\n",
      "Epoch 17/42\n",
      "36/36 [==============================] - 12s 324ms/step - loss: 0.8794 - accuracy: 0.8468 - val_loss: 1.6970 - val_accuracy: 0.5052\n",
      "Epoch 18/42\n",
      "36/36 [==============================] - 13s 350ms/step - loss: 0.7765 - accuracy: 0.8764 - val_loss: 1.6710 - val_accuracy: 0.5470\n",
      "Epoch 19/42\n",
      "36/36 [==============================] - 12s 331ms/step - loss: 0.7015 - accuracy: 0.8860 - val_loss: 1.6422 - val_accuracy: 0.5122\n",
      "Epoch 20/42\n",
      "36/36 [==============================] - 12s 332ms/step - loss: 0.6230 - accuracy: 0.9025 - val_loss: 1.6034 - val_accuracy: 0.5122\n",
      "Epoch 21/42\n",
      "36/36 [==============================] - 13s 350ms/step - loss: 0.5600 - accuracy: 0.9199 - val_loss: 1.5984 - val_accuracy: 0.5749\n",
      "Epoch 22/42\n",
      "36/36 [==============================] - 12s 328ms/step - loss: 0.4996 - accuracy: 0.9373 - val_loss: 1.5558 - val_accuracy: 0.5470\n",
      "Epoch 23/42\n",
      "36/36 [==============================] - 12s 323ms/step - loss: 0.4431 - accuracy: 0.9487 - val_loss: 1.5411 - val_accuracy: 0.5470\n",
      "Epoch 24/42\n",
      "36/36 [==============================] - 12s 324ms/step - loss: 0.3965 - accuracy: 0.9617 - val_loss: 1.5240 - val_accuracy: 0.5714\n",
      "Epoch 25/42\n",
      "36/36 [==============================] - 12s 326ms/step - loss: 0.3482 - accuracy: 0.9669 - val_loss: 1.5113 - val_accuracy: 0.5575\n",
      "Epoch 26/42\n",
      "36/36 [==============================] - 12s 320ms/step - loss: 0.3063 - accuracy: 0.9800 - val_loss: 1.4909 - val_accuracy: 0.5749\n",
      "Epoch 27/42\n",
      "36/36 [==============================] - 11s 315ms/step - loss: 0.2918 - accuracy: 0.9826 - val_loss: 1.4860 - val_accuracy: 0.5679\n",
      "Epoch 28/42\n",
      "36/36 [==============================] - 11s 311ms/step - loss: 0.2827 - accuracy: 0.9809 - val_loss: 1.4839 - val_accuracy: 0.5714\n",
      "Epoch 29/42\n",
      "36/36 [==============================] - 11s 316ms/step - loss: 0.2687 - accuracy: 0.9878 - val_loss: 1.4776 - val_accuracy: 0.5714\n",
      "Epoch 30/42\n",
      "36/36 [==============================] - 11s 317ms/step - loss: 0.2612 - accuracy: 0.9843 - val_loss: 1.4773 - val_accuracy: 0.5610\n",
      "Epoch 31/42\n",
      "36/36 [==============================] - 11s 317ms/step - loss: 0.2488 - accuracy: 0.9904 - val_loss: 1.4744 - val_accuracy: 0.5645\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1.4744 - accuracy: 0.5645\n",
      "Epoch 1/42\n",
      "36/36 [==============================] - 12s 345ms/step - loss: 2.7671 - accuracy: 0.1445 - val_loss: 2.6327 - val_accuracy: 0.1986\n",
      "Epoch 2/42\n",
      "36/36 [==============================] - 11s 317ms/step - loss: 2.5478 - accuracy: 0.2037 - val_loss: 2.5661 - val_accuracy: 0.1916\n",
      "Epoch 3/42\n",
      "36/36 [==============================] - 12s 340ms/step - loss: 2.4911 - accuracy: 0.2289 - val_loss: 2.5158 - val_accuracy: 0.2160\n",
      "Epoch 4/42\n",
      "36/36 [==============================] - 11s 318ms/step - loss: 2.3674 - accuracy: 0.2898 - val_loss: 2.5063 - val_accuracy: 0.1847\n",
      "Epoch 5/42\n",
      "36/36 [==============================] - 12s 341ms/step - loss: 2.2866 - accuracy: 0.3133 - val_loss: 2.4287 - val_accuracy: 0.2195\n",
      "Epoch 6/42\n",
      "36/36 [==============================] - 12s 341ms/step - loss: 2.1756 - accuracy: 0.3934 - val_loss: 2.3850 - val_accuracy: 0.2648\n",
      "Epoch 7/42\n",
      "36/36 [==============================] - 12s 338ms/step - loss: 2.0385 - accuracy: 0.4430 - val_loss: 2.3090 - val_accuracy: 0.2857\n",
      "Epoch 8/42\n",
      "36/36 [==============================] - 12s 342ms/step - loss: 1.8978 - accuracy: 0.4970 - val_loss: 2.2674 - val_accuracy: 0.3310\n",
      "Epoch 9/42\n",
      "36/36 [==============================] - 12s 342ms/step - loss: 1.7912 - accuracy: 0.5431 - val_loss: 2.1865 - val_accuracy: 0.3554\n",
      "Epoch 10/42\n",
      "36/36 [==============================] - 12s 341ms/step - loss: 1.6509 - accuracy: 0.5988 - val_loss: 2.1098 - val_accuracy: 0.4042\n",
      "Epoch 11/42\n",
      "36/36 [==============================] - 12s 342ms/step - loss: 1.5297 - accuracy: 0.6388 - val_loss: 2.0775 - val_accuracy: 0.4739\n",
      "Epoch 12/42\n",
      "36/36 [==============================] - 13s 348ms/step - loss: 1.3867 - accuracy: 0.7093 - val_loss: 2.0114 - val_accuracy: 0.4774\n",
      "Epoch 13/42\n",
      "36/36 [==============================] - 12s 331ms/step - loss: 1.2675 - accuracy: 0.7537 - val_loss: 1.9380 - val_accuracy: 0.4704\n",
      "Epoch 14/42\n",
      "36/36 [==============================] - 12s 321ms/step - loss: 1.1476 - accuracy: 0.7685 - val_loss: 1.8905 - val_accuracy: 0.4669\n",
      "Epoch 15/42\n",
      "36/36 [==============================] - 12s 344ms/step - loss: 1.0351 - accuracy: 0.7981 - val_loss: 1.9074 - val_accuracy: 0.5087\n",
      "Epoch 16/42\n",
      "36/36 [==============================] - 13s 347ms/step - loss: 0.9233 - accuracy: 0.8451 - val_loss: 1.8515 - val_accuracy: 0.5540\n",
      "Epoch 17/42\n",
      "36/36 [==============================] - 12s 326ms/step - loss: 0.8376 - accuracy: 0.8599 - val_loss: 1.7997 - val_accuracy: 0.5122\n",
      "Epoch 18/42\n",
      "36/36 [==============================] - 13s 353ms/step - loss: 0.7393 - accuracy: 0.8886 - val_loss: 1.7659 - val_accuracy: 0.5784\n",
      "Epoch 19/42\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 0.6603 - accuracy: 0.9043 - val_loss: 1.7357 - val_accuracy: 0.5366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/42\n",
      "36/36 [==============================] - 12s 324ms/step - loss: 0.5848 - accuracy: 0.9269 - val_loss: 1.7164 - val_accuracy: 0.5157\n",
      "Epoch 21/42\n",
      "36/36 [==============================] - 12s 321ms/step - loss: 0.5295 - accuracy: 0.9269 - val_loss: 1.7021 - val_accuracy: 0.5610\n",
      "Epoch 22/42\n",
      "36/36 [==============================] - 12s 322ms/step - loss: 0.4622 - accuracy: 0.9495 - val_loss: 1.6478 - val_accuracy: 0.5749\n",
      "Epoch 23/42\n",
      "36/36 [==============================] - 12s 329ms/step - loss: 0.4070 - accuracy: 0.9565 - val_loss: 1.6287 - val_accuracy: 0.5714\n",
      "Epoch 24/42\n",
      "36/36 [==============================] - 12s 345ms/step - loss: 0.3869 - accuracy: 0.9634 - val_loss: 1.6237 - val_accuracy: 0.6098\n",
      "Epoch 25/42\n",
      "36/36 [==============================] - 12s 324ms/step - loss: 0.3718 - accuracy: 0.9652 - val_loss: 1.6216 - val_accuracy: 0.5993\n",
      "Epoch 26/42\n",
      "36/36 [==============================] - 12s 322ms/step - loss: 0.3577 - accuracy: 0.9687 - val_loss: 1.6142 - val_accuracy: 0.5958\n",
      "Epoch 27/42\n",
      "36/36 [==============================] - 12s 322ms/step - loss: 0.3444 - accuracy: 0.9739 - val_loss: 1.5995 - val_accuracy: 0.6028\n",
      "Epoch 28/42\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 0.3300 - accuracy: 0.9730 - val_loss: 1.6123 - val_accuracy: 0.6063\n",
      "Epoch 29/42\n",
      "36/36 [==============================] - 12s 322ms/step - loss: 0.3180 - accuracy: 0.9748 - val_loss: 1.6038 - val_accuracy: 0.6098\n",
      "Epoch 30/42\n",
      "36/36 [==============================] - 12s 322ms/step - loss: 0.3121 - accuracy: 0.9739 - val_loss: 1.6017 - val_accuracy: 0.6063\n",
      "Epoch 31/42\n",
      "36/36 [==============================] - 12s 321ms/step - loss: 0.3089 - accuracy: 0.9756 - val_loss: 1.5984 - val_accuracy: 0.6028\n",
      "Epoch 32/42\n",
      "36/36 [==============================] - 12s 322ms/step - loss: 0.3058 - accuracy: 0.9756 - val_loss: 1.5953 - val_accuracy: 0.6028\n",
      "Epoch 33/42\n",
      "36/36 [==============================] - 12s 322ms/step - loss: 0.3024 - accuracy: 0.9782 - val_loss: 1.5938 - val_accuracy: 0.6028\n",
      "Epoch 34/42\n",
      "36/36 [==============================] - 12s 322ms/step - loss: 0.2981 - accuracy: 0.9782 - val_loss: 1.5941 - val_accuracy: 0.6028\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1.5941 - accuracy: 0.6028\n"
     ]
    }
   ],
   "source": [
    "n_encoder = 0\n",
    "models, preds, scores = train_pred(n_encoder = 0,epochs = 42)\n",
    "# outputs.append([pred_val_y, pred_test_y, 'transformer_enc{}'.format(n_encoder)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "e3dc6e5362fa13e00b291986b94ea9d6e5acdebf"
   },
   "outputs": [],
   "source": [
    "# for thresh in np.arange(0.1, 0.51, 0.01):\n",
    "#     thresh = np.round(thresh, 2)\n",
    "#     print(\"F1 score at threshold {0:.2f} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))\n",
    "models_trans = models\n",
    "# scores\n",
    "# preds\n",
    "# np.mean(preds,axis = 0)\n",
    "# models == models_trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "43e0969fd3af4b65decfbba8cce0a8a36d552176"
   },
   "outputs": [],
   "source": [
    "# pred_test_y = (pred_test_y > 0.42).astype(int)\n",
    "# test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\n",
    "# out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "# out_df['prediction'] = pred_test_y\n",
    "# out_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "c378fe1cfd53529831f3928fb9866329eb7a9184"
   },
   "outputs": [],
   "source": [
    "# idx = (pred_test_y > 0.42).astype(int)\n",
    "# test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\n",
    "# out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "# out_df['prediction'] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "c378fe1cfd53529831f3928fb9866329eb7a9184"
   },
   "outputs": [],
   "source": [
    "# mylist = out_df[out_df.prediction == 1].index\n",
    "# for i in mylist:\n",
    "#     print(i, end=',')\n",
    "\n",
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a3b5a15d3745fa2ffb2e0bfd1c98ba890e27550"
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model)\n",
    "train_df = pd.read_csv(\"../data/Train.csv\")\n",
    "test_df = pd.read_csv(\"../data/Test.csv\")\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "#     text = re.sub(r\"what's\", \"what is \", text)\n",
    "#     text = re.sub(r\"\\'s\", \" \", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "#     text = re.sub(r\"can't\", \"cannot \", text)\n",
    "#     text = re.sub(r\"n't\", \" not \", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "#     text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(r\",\", \" \", text) \n",
    "    text = re.sub(r\"!\", \" \", text) \n",
    "    text = re.sub(r\"\\(\", \" \", text) \n",
    "    text = re.sub(r\"\\)\", \" \", text) \n",
    "    text = re.sub(r\"\\?\", \" \", text) \n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  \n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "# removing stop words\n",
    "# other_stop_w = pd.read_csv('../Downloaded_notebooks/words_shared_by_all.csv')\n",
    "# stopw = [item for sublist in other_stop_w.values.tolist() for item in sublist]\n",
    "# train_df['Text'].apply(lambda x: [item for item in x.split() if item not in stopw])\n",
    "# test_df['Text'].apply(lambda x: [item for item in x.split() if item not in stopw])\n",
    "\n",
    "train_df['Text'] = train_df['Text'].map(lambda com : clean_text(com))\n",
    "test_df['Text'] = test_df['Text'].map(lambda com : clean_text(com))\n",
    "X_tfidf = train_df.Text\n",
    "test_X_tfidf = test_df.Text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [base_m.trainable = False ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(max_features=45000,sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "\n",
    "X_dtm = vect.fit_transform(X_tfidf).toarray()\n",
    "\n",
    "test_X_dtm = vect.transform(test_X_tfidf).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train_df['Label'])\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= lb.classes_)\n",
    "# # y_train\n",
    "cols_target = train_df['Label'].unique().tolist()\n",
    "train_df = pd.concat([train_df, y_train], axis = 1)\n",
    "# # train_df\n",
    "\n",
    "# x_train, x_val, y_train, y_val = train_test_split(X_dtm, train_df[cols_target], test_size=0.1, random_state = 0,stratify = train_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idx = list(set(X_tfidf.index.tolist()) - set(val_idx.tolist()))\n",
    "# base_model.evaluate(X_dtm[val_idx],train_df.loc[val_idx,cols_target])\n",
    "# (y_val == val_y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y.shape\n",
    "# len(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "# Build the model\n",
    "from keras import backend as K \n",
    "\n",
    "# Do some code, e.g. train and save model\n",
    "\n",
    "# K.clear_session()\n",
    "# seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "# os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "# random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "# np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "# tf.random.set_seed(seed_value)\n",
    "def build_base_model():\n",
    "    K.clear_session()\n",
    "    seed_value = 0\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    base_model = models.Sequential()\n",
    "    base_model.add(layers.Dense(1000, input_shape=(45000,)))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    base_model.add(layers.Activation('linear'))\n",
    "    base_model.add(layers.Dropout(0.2))\n",
    "    # model.add(layers.Dense(2048))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Activation('relu'))\n",
    "    # model.add(layers.Dense(512))\n",
    "    # # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Activation('relu'))\n",
    "    # model.add(layers.Dense(128))\n",
    "    # # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Activation('relu'))\n",
    "\n",
    "    # model.add(layers.Dropout(drop_ratio))\n",
    "    base_model.add(layers.Dense(20))\n",
    "    base_model.add(layers.Activation('softmax'))\n",
    "\n",
    "    base_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_dtm[train_idx].shape\n",
    "# base_model\n",
    "# history = base_model.fit(x_train, y_train,\n",
    "#                     batch_size=64,\n",
    "#                     epochs=10,\n",
    "#                     verbose=1,\n",
    "#                    validation_split = 0.1)\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback, ReduceLROnPlateau\n",
    "file_path = \"weights_base.best.hdf5\"\n",
    "def benchmark():\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "#     print(clf)\n",
    "#     t0 = time()\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=0)\n",
    "    models, preds, scores = [], [],[]\n",
    "#     vectorizer = vect(max_df = 0.5)\n",
    "    for train, test in skf.split(train_df.Text, train_df.Label):\n",
    "#     print(train, test)\n",
    "#     clf = LogisticRegression(penalty='l1')\n",
    "#         clf.fit(vectorizer.transform(), data_train.Label.loc[data_train.index.intersection(train)])\n",
    "#         K.clear_session()\n",
    "        clf = build_base_model()\n",
    "        X_train = train_df.Text.loc[train_df.index.intersection(train)]\n",
    "        X_val = train_df.Text.loc[train_df.index.intersection(test)]\n",
    "        y_train = train_df[cols_target].loc[train_df.index.intersection(train)]\n",
    "        y_val = train_df[cols_target].loc[train_df.index.intersection(test)]\n",
    "        X_train = vect.transform(X_train).toarray()\n",
    "        X_val = vect.transform(X_val).toarray()\n",
    "        X_test = vect.transform(test_df.Text).toarray()\n",
    "        \n",
    "        \n",
    "        \n",
    "        clf.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                   validation_data = (X_val,y_val),\n",
    "                   callbacks=[\n",
    "#               RocAucEvaluation(verbose=True),\n",
    "              ModelCheckpoint(file_path,    monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "              EarlyStopping(patience=10,    monitor=\"val_accuracy\", mode=\"max\"),\n",
    "              ReduceLROnPlateau(patience=4, monitor='val_accuracy', mode='max', cooldown=2, min_lr=1e-7, factor=0.3)])\n",
    "        preds.append(clf.predict(X_test))\n",
    "        models.append(clf)\n",
    "        scores.append(clf.evaluate(X_val,y_val))\n",
    "#         coefs.append(clf.coef_[0])\n",
    "#         clf.fit(X_train, y_train)\n",
    "#     train_time = time() - t0\n",
    "#     print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "#     t0 = time()\n",
    "#     pred = clf.predict(X_test)\n",
    "#     test_time = time() - t0\n",
    "#     print(\"test time:  %0.3fs\" % test_time)\n",
    "    pred = np.mean(preds,axis = 0)\n",
    "#     score = metrics.accuracy_score(data_test.Label, pred)\n",
    "#     print(\"accuracy:   %0.3f\" % score)\n",
    "    return models, pred,scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 17s 479ms/step - loss: 2.4258 - accuracy: 0.2605 - val_loss: 2.0041 - val_accuracy: 0.4931\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 15s 424ms/step - loss: 0.9089 - accuracy: 0.8441 - val_loss: 1.5423 - val_accuracy: 0.5660\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 15s 417ms/step - loss: 0.2080 - accuracy: 0.9878 - val_loss: 1.4053 - val_accuracy: 0.6076\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 15s 406ms/step - loss: 0.0551 - accuracy: 0.9965 - val_loss: 1.3657 - val_accuracy: 0.6146\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 12s 328ms/step - loss: 0.0267 - accuracy: 0.9974 - val_loss: 1.3525 - val_accuracy: 0.6111\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 12s 320ms/step - loss: 0.0204 - accuracy: 0.9983 - val_loss: 1.3456 - val_accuracy: 0.6076\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 13s 349ms/step - loss: 0.0135 - accuracy: 0.9991 - val_loss: 1.3414 - val_accuracy: 0.6111\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 16s 433ms/step - loss: 0.0116 - accuracy: 0.9983 - val_loss: 1.3385 - val_accuracy: 0.6076\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 14s 385ms/step - loss: 0.0097 - accuracy: 0.9991 - val_loss: 1.3387 - val_accuracy: 0.6111\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 13s 367ms/step - loss: 0.0084 - accuracy: 0.9991 - val_loss: 1.3380 - val_accuracy: 0.6076\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.3380 - accuracy: 0.6076\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 17s 468ms/step - loss: 2.4194 - accuracy: 0.2620 - val_loss: 1.9161 - val_accuracy: 0.5052\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 16s 445ms/step - loss: 0.8915 - accuracy: 0.8503 - val_loss: 1.4407 - val_accuracy: 0.6272\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 16s 443ms/step - loss: 0.1924 - accuracy: 0.9869 - val_loss: 1.3315 - val_accuracy: 0.6481\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 14s 380ms/step - loss: 0.0460 - accuracy: 0.9991 - val_loss: 1.3006 - val_accuracy: 0.6411\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 12s 335ms/step - loss: 0.0236 - accuracy: 0.9991 - val_loss: 1.2859 - val_accuracy: 0.6341\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 13s 356ms/step - loss: 0.0158 - accuracy: 0.9991 - val_loss: 1.2779 - val_accuracy: 0.6376\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 13s 351ms/step - loss: 0.0132 - accuracy: 0.9983 - val_loss: 1.2758 - val_accuracy: 0.6341\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 13s 357ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.2751 - val_accuracy: 0.6341\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 13s 357ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.2741 - val_accuracy: 0.6341\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 14s 375ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.2738 - val_accuracy: 0.6341\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.2738 - accuracy: 0.6341\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 16s 448ms/step - loss: 2.4324 - accuracy: 0.2742 - val_loss: 1.8869 - val_accuracy: 0.5157\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 16s 446ms/step - loss: 0.9197 - accuracy: 0.8259 - val_loss: 1.3698 - val_accuracy: 0.6481\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 16s 437ms/step - loss: 0.1939 - accuracy: 0.9904 - val_loss: 1.2341 - val_accuracy: 0.6620\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 16s 450ms/step - loss: 0.0482 - accuracy: 0.9983 - val_loss: 1.1954 - val_accuracy: 0.6725\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 13s 356ms/step - loss: 0.0255 - accuracy: 0.9974 - val_loss: 1.1803 - val_accuracy: 0.6725\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 15s 430ms/step - loss: 0.0185 - accuracy: 0.9983 - val_loss: 1.1728 - val_accuracy: 0.6760\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 13s 350ms/step - loss: 0.0159 - accuracy: 0.9983 - val_loss: 1.1666 - val_accuracy: 0.6725\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 13s 351ms/step - loss: 0.0104 - accuracy: 0.9983 - val_loss: 1.1632 - val_accuracy: 0.6725\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 12s 346ms/step - loss: 0.0088 - accuracy: 0.9974 - val_loss: 1.1599 - val_accuracy: 0.6725\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 13s 348ms/step - loss: 0.0072 - accuracy: 0.9991 - val_loss: 1.1603 - val_accuracy: 0.6690\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.1603 - accuracy: 0.6690\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 17s 468ms/step - loss: 2.4084 - accuracy: 0.2576 - val_loss: 1.9020 - val_accuracy: 0.5157\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 14s 401ms/step - loss: 0.9165 - accuracy: 0.8277 - val_loss: 1.4190 - val_accuracy: 0.6098\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 15s 419ms/step - loss: 0.2072 - accuracy: 0.9913 - val_loss: 1.2764 - val_accuracy: 0.6307\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 12s 346ms/step - loss: 0.0534 - accuracy: 0.9983 - val_loss: 1.2325 - val_accuracy: 0.6307\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 13s 361ms/step - loss: 0.0279 - accuracy: 0.9974 - val_loss: 1.2136 - val_accuracy: 0.6202\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 12s 342ms/step - loss: 0.0176 - accuracy: 0.9983 - val_loss: 1.2016 - val_accuracy: 0.6202\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 11s 314ms/step - loss: 0.0181 - accuracy: 0.9983 - val_loss: 1.1989 - val_accuracy: 0.6202\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 13s 348ms/step - loss: 0.0114 - accuracy: 0.9983 - val_loss: 1.1953 - val_accuracy: 0.6237\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 13s 372ms/step - loss: 0.0104 - accuracy: 0.9983 - val_loss: 1.1921 - val_accuracy: 0.6307\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 12s 343ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 1.1908 - val_accuracy: 0.6307\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.1908 - accuracy: 0.6307\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 18s 496ms/step - loss: 2.4138 - accuracy: 0.2602 - val_loss: 1.9680 - val_accuracy: 0.4878\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 15s 425ms/step - loss: 0.9205 - accuracy: 0.8268 - val_loss: 1.5199 - val_accuracy: 0.5784\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 15s 410ms/step - loss: 0.2081 - accuracy: 0.9896 - val_loss: 1.3900 - val_accuracy: 0.6028\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 13s 354ms/step - loss: 0.0528 - accuracy: 0.9974 - val_loss: 1.3567 - val_accuracy: 0.5993\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 12s 329ms/step - loss: 0.0283 - accuracy: 0.9965 - val_loss: 1.3455 - val_accuracy: 0.5958\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 13s 373ms/step - loss: 0.0178 - accuracy: 0.9983 - val_loss: 1.3394 - val_accuracy: 0.5958\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 13s 360ms/step - loss: 0.0181 - accuracy: 0.9983 - val_loss: 1.3389 - val_accuracy: 0.5958\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 12s 336ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 1.3361 - val_accuracy: 0.5993\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.0099 - accuracy: 0.9983 - val_loss: 1.3336 - val_accuracy: 0.5993\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 13s 351ms/step - loss: 0.0093 - accuracy: 0.9983 - val_loss: 1.3338 - val_accuracy: 0.5993\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.3338 - accuracy: 0.5993\n"
     ]
    }
   ],
   "source": [
    "# base_model.evaluate(x_val,y_val)\n",
    "# train_X.shape\n",
    "# (y_train == train_y).all()\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,cross_val_score,train_test_split,StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron,LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "base_models, pred,scores = benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vect\n",
    "# base_models[0].trainable\n",
    "for base_m in models_trans:\n",
    "    base_m.trainable = False\n",
    "for base_m in base_models:\n",
    "    base_m.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.trainable = False\n",
    "# model.trainable = False\n",
    "# base_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.shape\n",
    "# X_dtm\n",
    "# lb.inverse_transform(pred)\n",
    "# lb.inverse_transform(pd.DataFrame(pred,columns = cols_target)[lb.classes_].values)\n",
    "# pred = np.mean(preds,axis = 0)\n",
    "\n",
    "# test_df['Label']= lb.inverse_transform(pd.DataFrame(pred,columns = cols_target)[lb.classes_].values)\n",
    "# sub = test_df[['ID', 'Label']]\n",
    "# sub.to_csv('cross_enc_001.csv', index = False)\n",
    "# sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"../Translated/cleaned/train.csv\")\n",
    "# test_df = pd.read_csv(\"../Translated/cleaned/test.csv\")\n",
    "\n",
    "K.clear_session()\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_supermodel():\n",
    "    K.clear_session()\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "    input_trans = layers.Input(shape=(maxlen,))\n",
    "    input_tf = layers.Input(shape=(45000,))\n",
    "    output1 = []\n",
    "    for i, base_model in enumerate(base_models) : \n",
    "        base_model._name = 'base_model_'+str(i)\n",
    "        output1.append(base_model)\n",
    "    output_1 = [base_model(input_tf,training = False) for base_model in output1]\n",
    "\n",
    "    output_2 = [model(input_trans,training = False) for model in models_trans]\n",
    "\n",
    "    y = layers.Concatenate( name = 'output_1')(output_1)\n",
    "    x = layers.Concatenate()(output_2)\n",
    "    x = layers.Concatenate()([x,y])\n",
    "    # x = layers.Dense(1024, activation = 'linear')(x)\n",
    "    x = layers.Dense(512, activation = 'linear')(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation = 'sigmoid')(x)\n",
    "    x = layers.Dense(128, activation = 'linear',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    outputs = layers.Dense(20, activation=\"softmax\")(x)\n",
    "    super_model = keras.Model(inputs=[input_trans, input_tf], outputs=outputs)\n",
    "    super_model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return super_model\n",
    "# history = super_model.fit(\n",
    "#     [train_X,x_train], y_train, batch_size=32, epochs=19, validation_split = 0.1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_stack():\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "#     print(clf)\n",
    "#     t0 = time()\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=0)\n",
    "    models, preds, scores = [], [],[]\n",
    "#     vectorizer = vect(max_df = 0.5)\n",
    "    for train, test in skf.split(train_df.Text, train_df.Label):\n",
    "#     print(train, test)\n",
    "#     clf = LogisticRegression(penalty='l1')\n",
    "#         clf.fit(vectorizer.transform(), data_train.Label.loc[data_train.index.intersection(train)])\n",
    "#         K.clear_session()\n",
    "        clf = build_supermodel()\n",
    "        X_train = train_df.Text.loc[train_df.index.intersection(train)]\n",
    "        X_val = train_df.Text.loc[train_df.index.intersection(test)]\n",
    "        y_train = train_df[cols_target].loc[train_df.index.intersection(train)]\n",
    "        y_val = train_df[cols_target].loc[train_df.index.intersection(test)]\n",
    "        \n",
    "        X_train_ker = tokenizer.texts_to_sequences(X_train)\n",
    "        X_val_ker = tokenizer.texts_to_sequences(X_val)\n",
    "        X_test_ker = tokenizer.texts_to_sequences(test_df.Text)\n",
    "\n",
    "        ## Pad the sentences \n",
    "        X_train_ker = pad_sequences(X_train_ker, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        X_val_ker = pad_sequences(X_val_ker, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        X_test_ker = pad_sequences(X_test_ker, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        \n",
    "        X_train_tfidf = vect.transform(X_train).toarray()\n",
    "        X_val_tfidf = vect.transform(X_val).toarray()\n",
    "        X_test_tfidf = vect.transform(test_df.Text).toarray()\n",
    "        \n",
    "        \n",
    "        clf.fit([X_train_ker, X_train_tfidf], y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                   validation_data = ([X_val_ker, X_val_tfidf],y_val),\n",
    "                   callbacks=[\n",
    "#               RocAucEvaluation(verbose=True),\n",
    "              ModelCheckpoint(file_path,    monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "              EarlyStopping(patience=10,    monitor=\"val_accuracy\", mode=\"max\"),\n",
    "              ReduceLROnPlateau(patience=4, monitor='val_accuracy', mode='max', cooldown=2, min_lr=1e-7, factor=0.3)])\n",
    "        preds.append(clf.predict([X_test_ker,X_test_tfidf]))\n",
    "        models.append(clf)\n",
    "        scores.append(clf.evaluate([X_val_ker, X_val_tfidf],y_val))\n",
    "#         coefs.append(clf.coef_[0])\n",
    "#         clf.fit(X_train, y_train)\n",
    "#     train_time = time() - t0\n",
    "#     print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "#     t0 = time()\n",
    "#     pred = clf.predict(X_test)\n",
    "#     test_time = time() - t0\n",
    "#     print(\"test time:  %0.3fs\" % test_time)\n",
    "    pred = np.mean(preds,axis = 0)\n",
    "#     score = metrics.accuracy_score(data_test.Label, pred)\n",
    "#     print(\"accuracy:   %0.3f\" % score)\n",
    "    return models, pred,scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "36/36 [==============================] - 26s 721ms/step - loss: 2.8252 - accuracy: 0.7430 - val_loss: 1.8358 - val_accuracy: 0.9688\n",
      "Epoch 2/3\n",
      "36/36 [==============================] - 24s 668ms/step - loss: 1.5596 - accuracy: 0.9887 - val_loss: 1.3585 - val_accuracy: 0.9965\n",
      "Epoch 3/3\n",
      "36/36 [==============================] - 24s 670ms/step - loss: 1.2053 - accuracy: 0.9991 - val_loss: 1.0568 - val_accuracy: 1.0000\n",
      "9/9 [==============================] - 3s 285ms/step - loss: 1.0568 - accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "36/36 [==============================] - 26s 719ms/step - loss: 2.8394 - accuracy: 0.7389 - val_loss: 1.8402 - val_accuracy: 0.9826\n",
      "Epoch 2/3\n",
      "36/36 [==============================] - 26s 713ms/step - loss: 1.5696 - accuracy: 0.9887 - val_loss: 1.3798 - val_accuracy: 0.9965\n",
      "Epoch 3/3\n",
      "36/36 [==============================] - 18s 491ms/step - loss: 1.2201 - accuracy: 1.0000 - val_loss: 1.0863 - val_accuracy: 0.9965\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 1.0863 - accuracy: 0.9965\n",
      "Epoch 1/3\n",
      "36/36 [==============================] - 26s 722ms/step - loss: 2.8256 - accuracy: 0.7293 - val_loss: 1.8262 - val_accuracy: 0.9791\n",
      "Epoch 2/3\n",
      "36/36 [==============================] - 24s 663ms/step - loss: 1.5684 - accuracy: 0.9869 - val_loss: 1.3633 - val_accuracy: 0.9930\n",
      "Epoch 3/3\n",
      "36/36 [==============================] - 24s 653ms/step - loss: 1.2101 - accuracy: 0.9991 - val_loss: 1.0650 - val_accuracy: 0.9965\n",
      "9/9 [==============================] - 3s 288ms/step - loss: 1.0650 - accuracy: 0.9965\n",
      "Epoch 1/3\n",
      "36/36 [==============================] - 26s 723ms/step - loss: 2.7861 - accuracy: 0.7502 - val_loss: 1.8062 - val_accuracy: 0.9791\n",
      "Epoch 2/3\n",
      "36/36 [==============================] - 24s 657ms/step - loss: 1.5537 - accuracy: 0.9861 - val_loss: 1.3437 - val_accuracy: 0.9895\n",
      "Epoch 3/3\n",
      "36/36 [==============================] - 25s 682ms/step - loss: 1.1911 - accuracy: 0.9983 - val_loss: 1.0372 - val_accuracy: 1.0000\n",
      "9/9 [==============================] - 3s 287ms/step - loss: 1.0372 - accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "36/36 [==============================] - 25s 686ms/step - loss: 2.7780 - accuracy: 0.7528 - val_loss: 1.8052 - val_accuracy: 0.9791\n",
      "Epoch 2/3\n",
      "36/36 [==============================] - 25s 698ms/step - loss: 1.5432 - accuracy: 0.9878 - val_loss: 1.3360 - val_accuracy: 0.9930\n",
      "Epoch 3/3\n",
      "36/36 [==============================] - 24s 661ms/step - loss: 1.1788 - accuracy: 1.0000 - val_loss: 1.0261 - val_accuracy: 1.0000\n",
      "9/9 [==============================] - 3s 285ms/step - loss: 1.0261 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models, pred,scores = benchmark_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super_model.evaluate([val_X,x_val], y_val)\n",
    "# gsh = output_1[0]\n",
    "# x\n",
    "# gsh.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (y_val.values == val_y).all()\n",
    "# y_val.columns == lb.classes_\n",
    "# preds  = super_model.predict([test_X, test_X_dtm])\n",
    "# lb.inverse_transform(pd.DataFrame(preds,columns = cols_target)[lb.classes_].values)\n",
    "test_df['Label'] = lb.inverse_transform(pd.DataFrame(pred,columns = cols_target)[lb.classes_].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_ADHEtjTi</td>\n",
       "      <td>abambo odzikhweza akuchuluka kafukufuku wa apo...</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_AHfJktdQ</td>\n",
       "      <td>ambuye ziyaye ayamikira aphunzitsi a tilitonse...</td>\n",
       "      <td>RELIGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_AUJIHpZr</td>\n",
       "      <td>anatcheleza: akundiopseza a gogo wanga akundio...</td>\n",
       "      <td>RELATIONSHIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_AUKYBbIM</td>\n",
       "      <td>ulova wafika posauzana adatenga digiri ya uphu...</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_AZnsVPEi</td>\n",
       "      <td>dzombe kukoma koma kuyambira makedzana panthaw...</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ID_zdpOUWyJ</td>\n",
       "      <td>kanyongolo wapempha oyimira milandu kuti atsat...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>ID_zhnOomuu</td>\n",
       "      <td>amandimenya zikomo gogo ndine mtsikana wa zaka...</td>\n",
       "      <td>RELATIONSHIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>ID_zmWHvBJb</td>\n",
       "      <td>apolisi athotha gulu la myp asilikali 56 a gul...</td>\n",
       "      <td>LAW/ORDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>ID_zphjdFIb</td>\n",
       "      <td>mwambo wa ukwati wa chitonga mtundu wina uliwo...</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>ID_ztdtrNxt</td>\n",
       "      <td>mwapasa autsa mapiri pamene pali kusamvana pak...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               Text  \\\n",
       "0    ID_ADHEtjTi  abambo odzikhweza akuchuluka kafukufuku wa apo...   \n",
       "1    ID_AHfJktdQ  ambuye ziyaye ayamikira aphunzitsi a tilitonse...   \n",
       "2    ID_AUJIHpZr  anatcheleza: akundiopseza a gogo wanga akundio...   \n",
       "3    ID_AUKYBbIM  ulova wafika posauzana adatenga digiri ya uphu...   \n",
       "4    ID_AZnsVPEi  dzombe kukoma koma kuyambira makedzana panthaw...   \n",
       "..           ...                                                ...   \n",
       "615  ID_zdpOUWyJ  kanyongolo wapempha oyimira milandu kuti atsat...   \n",
       "616  ID_zhnOomuu  amandimenya zikomo gogo ndine mtsikana wa zaka...   \n",
       "617  ID_zmWHvBJb  apolisi athotha gulu la myp asilikali 56 a gul...   \n",
       "618  ID_zphjdFIb  mwambo wa ukwati wa chitonga mtundu wina uliwo...   \n",
       "619  ID_ztdtrNxt  mwapasa autsa mapiri pamene pali kusamvana pak...   \n",
       "\n",
       "             Label  \n",
       "0    SOCIAL ISSUES  \n",
       "1         RELIGION  \n",
       "2    RELATIONSHIPS  \n",
       "3    SOCIAL ISSUES  \n",
       "4    SOCIAL ISSUES  \n",
       "..             ...  \n",
       "615       POLITICS  \n",
       "616  RELATIONSHIPS  \n",
       "617      LAW/ORDER  \n",
       "618  SOCIAL ISSUES  \n",
       "619       POLITICS  \n",
       "\n",
       "[620 rows x 3 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X.shape\n",
    "sub = test_df[['ID','Label']]\n",
    "sub.to_csv('submission_keras_stack003A_reg6.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sub.Label == pd.read_csv('submission_keras_stack002.csv').Label).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
