{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d5221cf58859cd6496cf4294414a3bc37d4c95f"
   },
   "source": [
    "In this kernel, I have implemented the encoder part of the transformer architecture as mentioned in the famous paper: Attention is all you need.(https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "Many of other codes are adopted from other kernels. For example, loading the embeddings,  load the training and test data and preprocessing, etc. I really appreciate their contributions.\n",
    "\n",
    "p.s. When I run this locally, I get validation f1-score around 0.688.\n",
    "\n",
    "Happy transforming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a947373c706a15ed71a686d92703b9677561894"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.layers import BatchNormalization, InputSpec, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, activations\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b37e0d09f42f5bc5ad73a366580f6b778c9aad5a"
   },
   "source": [
    "## Some pre-configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "c7498e1cc6e3dd7e7c58f24e10fd5ad1b06b4489"
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 45000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 400 # max number of words in a question to use\n",
    "n_heads = 4 # Number of heads as in Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "05385c91a5e5603c346bfe53010aa4cb0f3ddd4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1436, 3)\n",
      "Test shape :  (620, 2)\n"
     ]
    }
   ],
   "source": [
    "# def load_and_prec():\n",
    "train_df = pd.read_csv(\"../Translated/cleaned/train.csv\")\n",
    "test_df = pd.read_csv(\"../Translated/cleaned/test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train_df['Label'])\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= lb.classes_)\n",
    "train_df = pd.concat([train_df, y_train], axis = 1)\n",
    "cols_target = train_df.Label.unique().tolist()\n",
    "\n",
    "## split to train and val\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=0,shuffle = True) # hahaha\n",
    "# train_X, val_X, train_y , val_y = train_test_split(train_df, train_df[cols_target], test_size=0.1, random_state = 0,stratify = train_df['Label'])\n",
    "\n",
    "# trn_idx = train_y.index.tolist()\n",
    "# val_idx = val_y.index.tolist()\n",
    "\n",
    "\n",
    "## fill up the missing values\n",
    "# train_X = train_X[\"Text\"].fillna(\"_##_\").values\n",
    "# val_X = val_X[\"Text\"].fillna(\"_##_\").values\n",
    "# test_X = test_df[\"Text\"].fillna(\"_##_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_df.Text)\n",
    "# train_X = tokenizer.texts_to_sequences(train_X)\n",
    "# val_X = tokenizer.texts_to_sequences(val_X)\n",
    "# test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "# ## Pad the sentences \n",
    "# train_X = pad_sequences(train_X, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "# val_X = pad_sequences(val_X, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "# test_X = pad_sequences(test_X, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "\n",
    "# ## Get the target values\n",
    "# train_y = train_y.values\n",
    "# val_y = val_y.values  \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "#shuffling the data\n",
    "# np.random.seed(2018)\n",
    "# trn_idx = np.random.permutation(len(train_X))\n",
    "# val_idx = np.random.permutation(len(val_X))\n",
    "\n",
    "# train_X = train_X[trn_idx]\n",
    "# val_X = val_X[val_idx]\n",
    "# train_y = train_y[trn_idx]\n",
    "# val_y = val_y[val_idx]    \n",
    "\n",
    "#     return train_X, val_X, test_X, train_y, val_y, tokenizer.word_index,val_idx , trn_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20682431e22eab3cbf634777cb0d2bc2730ab754"
   },
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cf6dc22b3b2a9f2ec94f70e2aa5dfe36f6f142d3"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
    "    embed_size = all_embs.shape[1]\n",
    "    print(emb_mean,emb_std,\"para\")\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a35ee76c0f926bcdf817fcb91dbd20ee90007f06"
   },
   "source": [
    "## Scaled Dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "92c050cb313508d5c88b288dd1561493bcfacbed"
   },
   "outputs": [],
   "source": [
    "class DotProdSelfAttention(Layer):\n",
    "    \"\"\"The self-attention layer as in 'Attention is all you need'.\n",
    "    paper reference: https://arxiv.org/abs/1706.03762\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=False,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(DotProdSelfAttention, self).__init__(*kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super(DotProdSelfAttention, self).get_config().copy()\n",
    "        config.update({\n",
    "            'units' : self.units, \n",
    "            'activation' : self.activation ,\n",
    "            'use_bias' : self.use_bias,\n",
    "            'kernel_initializer' : self.kernel_initializer,\n",
    "            'bias_initializer' : self.bias_initializer,\n",
    "            'kernel_regularizer' : self.kernel_regularizer,\n",
    "            'bias_regularizer' : self.bias_regularizer,\n",
    "            'activity_regularizer' : self.activity_regularizer,\n",
    "            'kernel_constraint' : self.kernel_constraint ,\n",
    "            'bias_constraint' : self.bias_constraint ,\n",
    "            'input_spec' : self.input_spec ,\n",
    "            'supports_masking' : self.supports_masking \n",
    "        })\n",
    "        return config\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        input_dim = input_shape[-1]\n",
    "        # We assume the output-dim of Q, K, V are the same\n",
    "        self.kernels = dict.fromkeys(['Q', 'K', 'V'])\n",
    "        for key, _ in self.kernels.items():\n",
    "            self.kernels[key] = self.add_weight(shape=(input_dim, self.units),\n",
    "                                                initializer=self.kernel_initializer,\n",
    "                                                name='kernel_{}'.format(key),\n",
    "                                                regularizer=self.kernel_regularizer,\n",
    "                                                constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            raise NotImplementedError\n",
    "        super(DotProdSelfAttention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        Q = K.dot(x, self.kernels['Q'])\n",
    "        K_mat = K.dot(x, self.kernels['K'])\n",
    "        V = K.dot(x, self.kernels['V'])\n",
    "        attention = K.batch_dot(Q, K.permute_dimensions(K_mat, [0, 2, 1]))\n",
    "        d_k = K.constant(self.units, dtype=K.floatx())\n",
    "        attention = attention / K.sqrt(d_k)\n",
    "        attention = K.batch_dot(K.softmax(attention, axis=-1), V)\n",
    "        return attention\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        assert input_shape[-1]\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "      \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d931bd69e53756c2f8aa8cf63d07d4c7eb02a8d9"
   },
   "source": [
    "## The Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "399ddb0b7367ac0f6bd4121396b24ac3a6d1edfe"
   },
   "outputs": [],
   "source": [
    "def encoder(input_tensor):\n",
    "    \"\"\"One encoder as in Attention Is All You Need\n",
    "    \"\"\"\n",
    "    # Sub-layer 1\n",
    "    # Multi-Head Attention\n",
    "    multiheads = []\n",
    "    d_v = embed_size // n_heads\n",
    "    for i in range(n_heads):\n",
    "        multiheads.append(DotProdSelfAttention(d_v)(input_tensor))\n",
    "    multiheads = concatenate(multiheads, axis=-1)\n",
    "    multiheads = Dense(embed_size)(multiheads)\n",
    "    multiheads = Dropout(0.1)(multiheads)\n",
    "    \n",
    "    # Residual Connection\n",
    "    res_con = add([input_tensor, multiheads])\n",
    "    # Didn't use layer normalization, use Batch Normalization instead here\n",
    "#     res_con = BatchNormalization(axis=-1)(res_con)\n",
    "    \n",
    "    # Sub-layer 2\n",
    "    # 2 Feed forward layer\n",
    "    ff1 = Dense(64, activation='relu')(res_con)\n",
    "    ff2 = Dense(embed_size)(ff1)\n",
    "    output = add([res_con, ff2])\n",
    "#     output = BatchNormalization(axis=-1)(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8740be2622e8195ed3f0c7d9568a06ddb64cd98b"
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "8bd0cd11b96080f965195da697ba34c865f5b7e2"
   },
   "outputs": [],
   "source": [
    "# https://github.com/kpot/keras-transformer/blob/master/keras_transformer/position.py\n",
    "def positional_signal(hidden_size: int, length: int,\n",
    "                      min_timescale: float = 1.0, max_timescale: float = 1e4):\n",
    "    \"\"\"\n",
    "    Helper function, constructing basic positional encoding.\n",
    "    The code is partially based on implementation from Tensor2Tensor library\n",
    "    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n",
    "    \"\"\"\n",
    "\n",
    "    if hidden_size % 2 != 0:\n",
    "        raise ValueError(\n",
    "            f\"The hidden dimension of the model must be divisible by 2.\"\n",
    "            f\"Currently it is {hidden_size}\")\n",
    "    position = K.arange(0, length, dtype=K.floatx())\n",
    "    num_timescales = hidden_size // 2\n",
    "    log_timescale_increment = K.constant(\n",
    "        (np.log(float(max_timescale) / float(min_timescale)) /\n",
    "         (num_timescales - 1)),\n",
    "        dtype=K.floatx())\n",
    "    inv_timescales = (\n",
    "            min_timescale *\n",
    "            K.exp(K.arange(num_timescales, dtype=K.floatx()) *\n",
    "                  -log_timescale_increment))\n",
    "    scaled_time = K.expand_dims(position, 1) * K.expand_dims(inv_timescales, 0)\n",
    "    signal = K.concatenate([K.sin(scaled_time), K.cos(scaled_time)], axis=1)\n",
    "    return K.expand_dims(signal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "309135be6416acde8d5088af910eee3597e25d4e"
   },
   "outputs": [],
   "source": [
    "# https://github.com/kpot/keras-transformer/blob/master/keras_transformer/position.py\n",
    "class AddPositionalEncoding(Layer):\n",
    "    \"\"\"\n",
    "    Injects positional encoding signal described in section 3.5 of the original\n",
    "    paper \"Attention is all you need\". Also a base class for more complex\n",
    "    coordinate encoding described in \"Universal Transformers\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4, **kwargs):\n",
    "        self.min_timescale = min_timescale\n",
    "        self.max_timescale = max_timescale\n",
    "        self.signal = None\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['min_timescale'] = self.min_timescale\n",
    "        config['max_timescale'] = self.max_timescale\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, length, hidden_size = input_shape\n",
    "        self.signal = positional_signal(\n",
    "            hidden_size, length, self.min_timescale, self.max_timescale)\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return inputs + self.signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "312bd7b8472de749134273fead7f939a234c551c"
   },
   "source": [
    "## Transformer Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "235ae9ecb6873475900b1ad88e3a07a161195370"
   },
   "outputs": [],
   "source": [
    "def model_transformer( n_encoder=3):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, trainable=True)(inp)\n",
    "    # Add positional encoding\n",
    "    x = AddPositionalEncoding()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    for i in range(n_encoder):\n",
    "        x = encoder(x)\n",
    "    # These are my own experiments\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "#     conc = Dense(512, activation=\"relu\")(conc)\n",
    "#     conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(20, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.layers.recurrent_v2.LSTM"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "e2f42600ff37e6c286215562afc11a1be28f0981"
   },
   "outputs": [],
   "source": [
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class BaseDataGenerator(Sequence):\n",
    "    \"\"\"A data generator\"\"\"\n",
    "    def __init__(self, list_IDs, batch_size=64, shuffle=True):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"number of steps in one epoch\"\"\"\n",
    "        # Here is the trick\n",
    "        return len(self.list_IDs) // (self.batch_size * 2**2)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size: (index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        X = train_X[list_IDs_temp, :]\n",
    "        y = train_y[list_IDs_temp]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee0611a48264188ec3063bc6c6ce221eb3724d8e"
   },
   "source": [
    "### Train and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c9d6121293c82701f3c07ae00f396564d8aa2c9"
   },
   "source": [
    "Here I used early stopping and model checkpoint to load the best_val model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "5f90446889d864f0a318d305eca2d3a04d1baa55"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "def train_pred(n_encoder = 1, epochs=2):\n",
    "    # learning schedule callback\n",
    "#     loss_history = LossHistory()\n",
    "#     lrate = BatchLRScheduler(step_decay)\n",
    "#     callbacks_list = [loss_history, lrate]\n",
    "#     es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "#     model_path = 'keras_models.h5'\n",
    "#     mc = ModelCheckpoint(filepath=model_path, monitor='val_loss', save_best_only=True)\n",
    "#     callbacks = [es, mc]\n",
    "#     train_generator = BaseDataGenerator(list(np.arange(train_X.shape[0])), batch_size=512)\n",
    "#     model.fit_generator(train_generator,\n",
    "#                         epochs=epochs,\n",
    "#                         validation_data=(val_X, val_y),)\n",
    "#                         callbacks=callbacks)\n",
    "#     model = load_model(model_path)\n",
    "    skf = StratifiedKFold(n_splits=7, random_state=0)\n",
    "    models, preds, scores = [], [],[]\n",
    "#     vectorizer = vect(max_df = 0.5)\n",
    "    for train, test in skf.split(train_df.Text, train_df.Label):\n",
    "#     print(train, test)\n",
    "#     clf = LogisticRegression(penalty='l1')\n",
    "#         clf.fit(vectorizer.transform(), data_train.Label.loc[data_train.index.intersection(train)])\n",
    "#         K.clear_session()\n",
    "#         clf = build_base_model()\n",
    "        model = model_transformer(n_encoder=n_encoder)\n",
    "        X_train = train_df.Text.loc[train_df.index.intersection(train)]\n",
    "        X_val = train_df.Text.loc[train_df.index.intersection(test)]\n",
    "        y_train = train_df[cols_target].loc[train_df.index.intersection(train)]\n",
    "        y_val = train_df[cols_target].loc[train_df.index.intersection(test)]\n",
    "        X_train = tokenizer.texts_to_sequences(X_train)\n",
    "        X_val = tokenizer.texts_to_sequences(X_val)\n",
    "        X_test = tokenizer.texts_to_sequences(test_df.Text)\n",
    "\n",
    "        ## Pad the sentences \n",
    "        X_train = pad_sequences(X_train, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        X_val = pad_sequences(X_val, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        X_test = pad_sequences(X_test, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "\n",
    "#         X_test = vect.transform(test_df.Text).toarray()\n",
    "        model.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                   validation_data = (X_val,y_val),\n",
    "                   callbacks=[\n",
    "#               RocAucEvaluation(verbose=True),\n",
    "              ModelCheckpoint(file_path,    monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "              EarlyStopping(patience=10,    monitor=\"val_accuracy\", mode=\"max\"),\n",
    "              ReduceLROnPlateau(patience=4, monitor='val_accuracy', mode='max', cooldown=2, min_lr=1e-7, factor=0.3)])\n",
    "        preds.append(model.predict(X_test))\n",
    "        models.append(model)\n",
    "        scores.append(model.evaluate(X_val,y_val))\n",
    "#         coefs.append(clf.coef_[0])\n",
    "#         clf.fit(X_train, y_train)\n",
    "#     train_time = time() - t0\n",
    "#     print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "#     t0 = time()\n",
    "#     pred = clf.predict(X_test)\n",
    "#     test_time = time() - t0\n",
    "#     print(\"test time:  %0.3fs\" % test_time)\n",
    "    pred = np.mean(preds,axis = 0)\n",
    "#     model.fit(train_X, train_y, batch_size=64,\n",
    "#               epochs=epochs,\n",
    "#               validation_data=(val_X, val_y),)\n",
    "\n",
    "#     pred_val_y = model.predict([val_X], batch_size=64, verbose=0)\n",
    "#     pred_test_y = model.predict([test_X], batch_size=64, verbose=0)\n",
    "    return models, preds, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9b632c9bea6c5e8c7e111fc97474fc9d8b099c4"
   },
   "source": [
    "### Main part: load, train, pred and blend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "6ddf2ae0c374759ef040db80346faaf609850e58"
   },
   "outputs": [],
   "source": [
    "# train_X, val_X, test_X, train_y, val_y, word_index,val_idx,trn_idx = load_and_prec()\n",
    "vocab = []\n",
    "for w,k in word_index.items():\n",
    "    vocab.append(w)\n",
    "    if k >= max_features:\n",
    "        break\n",
    "# embedding_matrix_1 = load_glove(word_index)\n",
    "# embedding_matrix_2 = load_fasttext(word_index)\n",
    "# embedding_matrix_3 = load_para(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ffc83124a9f220b31e698b922cad56d59f8c83e5"
   },
   "source": [
    "### Create New Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "622e0a2f777d15a9f74b3d1d56a0e5ad60ee90bc"
   },
   "outputs": [],
   "source": [
    "## Simple average: http://aclweb.org/anthology/N18-2031\n",
    "\n",
    "# We have presented an argument for averaging as\n",
    "# a valid meta-embedding technique, and found experimental\n",
    "# performance to be close to, or in some cases \n",
    "# better than that of concatenation, with the\n",
    "# additional benefit of reduced dimensionality  \n",
    "\n",
    "\n",
    "## Unweighted DME in https://arxiv.org/pdf/1804.07983.pdf\n",
    "\n",
    "# “The downside of concatenating embeddings and \n",
    "#  giving that as input to an RNN encoder, however,\n",
    "#  is that the network then quickly becomes inefficient\n",
    "#  as we combine more and more embeddings.”\n",
    "  \n",
    "# embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2, embedding_matrix_3], axis = 0)\n",
    "# embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_3], axis = 0)\n",
    "# np.shape(embedding_matrix)\n",
    "# model.evaluate(val_X,val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52627f8490364ed29f81e574c49af644726701e1"
   },
   "source": [
    "## Train and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3270c045c747a7c5da32704e6db44c18f470646"
   },
   "source": [
    "Here I am experimenting with 2 encoders, it's not guaranteed to be optimal, you can try out other numbers. Notice that I used epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "573d8e65c007aefd1fb4a0deef16e73894327486"
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,cross_val_score,train_test_split,StratifiedShuffleSplit\n",
    "file_path = \"weights_base.best.hdf5\"\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback, ReduceLROnPlateau\n",
    "\n",
    "# outputs[0][1]\n",
    "# type(train_X[0][0])\n",
    "# val_X\n",
    "# train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(train_X,train_y)\n",
    "# train_X.shape\n",
    "# train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "091d7915c8c00d088d60c09c306298950cd2afe6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 3.0868 - accuracy: 0.1439 - val_loss: 2.8078 - val_accuracy: 0.0874\n",
      "Epoch 2/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 2.6308 - accuracy: 0.1813 - val_loss: 2.6240 - val_accuracy: 0.1311\n",
      "Epoch 3/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 2.5645 - accuracy: 0.1919 - val_loss: 2.5386 - val_accuracy: 0.1505\n",
      "Epoch 4/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 2.5142 - accuracy: 0.2195 - val_loss: 2.6696 - val_accuracy: 0.1214\n",
      "Epoch 5/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 2.4207 - accuracy: 0.2260 - val_loss: 2.3837 - val_accuracy: 0.2621\n",
      "Epoch 6/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 2.2425 - accuracy: 0.2943 - val_loss: 2.4197 - val_accuracy: 0.2524\n",
      "Epoch 7/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 1.7882 - accuracy: 0.4480 - val_loss: 2.1543 - val_accuracy: 0.3447\n",
      "Epoch 8/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 1.4253 - accuracy: 0.5756 - val_loss: 2.1041 - val_accuracy: 0.3835\n",
      "Epoch 9/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 0.9801 - accuracy: 0.7122 - val_loss: 2.1708 - val_accuracy: 0.4272\n",
      "Epoch 10/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.5742 - accuracy: 0.8374 - val_loss: 2.5217 - val_accuracy: 0.3738\n",
      "Epoch 11/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.3042 - accuracy: 0.9195 - val_loss: 2.7439 - val_accuracy: 0.4126\n",
      "Epoch 12/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.1543 - accuracy: 0.9585 - val_loss: 3.2141 - val_accuracy: 0.4175\n",
      "Epoch 13/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.1131 - accuracy: 0.9667 - val_loss: 3.5309 - val_accuracy: 0.4272\n",
      "Epoch 14/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0390 - accuracy: 0.9927 - val_loss: 3.4058 - val_accuracy: 0.4272\n",
      "Epoch 15/42\n",
      "39/39 [==============================] - 47s 1s/step - loss: 0.0194 - accuracy: 0.9967 - val_loss: 3.3088 - val_accuracy: 0.4563\n",
      "Epoch 16/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 3.4197 - val_accuracy: 0.4369\n",
      "Epoch 17/42\n",
      "39/39 [==============================] - 49s 1s/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.3895 - val_accuracy: 0.4369\n",
      "Epoch 18/42\n",
      "39/39 [==============================] - 49s 1s/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 3.4386 - val_accuracy: 0.4563\n",
      "Epoch 19/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.5091 - val_accuracy: 0.4563\n",
      "Epoch 20/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.5061 - val_accuracy: 0.4660\n",
      "Epoch 21/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.5091 - val_accuracy: 0.4660\n",
      "Epoch 22/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 3.5148 - val_accuracy: 0.4612\n",
      "Epoch 23/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.5457 - val_accuracy: 0.4660\n",
      "Epoch 24/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0065 - accuracy: 0.9992 - val_loss: 3.6064 - val_accuracy: 0.4369\n",
      "Epoch 25/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.6103 - val_accuracy: 0.4369\n",
      "Epoch 26/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.6033 - val_accuracy: 0.4417\n",
      "Epoch 27/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.6003 - val_accuracy: 0.4417\n",
      "Epoch 28/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.6022 - val_accuracy: 0.4417\n",
      "Epoch 29/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.5929 - val_accuracy: 0.4466\n",
      "Epoch 30/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.5929 - val_accuracy: 0.4466\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 3.5929 - accuracy: 0.4466\n",
      "Epoch 1/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 2.9653 - accuracy: 0.1373 - val_loss: 2.7238 - val_accuracy: 0.1951\n",
      "Epoch 2/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 2.6321 - accuracy: 0.1803 - val_loss: 2.5808 - val_accuracy: 0.2000\n",
      "Epoch 3/42\n",
      "39/39 [==============================] - 49s 1s/step - loss: 2.5095 - accuracy: 0.2169 - val_loss: 2.5199 - val_accuracy: 0.2049\n",
      "Epoch 4/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 2.4293 - accuracy: 0.2250 - val_loss: 2.4529 - val_accuracy: 0.2049\n",
      "Epoch 5/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 2.3294 - accuracy: 0.2746 - val_loss: 2.3707 - val_accuracy: 0.3220\n",
      "Epoch 6/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 2.1200 - accuracy: 0.3347 - val_loss: 2.1710 - val_accuracy: 0.2829\n",
      "Epoch 7/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 1.7414 - accuracy: 0.4825 - val_loss: 1.9129 - val_accuracy: 0.3854\n",
      "Epoch 8/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 1.3156 - accuracy: 0.6125 - val_loss: 1.6837 - val_accuracy: 0.4927\n",
      "Epoch 9/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 0.8411 - accuracy: 0.7620 - val_loss: 1.7624 - val_accuracy: 0.4829\n",
      "Epoch 10/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 0.5187 - accuracy: 0.8448 - val_loss: 2.1514 - val_accuracy: 0.4293\n",
      "Epoch 11/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.2710 - accuracy: 0.9180 - val_loss: 2.0030 - val_accuracy: 0.4683\n",
      "Epoch 12/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.1443 - accuracy: 0.9578 - val_loss: 2.4922 - val_accuracy: 0.4585\n",
      "Epoch 13/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 0.0572 - accuracy: 0.9870 - val_loss: 2.4849 - val_accuracy: 0.4732\n",
      "Epoch 14/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0347 - accuracy: 0.9968 - val_loss: 2.5131 - val_accuracy: 0.4634\n",
      "Epoch 15/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0223 - accuracy: 0.9984 - val_loss: 2.6034 - val_accuracy: 0.4683\n",
      "Epoch 16/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 0.0223 - accuracy: 0.9992 - val_loss: 2.6864 - val_accuracy: 0.4780\n",
      "Epoch 17/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 2.7515 - val_accuracy: 0.4585\n",
      "Epoch 18/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 0.0130 - accuracy: 0.9984 - val_loss: 2.7587 - val_accuracy: 0.4488\n",
      "7/7 [==============================] - 1s 170ms/step - loss: 2.7587 - accuracy: 0.4488\n",
      "Epoch 1/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 3.0743 - accuracy: 0.1340 - val_loss: 2.9288 - val_accuracy: 0.1756\n",
      "Epoch 2/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 2.6695 - accuracy: 0.1885 - val_loss: 2.5695 - val_accuracy: 0.2488\n",
      "Epoch 3/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 2.5088 - accuracy: 0.2193 - val_loss: 2.5366 - val_accuracy: 0.2341\n",
      "Epoch 4/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 2.4397 - accuracy: 0.2445 - val_loss: 2.4176 - val_accuracy: 0.2439\n",
      "Epoch 5/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 2.3457 - accuracy: 0.2803 - val_loss: 2.3858 - val_accuracy: 0.3268\n",
      "Epoch 6/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 2.1393 - accuracy: 0.3404 - val_loss: 2.1334 - val_accuracy: 0.3415\n",
      "Epoch 7/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 1.7744 - accuracy: 0.4671 - val_loss: 1.8816 - val_accuracy: 0.3805\n",
      "Epoch 8/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 1.3465 - accuracy: 0.6109 - val_loss: 1.9740 - val_accuracy: 0.4146\n",
      "Epoch 9/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.9739 - accuracy: 0.7132 - val_loss: 1.9539 - val_accuracy: 0.4927\n",
      "Epoch 10/42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 45s 1s/step - loss: 0.5945 - accuracy: 0.8278 - val_loss: 2.1208 - val_accuracy: 0.5024\n",
      "Epoch 11/42\n",
      "39/39 [==============================] - 46s 1s/step - loss: 0.4218 - accuracy: 0.8749 - val_loss: 2.4945 - val_accuracy: 0.5122\n",
      "Epoch 12/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 0.2183 - accuracy: 0.9383 - val_loss: 2.7924 - val_accuracy: 0.4976\n",
      "Epoch 13/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 0.1131 - accuracy: 0.9708 - val_loss: 2.7173 - val_accuracy: 0.5463\n",
      "Epoch 14/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0619 - accuracy: 0.9862 - val_loss: 3.0381 - val_accuracy: 0.5122\n",
      "Epoch 15/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0456 - accuracy: 0.9919 - val_loss: 3.1936 - val_accuracy: 0.5122\n",
      "Epoch 16/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0466 - accuracy: 0.9878 - val_loss: 3.3787 - val_accuracy: 0.5073\n",
      "Epoch 17/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 0.0290 - accuracy: 0.9919 - val_loss: 3.3254 - val_accuracy: 0.5073\n",
      "Epoch 18/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 3.1536 - val_accuracy: 0.5512\n",
      "Epoch 19/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.1659 - val_accuracy: 0.5561\n",
      "Epoch 20/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.2010 - val_accuracy: 0.5610\n",
      "Epoch 21/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.2292 - val_accuracy: 0.5366\n",
      "Epoch 22/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 3.1755 - val_accuracy: 0.5659\n",
      "Epoch 23/42\n",
      "39/39 [==============================] - 39s 1s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.2204 - val_accuracy: 0.5463\n",
      "Epoch 24/42\n",
      "39/39 [==============================] - 40s 1s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.2444 - val_accuracy: 0.5610\n",
      "Epoch 25/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.2646 - val_accuracy: 0.5512\n",
      "Epoch 26/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.2870 - val_accuracy: 0.5463\n",
      "Epoch 27/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.2997 - val_accuracy: 0.5512\n",
      "Epoch 28/42\n",
      "39/39 [==============================] - 41s 1s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.2987 - val_accuracy: 0.5463\n",
      "Epoch 29/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.3058 - val_accuracy: 0.5415\n",
      "Epoch 30/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.3098 - val_accuracy: 0.5463\n",
      "Epoch 31/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.3453 - val_accuracy: 0.5366\n",
      "Epoch 32/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.3452 - val_accuracy: 0.5366\n",
      "7/7 [==============================] - 1s 178ms/step - loss: 3.3452 - accuracy: 0.5366\n",
      "Epoch 1/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 3.0641 - accuracy: 0.1292 - val_loss: 2.7160 - val_accuracy: 0.1951\n",
      "Epoch 2/42\n",
      "39/39 [==============================] - 49s 1s/step - loss: 2.6427 - accuracy: 0.1901 - val_loss: 2.5573 - val_accuracy: 0.1951\n",
      "Epoch 3/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 2.5437 - accuracy: 0.2153 - val_loss: 2.6356 - val_accuracy: 0.1951\n",
      "Epoch 4/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 2.5014 - accuracy: 0.2120 - val_loss: 2.3790 - val_accuracy: 0.2341\n",
      "Epoch 5/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 2.4342 - accuracy: 0.2567 - val_loss: 2.3699 - val_accuracy: 0.2683\n",
      "Epoch 6/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 2.3107 - accuracy: 0.2591 - val_loss: 2.1935 - val_accuracy: 0.2927\n",
      "Epoch 7/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 2.0566 - accuracy: 0.3721 - val_loss: 1.9715 - val_accuracy: 0.4390\n",
      "Epoch 8/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 1.6124 - accuracy: 0.5150 - val_loss: 1.8382 - val_accuracy: 0.4732\n",
      "Epoch 9/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 1.1493 - accuracy: 0.6726 - val_loss: 1.7348 - val_accuracy: 0.4878\n",
      "Epoch 10/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 0.6951 - accuracy: 0.8034 - val_loss: 1.6280 - val_accuracy: 0.5756\n",
      "Epoch 11/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.4584 - accuracy: 0.8627 - val_loss: 2.0296 - val_accuracy: 0.4976\n",
      "Epoch 12/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.3073 - accuracy: 0.9050 - val_loss: 2.1091 - val_accuracy: 0.5024\n",
      "Epoch 13/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 0.1677 - accuracy: 0.9504 - val_loss: 2.6466 - val_accuracy: 0.4683\n",
      "Epoch 14/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 0.0826 - accuracy: 0.9773 - val_loss: 2.4449 - val_accuracy: 0.4976\n",
      "Epoch 15/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0256 - accuracy: 0.9943 - val_loss: 2.5439 - val_accuracy: 0.4732\n",
      "Epoch 16/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0169 - accuracy: 0.9984 - val_loss: 2.5239 - val_accuracy: 0.5122\n",
      "Epoch 17/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0151 - accuracy: 0.9984 - val_loss: 2.5039 - val_accuracy: 0.5317\n",
      "Epoch 18/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 2.5515 - val_accuracy: 0.5220\n",
      "Epoch 19/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 0.0093 - accuracy: 0.9992 - val_loss: 2.5278 - val_accuracy: 0.5171\n",
      "Epoch 20/42\n",
      "39/39 [==============================] - 42s 1s/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 2.5622 - val_accuracy: 0.5220\n",
      "7/7 [==============================] - 1s 179ms/step - loss: 2.5622 - accuracy: 0.5220\n",
      "Epoch 1/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 2.9385 - accuracy: 0.1535 - val_loss: 2.7039 - val_accuracy: 0.1220\n",
      "Epoch 2/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 2.6900 - accuracy: 0.1738 - val_loss: 2.5665 - val_accuracy: 0.2439\n",
      "Epoch 3/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 2.5562 - accuracy: 0.2015 - val_loss: 2.4701 - val_accuracy: 0.2537\n",
      "Epoch 4/42\n",
      "39/39 [==============================] - 55s 1s/step - loss: 2.4864 - accuracy: 0.2283 - val_loss: 2.4021 - val_accuracy: 0.2439\n",
      "Epoch 5/42\n",
      "39/39 [==============================] - 54s 1s/step - loss: 2.3555 - accuracy: 0.2526 - val_loss: 2.3845 - val_accuracy: 0.2488\n",
      "Epoch 6/42\n",
      "39/39 [==============================] - 50s 1s/step - loss: 2.1887 - accuracy: 0.3290 - val_loss: 2.1612 - val_accuracy: 0.3463\n",
      "Epoch 7/42\n",
      "39/39 [==============================] - 59s 2s/step - loss: 1.6566 - accuracy: 0.4947 - val_loss: 1.9096 - val_accuracy: 0.4341\n",
      "Epoch 8/42\n",
      "39/39 [==============================] - 59s 2s/step - loss: 1.3424 - accuracy: 0.5890 - val_loss: 1.7022 - val_accuracy: 0.4341\n",
      "Epoch 9/42\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.8425 - accuracy: 0.7360 - val_loss: 1.8428 - val_accuracy: 0.4634\n",
      "Epoch 10/42\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.4629 - accuracy: 0.8725 - val_loss: 2.0778 - val_accuracy: 0.4439\n",
      "Epoch 11/42\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.2655 - accuracy: 0.9277 - val_loss: 2.8086 - val_accuracy: 0.4146\n",
      "Epoch 12/42\n",
      "39/39 [==============================] - 67s 2s/step - loss: 0.1571 - accuracy: 0.9553 - val_loss: 2.5647 - val_accuracy: 0.4341\n",
      "Epoch 13/42\n",
      "39/39 [==============================] - 55s 1s/step - loss: 0.0723 - accuracy: 0.9781 - val_loss: 2.5644 - val_accuracy: 0.4390\n",
      "Epoch 14/42\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.0265 - accuracy: 0.9984 - val_loss: 2.4339 - val_accuracy: 0.5073\n",
      "Epoch 15/42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 61s 2s/step - loss: 0.0126 - accuracy: 0.9984 - val_loss: 2.4909 - val_accuracy: 0.5073\n",
      "Epoch 16/42\n",
      "39/39 [==============================] - 60s 2s/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 2.4313 - val_accuracy: 0.4927\n",
      "Epoch 17/42\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.5343 - val_accuracy: 0.4976\n",
      "Epoch 18/42\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.5680 - val_accuracy: 0.5024\n",
      "Epoch 19/42\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.0059 - accuracy: 0.9992 - val_loss: 2.5721 - val_accuracy: 0.5122\n",
      "Epoch 20/42\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 2.6039 - val_accuracy: 0.5171\n",
      "Epoch 21/42\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 2.6150 - val_accuracy: 0.5073\n",
      "Epoch 22/42\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.6030 - val_accuracy: 0.5171\n",
      "Epoch 23/42\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 2.5838 - val_accuracy: 0.5268\n",
      "Epoch 24/42\n",
      "39/39 [==============================] - 52s 1s/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 2.5668 - val_accuracy: 0.5220\n",
      "Epoch 25/42\n",
      "39/39 [==============================] - 46s 1s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.5984 - val_accuracy: 0.5220\n",
      "Epoch 26/42\n",
      "39/39 [==============================] - 43s 1s/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.6175 - val_accuracy: 0.5171\n",
      "Epoch 27/42\n",
      "39/39 [==============================] - 50s 1s/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.6175 - val_accuracy: 0.5073\n",
      "Epoch 28/42\n",
      "39/39 [==============================] - 46s 1s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.6156 - val_accuracy: 0.5122\n",
      "Epoch 29/42\n",
      "39/39 [==============================] - 46s 1s/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 2.5964 - val_accuracy: 0.5122\n",
      "Epoch 30/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.6019 - val_accuracy: 0.5122\n",
      "Epoch 31/42\n",
      "39/39 [==============================] - 49s 1s/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.6123 - val_accuracy: 0.5122\n",
      "Epoch 32/42\n",
      "39/39 [==============================] - 52s 1s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.6191 - val_accuracy: 0.5122\n",
      "Epoch 33/42\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.6190 - val_accuracy: 0.5122\n",
      "7/7 [==============================] - 1s 178ms/step - loss: 2.6190 - accuracy: 0.5122\n",
      "Epoch 1/42\n",
      "39/39 [==============================] - 57s 1s/step - loss: 2.9242 - accuracy: 0.1365 - val_loss: 2.6617 - val_accuracy: 0.1951\n",
      "Epoch 2/42\n",
      "39/39 [==============================] - 69s 2s/step - loss: 2.6342 - accuracy: 0.1982 - val_loss: 2.6260 - val_accuracy: 0.1854\n",
      "Epoch 3/42\n",
      "39/39 [==============================] - 48s 1s/step - loss: 2.5438 - accuracy: 0.2039 - val_loss: 2.5631 - val_accuracy: 0.1659\n",
      "Epoch 4/42\n",
      "39/39 [==============================] - 72s 2s/step - loss: 2.4669 - accuracy: 0.2128 - val_loss: 2.5150 - val_accuracy: 0.1902\n",
      "Epoch 5/42\n",
      "39/39 [==============================] - 80s 2s/step - loss: 2.3389 - accuracy: 0.2721 - val_loss: 2.3745 - val_accuracy: 0.2780\n",
      "Epoch 6/42\n",
      "39/39 [==============================] - 79s 2s/step - loss: 2.0652 - accuracy: 0.3582 - val_loss: 2.2961 - val_accuracy: 0.3122\n",
      "Epoch 7/42\n",
      "39/39 [==============================] - 66s 2s/step - loss: 1.7327 - accuracy: 0.4476 - val_loss: 1.9393 - val_accuracy: 0.4098\n",
      "Epoch 8/42\n",
      "39/39 [==============================] - 59s 2s/step - loss: 1.3207 - accuracy: 0.5768 - val_loss: 2.0902 - val_accuracy: 0.4098\n",
      "Epoch 9/42\n",
      "39/39 [==============================] - 80s 2s/step - loss: 0.9750 - accuracy: 0.6872 - val_loss: 2.3917 - val_accuracy: 0.4488\n",
      "Epoch 10/42\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.7168 - accuracy: 0.7725 - val_loss: 2.1593 - val_accuracy: 0.4878\n",
      "Epoch 11/42\n",
      "39/39 [==============================] - 72s 2s/step - loss: 0.3921 - accuracy: 0.8863 - val_loss: 2.4940 - val_accuracy: 0.4829\n",
      "Epoch 12/42\n",
      "39/39 [==============================] - 81s 2s/step - loss: 0.2397 - accuracy: 0.9293 - val_loss: 2.5441 - val_accuracy: 0.4732\n",
      "Epoch 13/42\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.1063 - accuracy: 0.9634 - val_loss: 2.8485 - val_accuracy: 0.4780\n",
      "Epoch 14/42\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.0812 - accuracy: 0.9789 - val_loss: 3.2133 - val_accuracy: 0.4780\n",
      "Epoch 15/42\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.0278 - accuracy: 0.9976 - val_loss: 2.8352 - val_accuracy: 0.5171\n",
      "Epoch 16/42\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.0163 - accuracy: 0.9976 - val_loss: 2.8631 - val_accuracy: 0.5073\n",
      "Epoch 17/42\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.9128 - val_accuracy: 0.5122\n",
      "Epoch 18/42\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.9814 - val_accuracy: 0.5073\n",
      "Epoch 19/42\n",
      "39/39 [==============================] - 70s 2s/step - loss: 0.0095 - accuracy: 0.9992 - val_loss: 2.9107 - val_accuracy: 0.5024\n",
      "Epoch 20/42\n",
      "39/39 [==============================] - 71s 2s/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.9029 - val_accuracy: 0.5024\n",
      "Epoch 21/42\n",
      "39/39 [==============================] - 53s 1s/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.9242 - val_accuracy: 0.5122\n",
      "Epoch 22/42\n",
      "39/39 [==============================] - 48s 1s/step - loss: 0.0070 - accuracy: 0.9984 - val_loss: 2.9407 - val_accuracy: 0.5122\n",
      "Epoch 23/42\n",
      "39/39 [==============================] - 47s 1s/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.9395 - val_accuracy: 0.4927\n",
      "Epoch 24/42\n",
      "39/39 [==============================] - 49s 1s/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.9601 - val_accuracy: 0.4927\n",
      "Epoch 25/42\n",
      "39/39 [==============================] - 78s 2s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.9622 - val_accuracy: 0.4927\n",
      "7/7 [==============================] - 3s 454ms/step - loss: 2.9622 - accuracy: 0.4927\n",
      "Epoch 1/42\n",
      "39/39 [==============================] - 61s 2s/step - loss: 3.0546 - accuracy: 0.1519 - val_loss: 3.0326 - val_accuracy: 0.1902\n",
      "Epoch 2/42\n",
      "39/39 [==============================] - 65s 2s/step - loss: 2.6586 - accuracy: 0.1738 - val_loss: 2.6672 - val_accuracy: 0.1902\n",
      "Epoch 3/42\n",
      "39/39 [==============================] - 72s 2s/step - loss: 2.5477 - accuracy: 0.1868 - val_loss: 2.6030 - val_accuracy: 0.1854\n",
      "Epoch 4/42\n",
      "39/39 [==============================] - 67s 2s/step - loss: 2.4625 - accuracy: 0.2266 - val_loss: 2.5081 - val_accuracy: 0.2000\n",
      "Epoch 5/42\n",
      "39/39 [==============================] - 66s 2s/step - loss: 2.3655 - accuracy: 0.2729 - val_loss: 2.4934 - val_accuracy: 0.2683\n",
      "Epoch 6/42\n",
      "39/39 [==============================] - 75s 2s/step - loss: 2.1454 - accuracy: 0.3363 - val_loss: 2.2758 - val_accuracy: 0.2829\n",
      "Epoch 7/42\n",
      "39/39 [==============================] - 64s 2s/step - loss: 1.6800 - accuracy: 0.4825 - val_loss: 2.0429 - val_accuracy: 0.3902\n",
      "Epoch 8/42\n",
      "39/39 [==============================] - 52s 1s/step - loss: 1.2622 - accuracy: 0.6279 - val_loss: 1.8929 - val_accuracy: 0.4927\n",
      "Epoch 9/42\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.9129 - accuracy: 0.7181 - val_loss: 2.3371 - val_accuracy: 0.4829\n",
      "Epoch 10/42\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.5173 - accuracy: 0.8375 - val_loss: 2.3201 - val_accuracy: 0.5171\n",
      "Epoch 11/42\n",
      "39/39 [==============================] - 67s 2s/step - loss: 0.3024 - accuracy: 0.9098 - val_loss: 2.7019 - val_accuracy: 0.5073\n",
      "Epoch 12/42\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.1811 - accuracy: 0.9488 - val_loss: 2.8755 - val_accuracy: 0.4585\n",
      "Epoch 13/42\n",
      "39/39 [==============================] - 59s 2s/step - loss: 0.0857 - accuracy: 0.9773 - val_loss: 2.7087 - val_accuracy: 0.5171\n",
      "Epoch 14/42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 61s 2s/step - loss: 0.0390 - accuracy: 0.9919 - val_loss: 3.1598 - val_accuracy: 0.4829\n",
      "Epoch 15/42\n",
      "39/39 [==============================] - 54s 1s/step - loss: 0.0193 - accuracy: 0.9951 - val_loss: 2.9338 - val_accuracy: 0.5073\n",
      "Epoch 16/42\n",
      "39/39 [==============================] - 47s 1s/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 2.8674 - val_accuracy: 0.5024\n",
      "Epoch 17/42\n",
      "39/39 [==============================] - 46s 1s/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 2.8962 - val_accuracy: 0.5073\n",
      "Epoch 18/42\n",
      "39/39 [==============================] - 45s 1s/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.9063 - val_accuracy: 0.5024\n",
      "Epoch 19/42\n",
      "39/39 [==============================] - 44s 1s/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.9481 - val_accuracy: 0.5073\n",
      "Epoch 20/42\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.0050 - accuracy: 0.9992 - val_loss: 2.9358 - val_accuracy: 0.5122\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 2.9358 - accuracy: 0.5122\n"
     ]
    }
   ],
   "source": [
    "n_encoder = 1\n",
    "models, preds, scores = train_pred(n_encoder = n_encoder,epochs = 42)\n",
    "# outputs.append([pred_val_y, pred_test_y, 'transformer_enc{}'.format(n_encoder)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "e3dc6e5362fa13e00b291986b94ea9d6e5acdebf"
   },
   "outputs": [],
   "source": [
    "# for thresh in np.arange(0.1, 0.51, 0.01):\n",
    "#     thresh = np.round(thresh, 2)\n",
    "#     print(\"F1 score at threshold {0:.2f} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))\n",
    "models_trans = models\n",
    "# scores\n",
    "# preds\n",
    "# np.mean(preds,axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "43e0969fd3af4b65decfbba8cce0a8a36d552176"
   },
   "outputs": [],
   "source": [
    "# pred_test_y = (pred_test_y > 0.42).astype(int)\n",
    "# test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\n",
    "# out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "# out_df['prediction'] = pred_test_y\n",
    "# out_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "c378fe1cfd53529831f3928fb9866329eb7a9184"
   },
   "outputs": [],
   "source": [
    "# idx = (pred_test_y > 0.42).astype(int)\n",
    "# test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\n",
    "# out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "# out_df['prediction'] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "c378fe1cfd53529831f3928fb9866329eb7a9184"
   },
   "outputs": [],
   "source": [
    "# mylist = out_df[out_df.prediction == 1].index\n",
    "# for i in mylist:\n",
    "#     print(i, end=',')\n",
    "\n",
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a3b5a15d3745fa2ffb2e0bfd1c98ba890e27550"
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model)\n",
    "train_df = pd.read_csv(\"../Translated/cleaned/train.csv\")\n",
    "test_df = pd.read_csv(\"../Translated/cleaned/test.csv\")\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "#     text = re.sub(r\"what's\", \"what is \", text)\n",
    "#     text = re.sub(r\"\\'s\", \" \", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "#     text = re.sub(r\"can't\", \"cannot \", text)\n",
    "#     text = re.sub(r\"n't\", \" not \", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "#     text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(r\",\", \" \", text) \n",
    "    text = re.sub(r\"!\", \" \", text) \n",
    "    text = re.sub(r\"\\(\", \" \", text) \n",
    "    text = re.sub(r\"\\)\", \" \", text) \n",
    "    text = re.sub(r\"\\?\", \" \", text) \n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  \n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "# removing stop words\n",
    "# other_stop_w = pd.read_csv('../Downloaded_notebooks/words_shared_by_all.csv')\n",
    "# stopw = [item for sublist in other_stop_w.values.tolist() for item in sublist]\n",
    "# train_df['Text'].apply(lambda x: [item for item in x.split() if item not in stopw])\n",
    "# test_df['Text'].apply(lambda x: [item for item in x.split() if item not in stopw])\n",
    "\n",
    "train_df['Text'] = train_df['Text'].map(lambda com : clean_text(com))\n",
    "test_df['Text'] = test_df['Text'].map(lambda com : clean_text(com))\n",
    "X_tfidf = train_df.Text\n",
    "test_X_tfidf = test_df.Text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(max_features=45000,sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "\n",
    "X_dtm = vect.fit_transform(X_tfidf).toarray()\n",
    "\n",
    "test_X_dtm = vect.transform(test_X_tfidf).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_AASHwXxg</td>\n",
       "      <td>mwangonde: khansala wachinyamata akamati achin...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_AGoFySzn</td>\n",
       "      <td>mcp siidakhutire ndi kalembera chipani cha mal...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_AGrrkBGP</td>\n",
       "      <td>bungwe la manepo lapempha boma liganizire anth...</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_AIJeigeG</td>\n",
       "      <td>ndale zogawanitsa miyambo zanyanya si zachilen...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_APMprMbV</td>\n",
       "      <td>nanga wapolisi ataphofomoka masiku ano sichikh...</td>\n",
       "      <td>LAW/ORDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>ID_zmTmmEio</td>\n",
       "      <td>eni minibus ati ali ndi ufulu wokweza mitengo ...</td>\n",
       "      <td>TRANSPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>ID_znOlIaGQ</td>\n",
       "      <td>kachali apepesa: kulankhula motumbwa kuthe ant...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>ID_znracTjN</td>\n",
       "      <td>mawu supports non-fiction writers the malawi w...</td>\n",
       "      <td>EDUCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>ID_ztdsmmva</td>\n",
       "      <td>tame mwawa: phwete ndiye kudya kwake sewero la...</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>ID_zteydTpN</td>\n",
       "      <td>pac iunguza za boma la chifedulo nthumwi zomwe...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1436 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                               Text  \\\n",
       "0     ID_AASHwXxg  mwangonde: khansala wachinyamata akamati achin...   \n",
       "1     ID_AGoFySzn  mcp siidakhutire ndi kalembera chipani cha mal...   \n",
       "2     ID_AGrrkBGP  bungwe la manepo lapempha boma liganizire anth...   \n",
       "3     ID_AIJeigeG  ndale zogawanitsa miyambo zanyanya si zachilen...   \n",
       "4     ID_APMprMbV  nanga wapolisi ataphofomoka masiku ano sichikh...   \n",
       "...           ...                                                ...   \n",
       "1431  ID_zmTmmEio  eni minibus ati ali ndi ufulu wokweza mitengo ...   \n",
       "1432  ID_znOlIaGQ  kachali apepesa: kulankhula motumbwa kuthe ant...   \n",
       "1433  ID_znracTjN  mawu supports non-fiction writers the malawi w...   \n",
       "1434  ID_ztdsmmva  tame mwawa: phwete ndiye kudya kwake sewero la...   \n",
       "1435  ID_zteydTpN  pac iunguza za boma la chifedulo nthumwi zomwe...   \n",
       "\n",
       "              Label  \n",
       "0          POLITICS  \n",
       "1          POLITICS  \n",
       "2            HEALTH  \n",
       "3          POLITICS  \n",
       "4         LAW/ORDER  \n",
       "...             ...  \n",
       "1431      TRANSPORT  \n",
       "1432       POLITICS  \n",
       "1433      EDUCATION  \n",
       "1434  SOCIAL ISSUES  \n",
       "1435       POLITICS  \n",
       "\n",
       "[1436 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train_df['Label'])\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= lb.classes_)\n",
    "# # y_train\n",
    "cols_target = train_df['Label'].unique().tolist()\n",
    "train_df = pd.concat([train_df, y_train], axis = 1)\n",
    "# # train_df\n",
    "\n",
    "# x_train, x_val, y_train, y_val = train_test_split(X_dtm, train_df[cols_target], test_size=0.1, random_state = 0,stratify = train_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idx = list(set(X_tfidf.index.tolist()) - set(val_idx.tolist()))\n",
    "# base_model.evaluate(X_dtm[val_idx],train_df.loc[val_idx,cols_target])\n",
    "# (y_val == val_y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y.shape\n",
    "# len(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "# Build the model\n",
    "from keras import backend as K \n",
    "\n",
    "# Do some code, e.g. train and save model\n",
    "\n",
    "# K.clear_session()\n",
    "# seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "# os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "# random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "# np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "# tf.random.set_seed(seed_value)\n",
    "def build_base_model():\n",
    "    K.clear_session()\n",
    "    seed_value = 0\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    base_model = models.Sequential()\n",
    "    base_model.add(layers.Dense(1000, input_shape=(45000,)))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    base_model.add(layers.Activation('linear'))\n",
    "    base_model.add(layers.Dropout(0.2))\n",
    "    # model.add(layers.Dense(2048))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Activation('relu'))\n",
    "    # model.add(layers.Dense(512))\n",
    "    # # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Activation('relu'))\n",
    "    # model.add(layers.Dense(128))\n",
    "    # # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Activation('relu'))\n",
    "\n",
    "    # model.add(layers.Dropout(drop_ratio))\n",
    "    base_model.add(layers.Dense(20))\n",
    "    base_model.add(layers.Activation('softmax'))\n",
    "\n",
    "    base_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_dtm[train_idx].shape\n",
    "# base_model\n",
    "# history = base_model.fit(x_train, y_train,\n",
    "#                     batch_size=64,\n",
    "#                     epochs=10,\n",
    "#                     verbose=1,\n",
    "#                    validation_split = 0.1)\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback, ReduceLROnPlateau\n",
    "file_path = \"weights_base.best.hdf5\"\n",
    "def benchmark():\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "#     print(clf)\n",
    "#     t0 = time()\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=0)\n",
    "    models, preds, scores = [], [],[]\n",
    "#     vectorizer = vect(max_df = 0.5)\n",
    "    for train, test in skf.split(train_df.Text, train_df.Label):\n",
    "#     print(train, test)\n",
    "#     clf = LogisticRegression(penalty='l1')\n",
    "#         clf.fit(vectorizer.transform(), data_train.Label.loc[data_train.index.intersection(train)])\n",
    "#         K.clear_session()\n",
    "        clf = build_base_model()\n",
    "        X_train = train_df.Text.loc[train_df.index.intersection(train)]\n",
    "        X_val = train_df.Text.loc[train_df.index.intersection(test)]\n",
    "        y_train = train_df[cols_target].loc[train_df.index.intersection(train)]\n",
    "        y_val = train_df[cols_target].loc[train_df.index.intersection(test)]\n",
    "        X_train = vect.transform(X_train).toarray()\n",
    "        X_val = vect.transform(X_val).toarray()\n",
    "        X_test = vect.transform(test_df.Text).toarray()\n",
    "        clf.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                   validation_data = (X_val,y_val),\n",
    "                   callbacks=[\n",
    "#               RocAucEvaluation(verbose=True),\n",
    "              ModelCheckpoint(file_path,    monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "              EarlyStopping(patience=10,    monitor=\"val_accuracy\", mode=\"max\"),\n",
    "              ReduceLROnPlateau(patience=4, monitor='val_accuracy', mode='max', cooldown=2, min_lr=1e-7, factor=0.3)])\n",
    "        preds.append(clf.predict(X_test))\n",
    "        models.append(clf)\n",
    "        scores.append(clf.evaluate(X_val,y_val))\n",
    "#         coefs.append(clf.coef_[0])\n",
    "#         clf.fit(X_train, y_train)\n",
    "#     train_time = time() - t0\n",
    "#     print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "#     t0 = time()\n",
    "#     pred = clf.predict(X_test)\n",
    "#     test_time = time() - t0\n",
    "#     print(\"test time:  %0.3fs\" % test_time)\n",
    "    pred = np.mean(preds,axis = 0)\n",
    "#     score = metrics.accuracy_score(data_test.Label, pred)\n",
    "#     print(\"accuracy:   %0.3f\" % score)\n",
    "    return models, pred,scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 26s 734ms/step - loss: 2.4265 - accuracy: 0.2570 - val_loss: 2.0137 - val_accuracy: 0.4965\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 28s 785ms/step - loss: 0.9121 - accuracy: 0.8476 - val_loss: 1.5483 - val_accuracy: 0.5660\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 23s 650ms/step - loss: 0.2105 - accuracy: 0.9852 - val_loss: 1.4089 - val_accuracy: 0.6042\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 26s 715ms/step - loss: 0.0562 - accuracy: 0.9965 - val_loss: 1.3679 - val_accuracy: 0.6111\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 24s 679ms/step - loss: 0.0271 - accuracy: 0.9974 - val_loss: 1.3533 - val_accuracy: 0.6076\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 18s 509ms/step - loss: 0.0207 - accuracy: 0.9983 - val_loss: 1.3456 - val_accuracy: 0.6146\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 20s 567ms/step - loss: 0.0137 - accuracy: 0.9991 - val_loss: 1.3410 - val_accuracy: 0.6146\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 13s 372ms/step - loss: 0.0117 - accuracy: 0.9983 - val_loss: 1.3376 - val_accuracy: 0.6146\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 24s 658ms/step - loss: 0.0113 - accuracy: 0.9974 - val_loss: 1.3373 - val_accuracy: 0.6146\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 16s 449ms/step - loss: 0.0095 - accuracy: 0.9974 - val_loss: 1.3357 - val_accuracy: 0.6146\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 1.3357 - accuracy: 0.6146\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 31s 849ms/step - loss: 2.4225 - accuracy: 0.2594 - val_loss: 1.9056 - val_accuracy: 0.4843\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 18s 487ms/step - loss: 0.8867 - accuracy: 0.8503 - val_loss: 1.4251 - val_accuracy: 0.6272\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 25s 700ms/step - loss: 0.1887 - accuracy: 0.9878 - val_loss: 1.3182 - val_accuracy: 0.6481\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 24s 663ms/step - loss: 0.0454 - accuracy: 0.9991 - val_loss: 1.2889 - val_accuracy: 0.6516\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 24s 664ms/step - loss: 0.0233 - accuracy: 0.9991 - val_loss: 1.2741 - val_accuracy: 0.6411\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 15s 427ms/step - loss: 0.0156 - accuracy: 0.9991 - val_loss: 1.2667 - val_accuracy: 0.6376\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 25s 696ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 1.2649 - val_accuracy: 0.6376\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 16s 431ms/step - loss: 0.0087 - accuracy: 0.9991 - val_loss: 1.2637 - val_accuracy: 0.6376\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 12s 339ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2627 - val_accuracy: 0.6376\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 18s 499ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.2624 - val_accuracy: 0.6376\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.2624 - accuracy: 0.6376\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 19s 519ms/step - loss: 2.4322 - accuracy: 0.2681 - val_loss: 1.8922 - val_accuracy: 0.5122\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 15s 426ms/step - loss: 0.9213 - accuracy: 0.8268 - val_loss: 1.3763 - val_accuracy: 0.6411\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 16s 443ms/step - loss: 0.1967 - accuracy: 0.9896 - val_loss: 1.2398 - val_accuracy: 0.6585\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 19s 534ms/step - loss: 0.0491 - accuracy: 0.9983 - val_loss: 1.1999 - val_accuracy: 0.6760\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 12s 342ms/step - loss: 0.0257 - accuracy: 0.9974 - val_loss: 1.1837 - val_accuracy: 0.6725\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 13s 358ms/step - loss: 0.0186 - accuracy: 0.9983 - val_loss: 1.1760 - val_accuracy: 0.6690\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 0.0160 - accuracy: 0.9983 - val_loss: 1.1692 - val_accuracy: 0.6690\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 13s 368ms/step - loss: 0.0104 - accuracy: 0.9983 - val_loss: 1.1655 - val_accuracy: 0.6690\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 14s 402ms/step - loss: 0.0081 - accuracy: 0.9991 - val_loss: 1.1643 - val_accuracy: 0.6690\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 27s 740ms/step - loss: 0.0075 - accuracy: 0.9991 - val_loss: 1.1638 - val_accuracy: 0.6690\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 1.1638 - accuracy: 0.6690\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[45000,1000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:RandomUniform]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-dfcdca400f87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mbase_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-f20bdb7287ea>\u001b[0m in \u001b[0;36mbenchmark\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#         clf.fit(vectorizer.transform(), data_train.Label.loc[data_train.index.intersection(train)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#         K.clear_session()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_base_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-4522507f1ab0>\u001b[0m in \u001b[0;36mbuild_base_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mbase_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m45000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;31m# model.add(layers.BatchNormalization())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    204\u001b[0m           \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m           \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m           \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 926\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[1;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2641\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2642\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2643\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint:disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2644\u001b[0m       \u001b[1;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2645\u001b[0m       \u001b[1;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m         trainable=True)\n\u001b[0m\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         caching_device=caching_device)\n\u001b[0m\u001b[0;32m    615\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m       \u001b[1;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    748\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m         **kwargs_for_getter)\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[1;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[0;32m    143\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[1;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         shape=shape)\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m                         shape=None):\n\u001b[0;32m    198\u001b[0m     \u001b[1;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2596\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2597\u001b[1;33m         shape=shape)\n\u001b[0m\u001b[0;32m   2598\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2599\u001b[0m     return variables.RefVariable(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m   1516\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m   def _init_from_args(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[0;32m   1649\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Initializer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[1;32m-> 1651\u001b[1;33m                 \u001b[0minitial_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1652\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[0;32m   1653\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    395\u001b[0m        \u001b[1;33m(\u001b[0m\u001b[0mvia\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_floatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \"\"\"\n\u001b[1;32m--> 397\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVarianceScaling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_get_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[0;32m   1042\u001b[0m       \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m     return op(\n\u001b[1;32m-> 1044\u001b[1;33m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m       result = gen_random_ops.random_uniform(\n\u001b[1;32m--> 302\u001b[1;33m           shape, dtype, seed=seed1, seed2=seed2)\n\u001b[0m\u001b[0;32m    303\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mminval_is_zero\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmaxval_is_one\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[0;32m    724\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[45000,1000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:RandomUniform]"
     ]
    }
   ],
   "source": [
    "# base_model.evaluate(x_val,y_val)\n",
    "# train_X.shape\n",
    "# (y_train == train_y).all()\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,cross_val_score,train_test_split,StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron,LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "base_models, pred,scores = benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.335684061050415, 0.6145833134651184],\n",
       " [1.2624201774597168, 0.6376306414604187],\n",
       " [1.1637887954711914, 0.6689895391464233],\n",
       " [1.1812516450881958, 0.6236934065818787],\n",
       " [1.3277735710144043, 0.6097561120986938]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.sequential.Sequential at 0x1dd41b4b608>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1dd41d2e608>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1dd423f9388>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1dd441a9508>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1dd479cafc8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1dd49cc5148>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1dd39ecad48>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_model.trainable = False\n",
    "# model.trainable = False\n",
    "base_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_ADHEtjTi</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_AHfJktdQ</td>\n",
       "      <td>RELIGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_AUJIHpZr</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_AUKYBbIM</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_AZnsVPEi</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ID_zdpOUWyJ</td>\n",
       "      <td>SOCIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>ID_zhnOomuu</td>\n",
       "      <td>RELATIONSHIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>ID_zmWHvBJb</td>\n",
       "      <td>LAW/ORDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>ID_zphjdFIb</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>ID_ztdtrNxt</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID          Label\n",
       "0    ID_ADHEtjTi  SOCIAL ISSUES\n",
       "1    ID_AHfJktdQ       RELIGION\n",
       "2    ID_AUJIHpZr  SOCIAL ISSUES\n",
       "3    ID_AUKYBbIM  SOCIAL ISSUES\n",
       "4    ID_AZnsVPEi  SOCIAL ISSUES\n",
       "..           ...            ...\n",
       "615  ID_zdpOUWyJ         SOCIAL\n",
       "616  ID_zhnOomuu  RELATIONSHIPS\n",
       "617  ID_zmWHvBJb      LAW/ORDER\n",
       "618  ID_zphjdFIb  SOCIAL ISSUES\n",
       "619  ID_ztdtrNxt       POLITICS\n",
       "\n",
       "[620 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train.shape\n",
    "# X_dtm\n",
    "# lb.inverse_transform(pred)\n",
    "# lb.inverse_transform(pd.DataFrame(pred,columns = cols_target)[lb.classes_].values)\n",
    "pred = np.mean(preds,axis = 0)\n",
    "\n",
    "test_df['Label']= lb.inverse_transform(pd.DataFrame(pred,columns = cols_target)[lb.classes_].values)\n",
    "sub = test_df[['ID', 'Label']]\n",
    "sub.to_csv('cross_enc_001.csv', index = False)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"../Translated/cleaned/train.csv\")\n",
    "# test_df = pd.read_csv(\"../Translated/cleaned/test.csv\")\n",
    "\n",
    "K.clear_session()\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_supermodel():\n",
    "    K.clear_session()\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "    input_trans = layers.Input(shape=(maxlen,))\n",
    "    input_tf = layers.Input(shape=(45000,))\n",
    "    output1 = []\n",
    "    for i, base_model in enumerate(base_models) : \n",
    "        base_model._name = 'base_model_'+str(i)\n",
    "        output1.append(base_model)\n",
    "    output_1 = [base_model(input_tf,training = False) for base_model in output1]\n",
    "\n",
    "    output_2 = [model(input_trans,training = False) for model in models_trans]\n",
    "\n",
    "    y = layers.Concatenate( name = 'output_1')(output_1)\n",
    "    x = layers.Concatenate()(output_2)\n",
    "    x = layers.Concatenate()([x,y])\n",
    "    # x = layers.Dense(1024, activation = 'linear')(x)\n",
    "    x = layers.Dense(512, activation = 'linear')(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation = 'sigmoid')(x)\n",
    "    x = layers.Dense(128, activation = 'linear',kernel_regularizer=regularizers.l2(0.05))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    outputs = layers.Dense(20, activation=\"softmax\")(x)\n",
    "    super_model = keras.Model(inputs=[input_trans, input_tf], outputs=outputs)\n",
    "    super_model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return super_model\n",
    "# history = super_model.fit(\n",
    "#     [train_X,x_train], y_train, batch_size=32, epochs=19, validation_split = 0.1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_stack():\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "#     print(clf)\n",
    "#     t0 = time()\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=0)\n",
    "    models, preds, scores = [], [],[]\n",
    "#     vectorizer = vect(max_df = 0.5)\n",
    "    for train, test in skf.split(train_df.Text, train_df.Label):\n",
    "#     print(train, test)\n",
    "#     clf = LogisticRegression(penalty='l1')\n",
    "#         clf.fit(vectorizer.transform(), data_train.Label.loc[data_train.index.intersection(train)])\n",
    "#         K.clear_session()\n",
    "        clf = build_supermodel()\n",
    "        X_train = train_df.Text.loc[train_df.index.intersection(train)]\n",
    "        X_val = train_df.Text.loc[train_df.index.intersection(test)]\n",
    "        y_train = train_df[cols_target].loc[train_df.index.intersection(train)]\n",
    "        y_val = train_df[cols_target].loc[train_df.index.intersection(test)]\n",
    "        \n",
    "        X_train_ker = tokenizer.texts_to_sequences(X_train)\n",
    "        X_val_ker = tokenizer.texts_to_sequences(X_val)\n",
    "        X_test_ker = tokenizer.texts_to_sequences(test_df.Text)\n",
    "\n",
    "        ## Pad the sentences \n",
    "        X_train_ker = pad_sequences(X_train_ker, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        X_val_ker = pad_sequences(X_val_ker, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        X_test_ker = pad_sequences(X_test_ker, maxlen=maxlen,padding = 'post', truncating = 'post')\n",
    "        \n",
    "        X_train_tfidf = vect.transform(X_train).toarray()\n",
    "        X_val_tfidf = vect.transform(X_val).toarray()\n",
    "        X_test_tfidf = vect.transform(test_df.Text).toarray()\n",
    "        \n",
    "        \n",
    "        clf.fit([X_train_ker, X_train_tfidf], y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                   validation_data = ([X_val_ker, X_val_tfidf],y_val),\n",
    "                   callbacks=[\n",
    "#               RocAucEvaluation(verbose=True),\n",
    "              ModelCheckpoint(file_path,    monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "              EarlyStopping(patience=10,    monitor=\"val_accuracy\", mode=\"max\"),\n",
    "              ReduceLROnPlateau(patience=4, monitor='val_accuracy', mode='max', cooldown=2, min_lr=1e-7, factor=0.3)])\n",
    "        preds.append(clf.predict([X_test_ker,X_test_tfidf]))\n",
    "        models.append(clf)\n",
    "        scores.append(clf.evaluate([X_val_ker, X_val_tfidf],y_val))\n",
    "#         coefs.append(clf.coef_[0])\n",
    "#         clf.fit(X_train, y_train)\n",
    "#     train_time = time() - t0\n",
    "#     print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "#     t0 = time()\n",
    "#     pred = clf.predict(X_test)\n",
    "#     test_time = time() - t0\n",
    "#     print(\"test time:  %0.3fs\" % test_time)\n",
    "    pred = np.mean(preds,axis = 0)\n",
    "#     score = metrics.accuracy_score(data_test.Label, pred)\n",
    "#     print(\"accuracy:   %0.3f\" % score)\n",
    "    return models, pred,scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, pred,scores = benchmark_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 38ms/step - loss: 1.8374 - accuracy: 0.6806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8373671770095825, 0.6805555820465088]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# super_model.evaluate([val_X,x_val], y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (y_val.values == val_y).all()\n",
    "# y_val.columns == lb.classes_\n",
    "preds  = super_model.predict([test_X, test_X_dtm])\n",
    "# lb.inverse_transform(pd.DataFrame(preds,columns = cols_target)[lb.classes_].values)\n",
    "test_df['Label'] = lb.inverse_transform(pd.DataFrame(preds,columns = cols_target)[lb.classes_].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X.shape\n",
    "sub = test_df[['ID','Label']]\n",
    "sub.to_csv('submission_keras_stack003.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sub.Label == pd.read_csv('submission_keras_stack002.csv').Label).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
