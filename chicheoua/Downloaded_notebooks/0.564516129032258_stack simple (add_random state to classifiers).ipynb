{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC Text MultiClass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas, xgboost, numpy, string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [abambo, odzikhweza, akuchuluka, kafukufuku, a...\n",
       "1      [ambuye, ziyaye, ayamikira, aphunzitsi, tilito...\n",
       "2      [anatcheleza, akundiopseza, gogo, wanga, akund...\n",
       "3      [ulova, wafika, posauzana, adatenga, digiri, u...\n",
       "4      [dzombe, kukoma, kuyambira, makedzana, panthaw...\n",
       "                             ...                        \n",
       "615    [kanyongolo, wapempha, oyimira, milandu, atsat...\n",
       "616    [amandimenya, zikomo, gogo, ndine, mtsikana, z...\n",
       "617    [apolisi, athotha, gulu, myp, asilikali, gulu,...\n",
       "618    [mwambo, ukwati, chitonga, mtundu, wina, uliwo...\n",
       "619    [mwapasa, autsa, mapiri, kusamvana, pakati, ap...\n",
       "Name: Text, Length: 620, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "# trainDF = pd.read_csv('../input/bbc-text.csv') # encoding = \"latin\"\n",
    "other_stop_w = pd.read_csv('words_shared_by_all.csv')\n",
    "import re\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\",\", \" \", string) \n",
    "    string = re.sub(r\"!\", \" \", string) \n",
    "    string = re.sub(r\"\\(\", \" \", string) \n",
    "    string = re.sub(r\"\\)\", \" \", string) \n",
    "    string = re.sub(r\"\\?\", \" \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)   \n",
    "    string = re.sub(\" \\d+\", \" \", string)\n",
    "    return  string.strip().lower()\n",
    "\n",
    "TRAIN_FILEPATH = \"../Translated/cleaned/train.csv\"\n",
    "TEST_FILEPATH = \"../Translated/cleaned/test.csv\"\n",
    "SS_FILEPATH = \"../data/SampleSubmission.csv\"\n",
    "VECTORS_FILEPATH = \"\"\n",
    "trainDF = pd.read_csv(TRAIN_FILEPATH)\n",
    "test = pd.read_csv(TEST_FILEPATH)\n",
    "ss = pd.read_csv(SS_FILEPATH)\n",
    "trainDF[\"Text\"] =trainDF.Text.apply(lambda x: clean_str(x))\n",
    "test[\"Text\"] =test.Text.apply(lambda x: clean_str(x))\n",
    "import tqdm\n",
    "stopw = [item for sublist in other_stop_w.values.tolist() for item in sublist]\n",
    "trainDF['Text'].apply(lambda x: [item for item in x.split() if item not in stopw]) \n",
    "test['Text'].apply(lambda x: [item for item in x.split() if item not in stopw]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDF.head(10)\n",
    "# [other_stop_w.values.tolist()]\n",
    "# pd.read_csv(TEST_FILEPATH).Text[0]\n",
    "# trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDF['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDF['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(10,10)})\n",
    "# sns.countplot(trainDF['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['Text'], trainDF['Label'],random_state = 0)\n",
    "\n",
    "train_labels = train_y\n",
    "valid_labels = valid_y\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(trainDF['Label'])\n",
    "train_y = encoder.transform(train_y)\n",
    "valid_y = encoder.transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectors as features\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(pd.concat([trainDF['Text'], test['Text']],axis = 0, ignore_index = True))\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtest_count = count_vect.transform(test['Text'])\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the train features\n",
    "# pca = PCA(n_components=2).fit(xtrain_count.toarray())\n",
    "# data2D = pca.transform(xtrain_count.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=train_labels.tolist(),size=train_labels.tolist(),palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the validation features\n",
    "# pca = PCA(n_components=2).fit(xvalid_count.toarray())\n",
    "# data2D = pca.transform(xvalid_count.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=valid_labels.tolist(),size=valid_labels.tolist(),palette=\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TF-IDF Vectors as features\n",
    " \n",
    " a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n",
    " \n",
    " b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores  of N-grams\n",
    " \n",
    " c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1800)\n",
    "tfidf_vect.fit(pd.concat([trainDF['Text'], test['Text']],axis = 0, ignore_index = True))\n",
    "\n",
    "xtest_tfidf =  tfidf_vect.transform(test['Text'])\n",
    "\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train features\n",
    "# pca = PCA(n_components=2).fit(xtrain_tfidf.toarray())\n",
    "# data2D = pca.transform(xtrain_tfidf.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=train_labels.tolist(),size=train_labels.tolist(),palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the validation features\n",
    "# pca = PCA(n_components=2).fit(xvalid_tfidf.toarray())\n",
    "# data2D = pca.transform(xvalid_tfidf.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=valid_labels.tolist(),size=valid_labels.tolist(),palette=\"husl\")\n",
    "# test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram level tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,4), max_features=1800)\n",
    "tfidf_vect_ngram.fit(pd.concat([trainDF['Text'], test['Text']],axis = 0, ignore_index = True))\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test['Text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the train features\n",
    "# pca = PCA(n_components=2).fit(xtrain_tfidf_ngram.toarray())\n",
    "# data2D = pca.transform(xtrain_tfidf_ngram.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=train_labels.tolist(),size=train_labels.tolist(),palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the validation features\n",
    "# pca = PCA(n_components=2).fit(xvalid_tfidf_ngram.toarray())\n",
    "# data2D = pca.transform(xvalid_tfidf_ngram.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=valid_labels.tolist(),size=valid_labels.tolist(),palette=\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### characters level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:501: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=1800)\n",
    "tfidf_vect_ngram_chars.fit(pd.concat([trainDF['Text'], test['Text']],axis = 0, ignore_index = True))\n",
    "\n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test['Text']) \n",
    "\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train features\n",
    "# pca = PCA(n_components=2).fit(xtrain_tfidf_ngram_chars.toarray())\n",
    "# data2D = pca.transform(xtrain_tfidf_ngram_chars.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=train_labels.tolist(),size=train_labels.tolist(),palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the validation features\n",
    "# pca = PCA(n_components=2).fit(xvalid_tfidf_ngram_chars.toarray())\n",
    "# data2D = pca.transform(xvalid_tfidf_ngram_chars.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=valid_labels.tolist(),size=valid_labels.tolist(),palette=\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting train features\n",
    "hash_vectorizer = HashingVectorizer(n_features=1800)\n",
    "hash_vectorizer.fit(pd.concat([trainDF['Text'], test['Text']],axis = 0, ignore_index = True))\n",
    "\n",
    "xtest_hash_vectorizer =  hash_vectorizer.transform(test['Text'])\n",
    "\n",
    "xtrain_hash_vectorizer =  hash_vectorizer.transform(train_x) \n",
    "xvalid_hash_vectorizer =  hash_vectorizer.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train features\n",
    "# pca = PCA(n_components=2).fit(xtrain_hash_vectorizer.toarray())\n",
    "# data2D = pca.transform(xtrain_hash_vectorizer.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=train_labels.tolist(),size=train_labels.tolist(),palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the validation features\n",
    "# pca = PCA(n_components=2).fit(xvalid_hash_vectorizer.toarray())\n",
    "# data2D = pca.transform(xvalid_hash_vectorizer.toarray())\n",
    "# cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n",
    "# ax = sns.scatterplot(data2D[:,0], data2D[:,1],\n",
    "# hue=valid_labels.tolist(),size=valid_labels.tolist(),palette=\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return classifier, metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.5933147632311978\n",
      "NB, WordLevel TF-IDF:  0.6016713091922006\n",
      "NB, N-Gram Vectors:  0.520891364902507\n",
      "NB, CharLevel Vectors:  0.5738161559888579\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "\n",
    "classifierCV, accuracy = train_model(naive_bayes.ComplementNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "classifierW, accuracy = train_model(naive_bayes.ComplementNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "classifierNgram, accuracy = train_model(naive_bayes.ComplementNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "classifierChar, accuracy = train_model(naive_bayes.ComplementNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def Voting(model1,model2, model3, model4,model5,model6,model7,model8,\n",
    "           model11,model12, model13, model14,model15,model16,model17,model18,\n",
    "           x_model1, x_model2, x_model3, x_model4, y=None ,hard = True):\n",
    "    if hard :\n",
    "        pred1 = model1.predict(x_model1)\n",
    "        pred2 = model2.predict(x_model2)\n",
    "        pred3 = model3.predict(x_model3)\n",
    "        pred4 = model4.predict(x_model4)\n",
    "        pred5 = model5.predict(x_model1)\n",
    "        pred6 = model6.predict(x_model2)\n",
    "        pred7 = model7.predict(x_model3)\n",
    "        pred8 = model8.predict(x_model4)\n",
    "        pred11 = model11.predict(x_model1)\n",
    "        pred12 = model12.predict(x_model2)\n",
    "        pred13 = model13.predict(x_model3)\n",
    "        pred14 = model14.predict(x_model4)\n",
    "        pred15 = model15.predict(x_model1)\n",
    "        pred16 = model16.predict(x_model2)\n",
    "        pred17 = model17.predict(x_model3)\n",
    "        pred18 = model18.predict(x_model4)\n",
    "        return stats.mode(([pred1,pred2, pred3 ,pred4,pred5,pred6, pred7 ,pred8,pred11,pred12, pred13 ,pred14,pred15,pred16, pred17 ,pred18])).mode.reshape(-1,1)\n",
    "    else:\n",
    "        pred1 = model1.predict_proba(x_model1)\n",
    "        pred2 = model2.predict_proba(x_model2)\n",
    "        pred3 = model3.predict_proba(x_model3)\n",
    "        pred4 = model4.predict_proba(x_model4)\n",
    "        pred5 = model5.predict_proba(x_model1)\n",
    "        pred6 = model6.predict_proba(x_model2)\n",
    "        pred7 = model7.predict_proba(x_model3)\n",
    "        pred8 = model8.predict_proba(x_model4)\n",
    "        pred11 = model11.predict_proba(x_model1)\n",
    "        pred12 = model12.predict_proba(x_model2)\n",
    "        pred13 = model13.predict_proba(x_model3)\n",
    "        pred14 = model14.predict_proba(x_model4)\n",
    "        pred15 = model15.predict_proba(x_model1)\n",
    "        pred16 = model16.predict_proba(x_model2)\n",
    "        pred17 = model17.predict_proba(x_model3)\n",
    "        pred18 = model18.predict_proba(x_model4)\n",
    "        res = np.concatenate((pred1,pred2,pred3,pred4,pred5,pred6, pred7 ,pred8, pred11,pred12, pred13 ,pred14,pred15,pred16, pred17 ,pred18),axis = 1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.inverse_transform(classifierCV.predict_proba(xtrain_count).argmax(axis = 1))\n",
    "import numpy as np\n",
    "train_preds = Voting(classifierCV,classifierW, classifierNgram, classifierChar,\n",
    "               SGDClassifierCV,SGDClassifierW,SGDClassifierNgram,SGDClassifierChar,\n",
    "               modelCV,modelW,modelNgram,modelChar,\n",
    "               RFCV,RFW,RFNgram,RFChar,\n",
    "               \n",
    "               xtrain_count, xtrain_tfidf,xtrain_tfidf_ngram, xtrain_tfidf_ngram_chars,\n",
    "               y=None ,hard = False)\n",
    "val_preds = Voting(classifierCV,classifierW, classifierNgram, classifierChar,\n",
    "               SGDClassifierCV,SGDClassifierW,SGDClassifierNgram,SGDClassifierChar,\n",
    "               modelCV,modelW,modelNgram,modelChar,\n",
    "               RFCV,RFW,RFNgram,RFChar,\n",
    "               \n",
    "               xvalid_count, xvalid_tfidf,xvalid_tfidf_ngram, xvalid_tfidf_ngram_chars,\n",
    "               y=None ,hard = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = Voting(classifierCV,classifierW, classifierNgram, classifierChar,\n",
    "               SGDClassifierCV,SGDClassifierW,SGDClassifierNgram,SGDClassifierChar,\n",
    "               modelCV,modelW,modelNgram,modelChar,\n",
    "               RFCV,RFW,RFNgram,RFChar,\n",
    "               \n",
    "               xtest_count, xtest_tfidf,xtest_tfidf_ngram, xtest_tfidf_ngram_chars,\n",
    "               y=None ,hard = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_preds = model.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lb.inverse_transform(sub_preds)\n",
    "test['Label'] = lb.inverse_transform(sub_preds)\n",
    "sub = test[['ID','Label']]\n",
    "sub.to_csv('stack_sim.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02665007, 0.06146957, 0.07237813, 0.05469247, 0.05      ,\n",
       "       0.        , 0.        , 0.23054317, 0.68944676, 0.20993159,\n",
       "       0.18384075, 0.15935537, 0.185     , 0.209     , 0.135     ,\n",
       "       0.147     ])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0,15::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = train_preds[:,140:]\n",
    "new_valid = val_preds[:,140:]\n",
    "new_test = test_preds[:,140:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1077, 180)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Super model\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "\n",
    "\n",
    "# This code was tested with TensorFlow v1.8\n",
    "# print(\"You have TensorFlow version\", tf.__version__)\n",
    "# Build the model\n",
    "from keras import backend as K \n",
    "\n",
    "# Do some code, e.g. train and save model\n",
    "\n",
    "K.clear_session()\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, input_shape=(180,)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "# model.add(layers.Dense(128, input_shape=(320,)))\n",
    "model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Activation('relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(64))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(512))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Activation('relu'))\n",
    "# model.add(layers.Dense(128))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Activation('relu'))\n",
    "\n",
    "# model.add(layers.Dropout(drop_ratio))\n",
    "model.add(layers.Dense(20))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(trainDF['Label'])\n",
    "train_bin = lb.transform(train_labels)\n",
    "valid_bin = lb.transform(valid_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 2.1772 - accuracy: 0.4349 - val_loss: 2.3741 - val_accuracy: 0.9258\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.7367 - accuracy: 0.8792 - val_loss: 1.8136 - val_accuracy: 0.9777\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.3900 - accuracy: 0.9498 - val_loss: 1.4133 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.2295 - accuracy: 0.9740 - val_loss: 1.0519 - val_accuracy: 0.9944\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.1531 - accuracy: 0.9907 - val_loss: 0.7193 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.1087 - accuracy: 0.9926 - val_loss: 0.4603 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0896 - accuracy: 0.9907 - val_loss: 0.2641 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0604 - accuracy: 0.9981 - val_loss: 0.1440 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0589 - accuracy: 0.9963 - val_loss: 0.0716 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(new_train, train_bin,\n",
    "                    batch_size=16,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 1.6310 - accuracy: 0.5961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6310241222381592, 0.5961002707481384]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(new_valid,valid_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (encoder.inverse_transform(classifierNgram.predict_proba(xtrain_tfidf_ngram).argmax(axis = 1)) == encoder.inverse_transform(classifierCV.predict_proba(xtrain_count).argmax(axis = 1))).sum()/xtrain_count.shape[0]\n",
    "# (preds>19).sum()\n",
    "\n",
    "# metrics.accuracy_score((np.argmax(,axis=1)), valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_model = ensemble.RandomForestClassifier(n_estimators=1000)\n",
    "# super_model = xgboost.XGBClassifier()\n",
    "super_model.fit(preds,train_y)\n",
    "\n",
    "preds_s = super_model.predict(Voting(classifierCV,classifierW, classifierNgram, classifierChar,\n",
    "                                     SGDClassifierCV,SGDClassifierW,SGDClassifierNgram,SGDClassifierChar,xvalid_count, xvalid_tfidf,\n",
    "                       xvalid_tfidf_ngram, xvalid_tfidf_ngram_chars, y=None ,hard = True))\n",
    "\n",
    "metrics.accuracy_score(preds_s, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.inverse_transform(classifierW.predict_proba(xtrain_tfidf).argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.inverse_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.5710306406685237\n",
      "NB, WordLevel TF-IDF:  0.5236768802228412\n",
      "NB, N-Gram Vectors:  0.45125348189415043\n",
      "NB, CharLevel Vectors:  0.5682451253481894\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "\n",
    "SGDClassifierCV, accuracy = train_model(linear_model.SGDClassifier(loss = 'modified_huber', penalty = 'l1',random_state = 0), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "SGDClassifierW,accuracy = train_model(linear_model.SGDClassifier(loss = 'modified_huber', penalty = 'l1',random_state = 0), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "SGDClassifierNgram,accuracy = train_model(linear_model.SGDClassifier(loss = 'modified_huber', penalty = 'l1',random_state = 0), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "SGDClassifierChar, accuracy = train_model(linear_model.SGDClassifier(loss = 'modified_huber', penalty = 'l1',random_state = 0), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.5905292479108635\n",
      "LR, WordLevel TF-IDF:  0.5821727019498607\n",
      "LR, N-Gram Vectors:  0.4986072423398329\n",
      "LR, CharLevel Vectors:  0.4986072423398329\n",
      "LR, Hash Vectors:  0.5264623955431755\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "modelCV, accuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "modelW, accuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "modelNgram, accuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "modelChar, accuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Hash Vectors\n",
    "modelHash, accuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_hash_vectorizer, train_y, xvalid_hash_vectorizer)\n",
    "print(\"LR, Hash Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.520891364902507\n",
      "RF, WordLevel TF-IDF:  0.5710306406685237\n",
      "RF, N-Gram Vectors:  0.45403899721448465\n",
      "RF, CharLevel Vectors:  0.5626740947075209\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "n_estimators = 1000\n",
    "RFCV, accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=n_estimators), xtrain_count, train_y, xvalid_count)\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "RFW, accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=n_estimators), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# RF on Ngram Level TF IDF Vectors\n",
    "RFNgram, accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=n_estimators), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"RF, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# RF on Character Level TF IDF Vectors\n",
    "RFChar, accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=n_estimators), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"RF, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "# RF on Hash Vectors\n",
    "# accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=n_estimators), xtrain_hash_vectorizer, train_y, xvalid_hash_vectorizer)\n",
    "# print(\"RF, Hash Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme Gradient Boosting on Count Vector\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print(\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extreme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extreme Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"Xgb, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Extreme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print(\"Xgb, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "# Extreme Gradient Boosting on Hash Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_hash_vectorizer, train_y, xvalid_hash_vectorizer)\n",
    "print(\"Xgb, Hash Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
