{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer,LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# \n",
    "# vect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_df = pd.read_csv('../Translated/cleaned/train.csv')\n",
    "test_df = pd.read_csv('../Translated/cleaned/test.csv')\n",
    "cols_target = train_df.Label.unique().tolist()\n",
    "le = LabelEncoder()\n",
    "# y_train = lb.fit_transform(train_df['Label'])\n",
    "train_df['label'] = le.fit_transform(train_df['Label'])\n",
    "# y_train = pd.DataFrame(y_train, columns= lb.classes_)\n",
    "# y_train\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# train_df = pd.concat([train_df, y_train], axis = 1)\n",
    "train_df\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "#     text = re.sub(r\"what's\", \"what is \", text)\n",
    "#     text = re.sub(r\"\\'s\", \" \", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "#     text = re.sub(r\"can't\", \"cannot \", text)\n",
    "#     text = re.sub(r\"n't\", \" not \", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "#     text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "train_df['Text'] = train_df['Text'].map(lambda com : clean_text(com))\n",
    "test_df['Text'] = test_df['Text'].map(lambda com : clean_text(com))\n",
    "X = train_df.Text\n",
    "test_X = test_df.Text\n",
    "# import and instantiate TfidfVectorizer\n",
    "other_stop_w = pd.read_csv('words_shared_by_all.csv')\n",
    "stopw = [item for sublist in other_stop_w.values.tolist() for item in sublist]\n",
    "max_features = 45000\n",
    "max_df=0.9\n",
    "tfidf_vect = TfidfVectorizer(max_features=max_features,ngram_range=(1, 5), max_df=max_df)\n",
    "count_vect = CountVectorizer(max_df = max_df)\n",
    "\n",
    "X_tfidf = tfidf_vect.fit_transform(X)\n",
    "test_X_tfidf = tfidf_vect.transform(test_X)\n",
    "\n",
    "X_count = count_vect.fit_transform(X)\n",
    "test_X_count = count_vect.transform(test_X)\n",
    "\n",
    "hash_vectorizer = HashingVectorizer(n_features=10000,norm=None,alternate_sign=False)\n",
    "X_hash = hash_vectorizer.fit_transform(X)\n",
    "test_hash = hash_vectorizer.transform(test_X)\n",
    "\n",
    "\n",
    "\n",
    "cols_target = train_df.Label.unique().tolist()\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train_df['Label'])\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= lb.classes_)\n",
    "# y_train\n",
    "\n",
    "train_df = pd.concat([train_df, y_train], axis = 1)\n",
    "# train_df\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, train_df[cols_target], test_size=0.1, random_state = 0,stratify = train_df['Label'])\n",
    "\n",
    "train_ids = y_train.index\n",
    "test_ids = y_test.index\n",
    "\n",
    "\n",
    "X_train_count = X_count[train_ids]\n",
    "X_test_count = X_count[test_ids]\n",
    "\n",
    "X_train_hash  = X_hash[train_ids]\n",
    "X_test_hash = X_hash[test_ids]\n",
    "\n",
    "\n",
    "# vectorizer\n",
    "\n",
    "# X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(X_tfidf, train_df['label'], test_size=0.2, random_state = 0,stratify = train_df['Label'])\n",
    "\n",
    "# X_count_train, X_count_test, y_count_train, y_count_test = train_test_split(X_count, train_df['label'], test_size=0.2, random_state = 0,stratify = train_df['Label'])\n",
    "\n",
    "# logreg_tfidf = LogisticRegression(C=12.0,max_iter = 1000,random_state = 0, multi_class = 'ovr', solver = 'liblinear')\n",
    "\n",
    "# logreg_count = LogisticRegression(C=12.0,max_iter = 1000,random_state = 0, multi_class = 'ovr', solver = 'liblinear')\n",
    "\n",
    "\n",
    "# logreg_tfidf.fit(X_tfidf_train,y_tfidf_train)\n",
    "\n",
    "# logreg_count.fit(X_count_train,y_count_train)\n",
    "\n",
    "# # precision_score(logreg_tfidf.predict(X_tfidf_test),y_tfidf_test, average=None)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['label']\n",
    "# train_df\n",
    "# X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'label' : lb.classes_,\n",
    "#               'precision' : precision_score(logreg_tfidf.predict(X_tfidf_test),y_tfidf_test, average=None),\n",
    "#               'recall' : recall_score(logreg_tfidf.predict(X_tfidf_test),y_tfidf_test,average = None),\n",
    "#              'accuracy' : accuracy_score(logreg_tfidf.predict(X_tfidf_test),y_tfidf_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model_vect(count_vect, train_df,model,lb):\n",
    "# #     lb = LabelEncoder()\n",
    "#     y = lb.fit_transform(train_df['Label'])\n",
    "#     X = train_df.Text\n",
    "#     X_count = count_vect.fit_transform(X)\n",
    "#     X_count_train, X_count_test, y_count_train, y_count_test = train_test_split(X_count, y, test_size=0.2, random_state = 0,stratify = train_df['Label'])\n",
    "#     model.fit(X_count_train,y_count_train)\n",
    "#     results = pd.DataFrame({'label' : lb.classes_,\n",
    "#               'precision' : precision_score(model.predict(X_count_test),y_count_test, average=None),\n",
    "#               'recall' : recall_score(model.predict(X_count_test),y_count_test,average = None),\n",
    "#              'accuracy' : accuracy_score(model.predict(X_count_test),y_count_test)})\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_vect(tfidf_vect, train_df,logreg_tfidf,LabelEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg_tfidf = LogisticRegression(C=12.0,max_iter = 1000,random_state = 0, solver = 'liblinear')\n",
    "# count_vectorizer = CountVectorizer(ngram_range=(1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_vect(hash_vectorizer, train_df,logreg_tfidf,LabelEncoder())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_tfidf (InputLayer)          [(None, 45000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inp_hash (InputLayer)           [(None, 10000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inp_count (InputLayer)          [(None, 50464)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_tfidf (Dense)             (None, 1800)         81001800    inp_tfidf[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_hash (Dense)              (None, 1800)         18001800    inp_hash[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_count (Dense)             (None, 1800)         90837000    inp_count[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1800)         0           layer_tfidf[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1800)         0           layer_hash[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1800)         0           layer_count[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 120)          216120      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 120)          216120      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 120)          216120      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 120)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 120)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 120)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 360)          0           activation[0][0]                 \n",
      "                                                                 activation_2[0][0]               \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "out_layer (Dense)               (None, 20)           7220        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20)           0           out_layer[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 190,496,180\n",
      "Trainable params: 190,496,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "# Build the model\n",
    "from keras import backend as K \n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.layers import Dense, Embedding, Input, InputSpec, GlobalMaxPool1D, GlobalAvgPool1D, Masking,Dropout,Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback, ReduceLROnPlateau\n",
    "\n",
    "# Do some code, e.g. train and save model\n",
    "\n",
    "K.clear_session()\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Dense(1000, input_shape=(45000,)))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Activation('linear'))\n",
    "# model.add(layers.Dropout(0.2))\n",
    "# # model.add(layers.Dense(2048))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Activation('relu'))\n",
    "# # model.add(layers.Dense(512))\n",
    "# # # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Activation('relu'))\n",
    "# # model.add(layers.Dense(128))\n",
    "# # # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Activation('relu'))\n",
    "\n",
    "# # model.add(layers.Dropout(drop_ratio))\n",
    "# model.add(layers.Dense(20))\n",
    "# model.add(layers.Activation('softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "def build_model():\n",
    "    input_tfidf = Input(name='inp_tfidf',shape=45000)\n",
    "    \n",
    "    layer_tfidf = Dense(1000,name= 'layer_tfidf')(input_tfidf)\n",
    "    layer_tf = Dropout(0.2)(layer_tfidf)\n",
    "    layer_tfidf = Dense(20)(layer_tf)\n",
    "    layer_tf = Activation('softmax')(layer_tfidf)\n",
    "    \n",
    "#     layer = Embedding(max_words,300,input_length=max_len)(inputs)\n",
    "#     layer = LSTM(64)(layer)\n",
    "#     layer = Dense(20,name='lstm_layer')(layer)\n",
    "#     layer = Activation('linear')(layer)\n",
    "    \n",
    "    \n",
    "#     input_count = Input(name='inp_count',shape=50464)\n",
    "#     layer_count = Dense(1000,name= 'layer_count' )(input_count)\n",
    "#     layer_co = Dropout(0.2)(layer_count)\n",
    "#     layer_count = Dense(20)(layer_co)\n",
    "#     layer_co = Activation('softmax')(layer_count)\n",
    "    \n",
    "#     input_hash = Input(name='inp_hash',shape=10000)\n",
    "#     layer_hash = Dense(1800,name= 'layer_hash' )(input_hash)\n",
    "#     layer_ha = Dropout(0.2)(layer_hash)\n",
    "#     layer_hash = Dense(120)(layer_ha)\n",
    "#     layer_ha = Activation('sigmoid')(layer_hash)\n",
    "    \n",
    "#     conc1 = Concatenate()([layer_tf, layer_ha,layer_co])\n",
    "# #     layer = Concatenate()([layer, conc1])\n",
    "# #     layer = Dense(512,name='FC1')(layer)\n",
    "# #     layer = BatchNormalization()(layer)\n",
    "# #     layer = Activation('selu')(layer)\n",
    "# #     layer = Dropout(0.5)(layer)\n",
    "# #     layer = Dense(100,name='FC2')(layer)\n",
    "# #     layer = Activation('selu')(layer)\n",
    "    \n",
    "    layer = Dense(20,name='out_layer')(conc1)\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=[input_tfidf, input_count, input_hash],outputs=layer)\n",
    "    return model\n",
    "model = build_model()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note keep only tfidf and add attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "19/19 [==============================] - 52s 3s/step - loss: 0.0139 - accuracy: 0.9966 - val_loss: 1.3228 - val_accuracy: 0.6385\n",
      "Epoch 2/3\n",
      "19/19 [==============================] - 47s 2s/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 1.3831 - val_accuracy: 0.6385\n",
      "Epoch 3/3\n",
      "19/19 [==============================] - 49s 3s/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 1.4503 - val_accuracy: 0.6385\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_tfidf.toarray(), X_train_count.toarray(), X_train_hash.toarray()], y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                   validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 163ms/step - loss: 1.5447 - accuracy: 0.6875\n",
      "[1.5447415113449097, 0.6875]\n",
      "45/45 [==============================] - 11s 246ms/step - loss: 0.2894 - accuracy: 0.9345\n",
      "[0.289389044046402, 0.9345403909683228]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate([X_test_tfidf.toarray(), X_test_count.toarray(), X_test_hash.toarray()],y_test))\n",
    "\n",
    "print(model.evaluate([X_tfidf.toarray(), X_count.toarray(), X_hash.toarray()],train_df[cols_target]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([test_X_tfidf.toarray(), test_X_count.toarray(), test_hash.toarray()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SOCIAL ISSUES', 'RELIGION', 'RELATIONSHIPS', 'SOCIAL',\n",
       "       'SOCIAL ISSUES', 'RELIGION', 'POLITICS', 'ECONOMY', 'POLITICS',\n",
       "       'POLITICS', 'SPORTS', 'SOCIAL', 'RELIGION', 'SOCIAL ISSUES',\n",
       "       'LAW/ORDER', 'POLITICS', 'HEALTH', 'SOCIAL', 'SOCIAL', 'HEALTH',\n",
       "       'RELIGION', 'POLITICS', 'SOCIAL ISSUES', 'ECONOMY',\n",
       "       'SOCIAL ISSUES', 'POLITICS', 'RELIGION', 'POLITICS', 'FARMING',\n",
       "       'FARMING', 'LAW/ORDER', 'POLITICS', 'SPORTS',\n",
       "       'WILDLIFE/ENVIRONMENT', 'HEALTH', 'HEALTH', 'POLITICS', 'HEALTH',\n",
       "       'SOCIAL ISSUES', 'SPORTS', 'POLITICS', 'SOCIAL', 'SOCIAL',\n",
       "       'RELIGION', 'CULTURE', 'WILDLIFE/ENVIRONMENT', 'LAW/ORDER',\n",
       "       'EDUCATION', 'RELATIONSHIPS', 'SOCIAL ISSUES', 'SOCIAL ISSUES',\n",
       "       'RELIGION', 'POLITICS', 'EDUCATION', 'POLITICS', 'POLITICS',\n",
       "       'SOCIAL ISSUES', 'SOCIAL ISSUES', 'POLITICS', 'POLITICS',\n",
       "       'EDUCATION', 'SOCIAL', 'POLITICS', 'LAW/ORDER', 'RELATIONSHIPS',\n",
       "       'HEALTH', 'ECONOMY', 'HEALTH', 'LAW/ORDER', 'MUSIC', 'HEALTH',\n",
       "       'LAW/ORDER', 'SOCIAL', 'SOCIAL ISSUES', 'WILDLIFE/ENVIRONMENT',\n",
       "       'SPORTS', 'ECONOMY', 'EDUCATION', 'POLITICS', 'RELIGION',\n",
       "       'POLITICS', 'RELIGION', 'EDUCATION', 'POLITICS', 'POLITICS',\n",
       "       'SOCIAL', 'RELIGION', 'HEALTH', 'LOCALCHIEFS', 'SOCIAL',\n",
       "       'RELATIONSHIPS', 'HEALTH', 'RELATIONSHIPS', 'LAW/ORDER',\n",
       "       'SOCIAL ISSUES', 'OPINION/ESSAY', 'SOCIAL', 'SOCIAL',\n",
       "       'RELATIONSHIPS', 'SPORTS', 'LOCALCHIEFS', 'LAW/ORDER', 'POLITICS',\n",
       "       'SOCIAL ISSUES', 'POLITICS', 'ECONOMY', 'LAW/ORDER', 'ECONOMY',\n",
       "       'POLITICS', 'EDUCATION', 'SOCIAL ISSUES', 'POLITICS', 'LAW/ORDER',\n",
       "       'RELIGION', 'SOCIAL ISSUES', 'RELIGION', 'RELIGION', 'LAW/ORDER',\n",
       "       'RELIGION', 'FARMING', 'SOCIAL', 'HEALTH', 'POLITICS', 'EDUCATION',\n",
       "       'POLITICS', 'LAW/ORDER', 'POLITICS', 'SOCIAL ISSUES', 'RELIGION',\n",
       "       'HEALTH', 'HEALTH', 'POLITICS', 'ECONOMY', 'EDUCATION',\n",
       "       'LAW/ORDER', 'SOCIAL', 'SOCIAL ISSUES', 'SOCIAL', 'RELIGION',\n",
       "       'SOCIAL ISSUES', 'FARMING', 'SOCIAL ISSUES', 'RELIGION',\n",
       "       'RELIGION', 'POLITICS', 'RELIGION', 'FARMING', 'POLITICS',\n",
       "       'POLITICS', 'SOCIAL ISSUES', 'POLITICS', 'POLITICS', 'POLITICS',\n",
       "       'SOCIAL', 'SOCIAL ISSUES', 'HEALTH', 'LAW/ORDER', 'POLITICS',\n",
       "       'POLITICS', 'LAW/ORDER', 'OPINION/ESSAY', 'ECONOMY', 'LOCALCHIEFS',\n",
       "       'RELIGION', 'SPORTS', 'SOCIAL', 'SPORTS', 'WITCHCRAFT',\n",
       "       'OPINION/ESSAY', 'LAW/ORDER', 'POLITICS', 'FARMING', 'RELIGION',\n",
       "       'RELATIONSHIPS', 'HEALTH', 'RELIGION', 'POLITICS', 'POLITICS',\n",
       "       'RELIGION', 'SOCIAL', 'LAW/ORDER', 'SOCIAL', 'SOCIAL ISSUES',\n",
       "       'RELIGION', 'FARMING', 'SOCIAL', 'HEALTH', 'POLITICS', 'FARMING',\n",
       "       'HEALTH', 'LOCALCHIEFS', 'HEALTH', 'EDUCATION', 'HEALTH', 'SPORTS',\n",
       "       'SOCIAL', 'SOCIAL', 'SPORTS', 'POLITICS', 'POLITICS', 'SOCIAL',\n",
       "       'POLITICS', 'HEALTH', 'SOCIAL', 'HEALTH', 'RELATIONSHIPS',\n",
       "       'LAW/ORDER', 'SOCIAL', 'HEALTH', 'LAW/ORDER', 'SOCIAL ISSUES',\n",
       "       'EDUCATION', 'POLITICS', 'HEALTH', 'HEALTH', 'LAW/ORDER',\n",
       "       'RELIGION', 'LAW/ORDER', 'HEALTH', 'RELATIONSHIPS', 'LAW/ORDER',\n",
       "       'HEALTH', 'HEALTH', 'SPORTS', 'SOCIAL', 'POLITICS', 'LOCALCHIEFS',\n",
       "       'POLITICS', 'POLITICS', 'HEALTH', 'FARMING', 'RELATIONSHIPS',\n",
       "       'POLITICS', 'LAW/ORDER', 'SOCIAL ISSUES', 'SOCIAL', 'SOCIAL',\n",
       "       'LAW/ORDER', 'HEALTH', 'RELATIONSHIPS', 'SOCIAL ISSUES',\n",
       "       'POLITICS', 'POLITICS', 'SPORTS', 'SOCIAL ISSUES', 'POLITICS',\n",
       "       'SOCIAL', 'POLITICS', 'RELIGION', 'HEALTH', 'SOCIAL ISSUES',\n",
       "       'SOCIAL', 'SOCIAL', 'POLITICS', 'SOCIAL ISSUES', 'LOCALCHIEFS',\n",
       "       'HEALTH', 'SPORTS', 'SOCIAL ISSUES', 'POLITICS', 'HEALTH',\n",
       "       'POLITICS', 'HEALTH', 'RELIGION', 'SOCIAL ISSUES', 'SOCIAL ISSUES',\n",
       "       'LAW/ORDER', 'POLITICS', 'HEALTH', 'RELIGION', 'POLITICS',\n",
       "       'OPINION/ESSAY', 'POLITICS', 'POLITICS', 'POLITICS',\n",
       "       'WILDLIFE/ENVIRONMENT', 'HEALTH', 'RELATIONSHIPS', 'POLITICS',\n",
       "       'LAW/ORDER', 'EDUCATION', 'SOCIAL', 'SOCIAL', 'RELATIONSHIPS',\n",
       "       'FARMING', 'RELIGION', 'LAW/ORDER', 'LAW/ORDER', 'FARMING',\n",
       "       'POLITICS', 'LAW/ORDER', 'CULTURE', 'POLITICS', 'SOCIAL',\n",
       "       'POLITICS', 'POLITICS', 'POLITICS', 'HEALTH', 'POLITICS',\n",
       "       'LAW/ORDER', 'SOCIAL', 'HEALTH', 'SOCIAL', 'POLITICS', 'LAW/ORDER',\n",
       "       'FARMING', 'LAW/ORDER', 'WILDLIFE/ENVIRONMENT', 'SOCIAL ISSUES',\n",
       "       'SOCIAL', 'SPORTS', 'POLITICS', 'RELIGION', 'POLITICS', 'POLITICS',\n",
       "       'SOCIAL ISSUES', 'POLITICS', 'FARMING', 'LAW/ORDER', 'FARMING',\n",
       "       'POLITICS', 'HEALTH', 'FARMING', 'SOCIAL', 'POLITICS', 'FARMING',\n",
       "       'SOCIAL ISSUES', 'RELIGION', 'SOCIAL ISSUES', 'RELIGION',\n",
       "       'RELIGION', 'CULTURE', 'SOCIAL', 'EDUCATION', 'SOCIAL ISSUES',\n",
       "       'FARMING', 'SOCIAL ISSUES', 'ECONOMY', 'MUSIC', 'POLITICS',\n",
       "       'HEALTH', 'POLITICS', 'HEALTH', 'SOCIAL', 'HEALTH', 'POLITICS',\n",
       "       'SOCIAL', 'HEALTH', 'SOCIAL ISSUES', 'SOCIAL ISSUES', 'POLITICS',\n",
       "       'SOCIAL ISSUES', 'LAW/ORDER', 'ECONOMY', 'SOCIAL ISSUES',\n",
       "       'ECONOMY', 'EDUCATION', 'HEALTH', 'POLITICS', 'RELIGION',\n",
       "       'RELIGION', 'FARMING', 'POLITICS', 'RELIGION', 'RELIGION',\n",
       "       'HEALTH', 'FARMING', 'FARMING', 'SOCIAL ISSUES', 'LAW/ORDER',\n",
       "       'POLITICS', 'SOCIAL ISSUES', 'LAW/ORDER', 'RELIGION', 'SOCIAL',\n",
       "       'HEALTH', 'POLITICS', 'POLITICS', 'POLITICS', 'POLITICS', 'SPORTS',\n",
       "       'SOCIAL ISSUES', 'LOCALCHIEFS', 'ECONOMY', 'SOCIAL', 'LAW/ORDER',\n",
       "       'LAW/ORDER', 'SOCIAL', 'POLITICS', 'WILDLIFE/ENVIRONMENT',\n",
       "       'POLITICS', 'SOCIAL ISSUES', 'LAW/ORDER', 'LAW/ORDER', 'POLITICS',\n",
       "       'HEALTH', 'HEALTH', 'ECONOMY', 'POLITICS', 'LAW/ORDER',\n",
       "       'WILDLIFE/ENVIRONMENT', 'RELATIONSHIPS', 'EDUCATION', 'SOCIAL',\n",
       "       'POLITICS', 'POLITICS', 'FARMING', 'SOCIAL', 'SOCIAL ISSUES',\n",
       "       'POLITICS', 'SOCIAL', 'RELIGION', 'POLITICS', 'HEALTH', 'RELIGION',\n",
       "       'RELIGION', 'LAW/ORDER', 'RELIGION', 'POLITICS', 'POLITICS',\n",
       "       'LAW/ORDER', 'LAW/ORDER', 'POLITICS', 'WITCHCRAFT', 'ECONOMY',\n",
       "       'LAW/ORDER', 'ECONOMY', 'RELIGION', 'RELIGION', 'POLITICS',\n",
       "       'HEALTH', 'POLITICS', 'LAW/ORDER', 'RELIGION', 'HEALTH',\n",
       "       'POLITICS', 'HEALTH', 'FARMING', 'RELIGION', 'HEALTH', 'ECONOMY',\n",
       "       'POLITICS', 'SPORTS', 'POLITICS', 'SOCIAL ISSUES', 'SOCIAL',\n",
       "       'POLITICS', 'POLITICS', 'POLITICS', 'SPORTS',\n",
       "       'WILDLIFE/ENVIRONMENT', 'SOCIAL', 'SOCIAL ISSUES', 'POLITICS',\n",
       "       'SPORTS', 'POLITICS', 'POLITICS', 'SOCIAL ISSUES',\n",
       "       'ARTS AND CRAFTS', 'EDUCATION', 'POLITICS', 'POLITICS',\n",
       "       'LOCALCHIEFS', 'SOCIAL ISSUES', 'POLITICS', 'WILDLIFE/ENVIRONMENT',\n",
       "       'HEALTH', 'HEALTH', 'EDUCATION', 'POLITICS', 'HEALTH', 'POLITICS',\n",
       "       'FARMING', 'POLITICS', 'POLITICS', 'SOCIAL', 'ECONOMY',\n",
       "       'RELATIONSHIPS', 'SOCIAL', 'SOCIAL', 'POLITICS', 'POLITICS',\n",
       "       'LAW/ORDER', 'POLITICS', 'ECONOMY', 'POLITICS', 'RELIGION',\n",
       "       'SOCIAL', 'RELIGION', 'HEALTH', 'EDUCATION', 'EDUCATION',\n",
       "       'SOCIAL ISSUES', 'RELIGION', 'RELIGION', 'HEALTH', 'LAW/ORDER',\n",
       "       'LAW/ORDER', 'SOCIAL', 'LAW/ORDER', 'SPORTS', 'POLITICS',\n",
       "       'SOCIAL ISSUES', 'POLITICS', 'LOCALCHIEFS', 'ECONOMY', 'FARMING',\n",
       "       'HEALTH', 'LAW/ORDER', 'WILDLIFE/ENVIRONMENT', 'FARMING',\n",
       "       'LAW/ORDER', 'RELIGION', 'FARMING', 'POLITICS', 'SOCIAL',\n",
       "       'WILDLIFE/ENVIRONMENT', 'POLITICS', 'HEALTH', 'RELATIONSHIPS',\n",
       "       'SOCIAL', 'SPORTS', 'LAW/ORDER', 'RELIGION', 'LAW/ORDER',\n",
       "       'POLITICS', 'POLITICS', 'SOCIAL ISSUES', 'POLITICS', 'HEALTH',\n",
       "       'ECONOMY', 'FARMING', 'LAW/ORDER', 'ECONOMY', 'SOCIAL', 'POLITICS',\n",
       "       'RELIGION', 'HEALTH', 'FARMING', 'SOCIAL', 'POLITICS',\n",
       "       'LOCALCHIEFS', 'POLITICS', 'RELIGION', 'POLITICS',\n",
       "       'WILDLIFE/ENVIRONMENT', 'WILDLIFE/ENVIRONMENT', 'FARMING',\n",
       "       'SOCIAL ISSUES', 'POLITICS', 'HEALTH', 'ECONOMY', 'RELIGION',\n",
       "       'SOCIAL', 'ECONOMY', 'RELIGION', 'SOCIAL', 'HEALTH', 'SPORTS',\n",
       "       'RELIGION', 'RELATIONSHIPS', 'RELATIONSHIPS', 'POLITICS',\n",
       "       'LOCALCHIEFS', 'POLITICS', 'POLITICS', 'SOCIAL ISSUES', 'RELIGION',\n",
       "       'POLITICS', 'POLITICS', 'ECONOMY', 'SOCIAL', 'POLITICS',\n",
       "       'RELIGION', 'SOCIAL ISSUES', 'SOCIAL ISSUES', 'SOCIAL', 'SOCIAL',\n",
       "       'ECONOMY', 'SOCIAL', 'POLITICS', 'RELIGION', 'LAW/ORDER',\n",
       "       'POLITICS', 'FARMING', 'LAW/ORDER', 'SPORTS', 'POLITICS', 'SOCIAL',\n",
       "       'POLITICS', 'SOCIAL ISSUES', 'POLITICS', 'SPORTS', 'LAW/ORDER',\n",
       "       'POLITICS', 'HEALTH', 'POLITICS', 'RELIGION', 'LAW/ORDER',\n",
       "       'ECONOMY', 'SPORTS', 'POLITICS', 'RELIGION', 'FARMING', 'POLITICS',\n",
       "       'SOCIAL ISSUES', 'EDUCATION', 'SPORTS', 'SOCIAL ISSUES',\n",
       "       'SOCIAL ISSUES', 'LAW/ORDER', 'LOCALCHIEFS',\n",
       "       'WILDLIFE/ENVIRONMENT', 'POLITICS', 'POLITICS', 'POLITICS',\n",
       "       'RELATIONSHIPS', 'LAW/ORDER', 'SOCIAL ISSUES', 'POLITICS'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.inverse_transform(pd.DataFrame(preds,columns = cols_target)[lb.classes_].values)\n",
    "# lb.classes_\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Label'] = lb.inverse_transform(pd.DataFrame(preds,columns = cols_target)[lb.classes_].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = test_df[['ID','Label']]\n",
    "sub.to_csv('submission_keras_stack2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X[(y_test.index)]\n",
    "(sub.Label == pd.read_csv('submission_keras_gram2.csv').Label).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
