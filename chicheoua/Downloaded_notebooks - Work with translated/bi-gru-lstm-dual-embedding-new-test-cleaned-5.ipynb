{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "WARNING:tensorflow:From <ipython-input-3-94658830a874>:38: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math, operator, csv, random, pickle,re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import MaxPooling1D, BatchNormalization, Permute, Lambda, Activation, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Dense, Embedding, Dropout, Input, GRU, merge, LSTM, Flatten, TimeDistributed, concatenate, SpatialDropout1D, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "from keras import backend as K\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, GRU, Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\",\", \" \", string) \n",
    "    string = re.sub(r\"!\", \" \", string) \n",
    "    string = re.sub(r\"\\(\", \" \", string) \n",
    "    string = re.sub(r\"\\)\", \" \", string) \n",
    "    string = re.sub(r\"\\?\", \" \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)   \n",
    "    string = re.sub(\" \\d+\", \" \", string)\n",
    "    return  string.strip().lower()\n",
    "# import pandas as pd\n",
    "TRAIN_FILEPATH = \"../Translated/cleaned/train.csv\"\n",
    "TEST_FILEPATH = \"../Translated/cleaned/test.csv\"\n",
    "SS_FILEPATH = \"../data/SampleSubmission.csv\"\n",
    "VECTORS_FILEPATH = \"\"\n",
    "train = pd.read_csv(TRAIN_FILEPATH)\n",
    "test = pd.read_csv(TEST_FILEPATH)\n",
    "submission = pd.read_csv(SS_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220\n",
    "\n",
    "\n",
    "TEXT_COLUMN = 'Text'\n",
    "list_classes = train.Label.unique().tolist()\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "# submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list_classes\n",
    "    \n",
    "# data_folder = \"../input/jigsaw-toxic-comment-classification-challenge/\"\n",
    "pretrained_folder = \"../Vectors/\"\n",
    "# train_filepath = data_folder + \"train.csv\"\n",
    "# test_filepath = data_folder + \"test.csv\"\n",
    "\n",
    "#path to a submission\n",
    "# submission_path =  data_folder + \"submission.csv\"\n",
    "\n",
    "#path to pretrained vectors\n",
    "facebook_filepath = pretrained_folder + \"wiki.ny.vec\"\n",
    "# google_filepath = pretrained_folder + \"glove840b300dtxt/glove.840B.300d.txt\"\n",
    "\n",
    "\n",
    "embedding_dim_facebook = 300\n",
    "# embedding_dim_google = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to pretrained dictionaries\n",
    "# hyphens_filepath = \"../input/cleaning-dictionaries/hyphens_dictionary.bin\"\n",
    "# misspellings_filepath = \"../input/cleaning-dictionaries/misspellings_all_dictionary.bin\"\n",
    "# merged_filepath = \"../input/cleaning-dictionaries/merged_all_dictionary.bin\"\n",
    "\n",
    "# hyphens_dict = misspellings_dict = merged_dict = {}\n",
    "# with open(hyphens_filepath, mode='rb') as file: hyphens_dict = pickle.load(file)\n",
    "# with open(misspellings_filepath, mode='rb') as file: misspellings_dict = pickle.load(file)\n",
    "# with open(merged_filepath, mode='rb') as file: merged_dict = pickle.load(file)\n",
    "    \n",
    "# print(len(hyphens_dict))\n",
    "# print(len(misspellings_dict))\n",
    "# print(len(merged_dict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples_count = train.shape[0]*0.9\n",
    "validation_samples_count = train.shape[0]-training_samples_count\n",
    "\n",
    "length_threshold = 20000 #We are going to truncate a comment if its length > threshold\n",
    "word_count_threshold = 900 #We are going to truncate a comment if it has more words than our threshold\n",
    "words_limit = 310000\n",
    "\n",
    "#We will filter all characters except alphabet characters and some punctuation\n",
    "valid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
    "valid_characters_ext = valid_characters + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
    "valid_set = set(x for x in valid_characters)\n",
    "valid_set_ext = set(x for x in valid_characters_ext)\n",
    "\n",
    "#List of some words that often appear in toxic comments\n",
    "#Sorry about the level of toxicity in it!\n",
    "# toxic_words = [\"poop\", \"crap\", \"prick\", \"twat\", \"wikipedia\", \"wiki\", \"hahahahaha\", \"lol\", \"bastard\", \"sluts\", \"slut\", \"douchebag\", \"douche\", \"blowjob\", \"nigga\", \"dumb\", \"jerk\", \"wanker\", \"wank\", \"penis\", \"motherfucker\", \"fucker\", \"fuk\", \"fucking\", \"fucked\", \"fuck\", \"bullshit\", \"shit\", \"stupid\", \"bitches\", \"bitch\", \"suck\", \"cunt\", \"dick\", \"cocks\", \"cock\", \"die\", \"kill\", \"gay\", \"jewish\", \"jews\", \"jew\", \"niggers\", \"nigger\", \"faggot\", \"fag\", \"asshole\"]\n",
    "# astericks_words = [('mother****ers', 'motherfuckers'), ('motherf*cking', 'motherfucking'), ('mother****er', 'motherfucker'), ('motherf*cker', 'motherfucker'), ('bullsh*t', 'bullshit'), ('f**cking', 'fucking'), ('f*ucking', 'fucking'), ('fu*cking', 'fucking'), ('****ing', 'fucking'), ('a**hole', 'asshole'), ('assh*le', 'asshole'), ('f******', 'fucking'), ('f*****g', 'fucking'), ('f***ing', 'fucking'), ('f**king', 'fucking'), ('f*cking', 'fucking'), ('fu**ing', 'fucking'), ('fu*king', 'fucking'), ('fuc*ers', 'fuckers'), ('f*****', 'fucking'), ('f***ed', 'fucked'), ('f**ker', 'fucker'), ('f*cked', 'fucked'), ('f*cker', 'fucker'), ('f*ckin', 'fucking'), ('fu*ker', 'fucker'), ('fuc**n', 'fucking'), ('ni**as', 'niggas'), ('b**ch', 'bitch'), ('b*tch', 'bitch'), ('c*unt', 'cunt'), ('f**ks', 'fucks'), ('f*ing', 'fucking'), ('ni**a', 'nigga'), ('c*ck', 'cock'), ('c*nt', 'cunt'), ('cr*p', 'crap'), ('d*ck', 'dick'), ('f***', 'fuck'), ('f**k', 'fuck'), ('f*ck', 'fuck'), ('fc*k', 'fuck'), ('fu**', 'fuck'), ('fu*k', 'fuck'), ('s***', 'shit'), ('s**t', 'shit'), ('sh**', 'shit'), ('sh*t', 'shit'), ('tw*t', 'twat')]\n",
    "# fasttext_misspelings = {\"'n'balls\": 'balls', \"-nazi's\": 'nazis', 'adminabuse': 'admin abuse', \"admins's\": 'admins', 'arsewipe': 'arse wipe', 'assfack': 'asshole', 'assholifity': 'asshole', 'assholivity': 'asshole', 'asshoul': 'asshole', 'asssholeee': 'asshole', 'belizeans': 'mexicans', \"blowing's\": 'blowing', 'bolivians': 'mexicans', 'celtofascists': 'fascists', 'censorshipmeisters': 'censor', 'chileans': 'mexicans', 'clerofascist': 'fascist', 'cowcrap': 'crap', 'crapity': 'crap', \"d'idiots\": 'idiots', 'deminazi': 'nazi', 'dftt': \"don't feed the troll\", 'dildohs': 'dildo', 'dramawhores': 'drama whores', 'edophiles': 'pedophiles', 'eurocommunist': 'communist', 'faggotkike': 'faggot', 'fantard': 'retard', 'fascismnazism': 'fascism', 'fascistisized': 'fascist', 'favremother': 'mother', 'fuxxxin': 'fucking', \"g'damn\": 'goddamn', 'harassmentat': 'harassment', 'harrasingme': 'harassing me', 'herfuc': 'motherfucker', 'hilterism': 'fascism', 'hitlerians': 'nazis', 'hitlerites': 'nazis', 'hubrises': 'pricks', 'idiotizing': 'idiotic', 'inadvandals': 'vandals', \"jackass's\": 'jackass', 'jiggabo': 'nigga', 'jizzballs': 'jizz balls', 'jmbass': 'dumbass', 'lejittament': 'legitimate', \"m'igger\": 'nigger', \"m'iggers\": 'niggers', 'motherfacking': 'motherfucker', 'motherfuckenkiwi': 'motherfucker', 'muthafuggas': 'niggas', 'nazisms': 'nazis', 'netsnipenigger': 'nigger', 'niggercock': 'nigger', 'niggerspic': 'nigger', 'nignog': 'nigga', 'niqqass': 'niggas', \"non-nazi's\": 'not a nazi', 'panamanians': 'mexicans', 'pedidiots': 'idiots', 'picohitlers': 'hitler', 'pidiots': 'idiots', 'poopia': 'poop', 'poopsies': 'poop', 'presumingly': 'obviously', 'propagandaanddisinformation': 'propaganda and disinformation', 'propagandaministerium': 'propaganda', 'puertoricans': 'mexicans', 'puertorricans': 'mexicans', 'pussiest': 'pussies', 'pussyitis': 'pussy', 'rayaridiculous': 'ridiculous', 'redfascists': 'fascists', 'retardzzzuuufff': 'retard', \"revertin'im\": 'reverting', 'scumstreona': 'scums', 'southamericans': 'mexicans', 'strasserism': 'fascism', 'stuptarded': 'retarded', \"t'nonsense\": 'nonsense', \"threatt's\": 'threat', 'titoists': 'communists', 'twatbags': 'douchebags', 'youbollocks': 'you bollocks'}\n",
    "# acronym_words = {} #{\"btw\":\"by the way\", \"yo\": \"you\", \"u\": \"you\", \"r\": \"are\", \"ur\": \"your\", \"ima\": \"i am going to\", \"imma\": \"i am going to\", \"i'ma\":\"i am going to\", \"cos\":\"because\", \"coz\":\"because\", \"stfu\": \"shut the fuck up\", \"wat\": \"what\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont_patterns = [\n",
    "#     (r'(W|w)on\\'t', r'will not'),\n",
    "#     (r'(C|c)an\\'t', r'can not'),\n",
    "#     (r'(I|i)\\'m', r'i am'),\n",
    "#     (r'(A|a)in\\'t', r'is not'),\n",
    "#     (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
    "#     (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
    "#     (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
    "#     (r'(\\w+)\\'s', r'\\g<1> is'),\n",
    "#     (r'(\\w+)\\'re', r'\\g<1> are'),\n",
    "#     (r'(\\w+)\\'d', r'\\g<1> would'),\n",
    "# ]\n",
    "# patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "# def split_word(word, toxic_words):\n",
    "#     if word == \"\":\n",
    "#         return \"\"\n",
    "    \n",
    "#     lower = word.lower()\n",
    "#     for toxic_word in toxic_words:\n",
    "#         start = lower.find(toxic_word)\n",
    "#         if start >= 0:\n",
    "#             end = start + len(toxic_word)\n",
    "#             result = \" \".join([word[0:start], word[start:end], split_word(word[end:], toxic_words)])\n",
    "#             return result.replace(\"  \", \" \").strip()\n",
    "#     return word\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "def word_tokenize(sentence):\n",
    "    sentence = sentence.replace(\"$\", \"s\")\n",
    "    sentence = sentence.replace(\"@\", \"a\")    \n",
    "    sentence = sentence.replace(\"!\", \" ! \")\n",
    "    sentence = sentence.replace(\"?\", \" ? \")\n",
    "    \n",
    "    return tknzr.tokenize(sentence)\n",
    "\n",
    "# def replace_url(word):\n",
    "#     if \"http://\" in word or \"www.\" in word or \"https://\" in word or \"wikipedia.org\" in word:\n",
    "#         return \"\"\n",
    "#     return word\n",
    "\n",
    "def normalize_by_dictionary(normalized_word, dictionary):\n",
    "    result = []\n",
    "    for word in normalized_word.split():\n",
    "        if word == word.upper():\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()].upper())\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()])\n",
    "            else:\n",
    "                result.append(word)\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from spacy.symbols import nsubj, VERB, dobj\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def normalize_comment(comment):\n",
    "    comment = unidecode(comment)\n",
    "    comment = comment[:length_threshold]\n",
    "    \n",
    "    normalized_words = []\n",
    "    \n",
    "    for w in astericks_words:\n",
    "        if w[0] in comment:\n",
    "            comment = comment.replace(w[0], w[1])\n",
    "        if w[0].upper() in comment:\n",
    "            comment = comment.replace(w[0].upper(), w[1].upper())\n",
    "    \n",
    "    for word in word_tokenize(comment):\n",
    "        #for (pattern, repl) in patterns:\n",
    "        #    word = re.sub(pattern, repl, word)\n",
    "\n",
    "        if word == \".\" or word == \",\":\n",
    "            normalized_words.append(word)\n",
    "            continue\n",
    "        \n",
    "        word = replace_url(word)\n",
    "        if word.count(\".\") == 1:\n",
    "            word = word.replace(\".\", \" \")\n",
    "        filtered_word = \"\".join([x for x in word if x in valid_set])\n",
    "                    \n",
    "        #Kind of hack: for every word check if it has a toxic word as a part of it\n",
    "        #If so, split this word by swear and non-swear part.\n",
    "        normalized_word = split_word(filtered_word, toxic_words)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, misspellings_dict)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, acronym_words)\n",
    "\n",
    "        normalized_words.append(normalized_word)\n",
    "        \n",
    "    normalized_comment = \" \".join(normalized_words)\n",
    "    \n",
    "    result = []\n",
    "    for word in normalized_comment.split():\n",
    "        if word.upper() == word:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(word.lower())\n",
    "    \n",
    "    #apparently, people on wikipedia love to talk about sockpuppets :-)\n",
    "    result = \" \".join(result)\n",
    "    if \"sock puppet\" in result:\n",
    "        result = result.replace(\"sock puppet\", \"sockpuppet\")\n",
    "    \n",
    "    if \"SOCK PUPPET\" in result:\n",
    "        result = result.replace(\"SOCK PUPPET\", \"SOCKPUPPET\")\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_files(train_filepath, test_filepath):\n",
    "    #read train data\n",
    "    train = pd.read_csv(train_filepath)\n",
    "\n",
    "\n",
    "    labels = train[categories].values\n",
    "    \n",
    "    #read test data\n",
    "    test = pd.read_csv(test_filepath)\n",
    "\n",
    "    test_comments = test[\"Text\"].fillna(\"_na_\").values\n",
    "\n",
    "    #normalize comments\n",
    "    np_normalize = np.vectorize(normalize_comment)\n",
    "    comments = train[\"Text\"].fillna(\"_na_\").values\n",
    "    normalized_comments = np_normalize(comments)\n",
    "    del comments\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    comments = test[\"Text\"].fillna(\"_na_\").values\n",
    "    normalized_test_comments = np_normalize(test_comments)\n",
    "    del comments\n",
    "    gc.collect()\n",
    "       \n",
    "\n",
    "    print('Shape of data tensor:', normalized_comments.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "    print('Shape of test data tensor:', normalized_test_comments.shape)\n",
    "    \n",
    "    return (labels, normalized_comments, normalized_test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_filepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_filepath' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, x_train, x_test = read_data_files(train_filepath, test_filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 s, sys: 844 ms, total: 29 s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "fileObject = open('../input/cleaning-dictionaries/tokenizer','rb')  \n",
    "# load the object from the file into var b\n",
    "tokenizer = pickle.load(fileObject)  \n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 1.9 s, total: 1.93 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_matrix = np.load('../input/embedding-2/embedding_matrix_big.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, labels, test_size = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(6, activation='sigmoid')(hidden)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "0\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 100s - loss: 0.0488 - val_loss: 0.0399\n",
      "0.9875808046220457\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0379 - val_loss: 0.0381\n",
      "0.9899873233857368\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0348 - val_loss: 0.0370\n",
      "0.9907265471551661\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0327 - val_loss: 0.0375\n",
      "0.9906326996417496\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0313 - val_loss: 0.0370\n",
      "0.9908793227268761\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 98s - loss: 0.0490 - val_loss: 0.0424\n",
      "0.9875894782845156\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0381 - val_loss: 0.0378\n",
      "0.9903980669133886\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0348 - val_loss: 0.0368\n",
      "0.9905986428474642\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0327 - val_loss: 0.0368\n",
      "0.9908293279862167\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0314 - val_loss: 0.0370\n",
      "0.9906749191141823\n",
      "153164/153164 [==============================] - 20s 134us/step\n",
      "0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 99s - loss: 0.0493 - val_loss: 0.0416\n",
      "0.9875491042638811\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0382 - val_loss: 0.0384\n",
      "0.9897553249062595\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 96s - loss: 0.0348 - val_loss: 0.0368\n",
      "0.9903491522428787\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0327 - val_loss: 0.0374\n",
      "0.9899999484959665\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0313 - val_loss: 0.0372\n",
      "0.9902564032685286\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 99s - loss: 0.0493 - val_loss: 0.0407\n",
      "0.9886327667543554\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0379 - val_loss: 0.0378\n",
      "0.9904621550980213\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0347 - val_loss: 0.0371\n",
      "0.9910408830482884\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0327 - val_loss: 0.0367\n",
      "0.9909342348546171\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0314 - val_loss: 0.0369\n",
      "0.9906990739290897\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 99s - loss: 0.0492 - val_loss: 0.0406\n",
      "0.9860942235351926\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0379 - val_loss: 0.0412\n",
      "0.9899888796147995\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0347 - val_loss: 0.0370\n",
      "0.9907867587782294\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 96s - loss: 0.0326 - val_loss: 0.0366\n",
      "0.9907063720413858\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0313 - val_loss: 0.0371\n",
      "0.9904471865295079\n",
      "153164/153164 [==============================] - 20s 133us/step\n",
      "0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 100s - loss: 0.0497 - val_loss: 0.0419\n",
      "0.9864791993642601\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0380 - val_loss: 0.0376\n",
      "0.9906406328337514\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 96s - loss: 0.0348 - val_loss: 0.0373\n",
      "0.9907045243354876\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0326 - val_loss: 0.0371\n",
      "0.9907144157962966\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0312 - val_loss: 0.0369\n",
      "0.9906639184623863\n",
      "153164/153164 [==============================] - 21s 135us/step\n",
      "0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 100s - loss: 0.0493 - val_loss: 0.0405\n",
      "0.9882885000792903\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0377 - val_loss: 0.0381\n",
      "0.9903384552763385\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0347 - val_loss: 0.0378\n",
      "0.9908247492009665\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0325 - val_loss: 0.0370\n",
      "0.9905851982825\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0313 - val_loss: 0.0370\n",
      "0.9907098983371636\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 101s - loss: 0.0481 - val_loss: 0.0403\n",
      "0.9886463226977728\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0378 - val_loss: 0.0390\n",
      "0.9901236126707595\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0346 - val_loss: 0.0384\n",
      "0.9909752910419328\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0324 - val_loss: 0.0370\n",
      "0.991337932615461\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0310 - val_loss: 0.0368\n",
      "0.9912720801424157\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 102s - loss: 0.0489 - val_loss: 0.0408\n",
      "0.9889516407452902\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0378 - val_loss: 0.0380\n",
      "0.9898547777090425\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0346 - val_loss: 0.0367\n",
      "0.9906305983791719\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0326 - val_loss: 0.0372\n",
      "0.9900835571050599\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0312 - val_loss: 0.0372\n",
      "0.9900660607968889\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 102s - loss: 0.0491 - val_loss: 0.0409\n",
      "0.9872575651720263\n",
      "1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0378 - val_loss: 0.0393\n",
      "0.9903346871099301\n",
      "2\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0345 - val_loss: 0.0371\n",
      "0.9909077808702156\n",
      "3\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0324 - val_loss: 0.0375\n",
      "0.9905555563938467\n",
      "4\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " - 97s - loss: 0.0310 - val_loss: 0.0373\n",
      "0.9905678464878825\n",
      "153164/153164 [==============================] - 21s 135us/step\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "SEEDS = 10\n",
    "\n",
    "pred = 0\n",
    "\n",
    "for ii in range(SEEDS):\n",
    "    model = build_model(embedding_matrix)\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        print(global_epoch)\n",
    "        model.fit(\n",
    "                    x_train,\n",
    "                    y_train,\n",
    "                    validation_data = (x_valid, y_valid),\n",
    "                    batch_size=128,\n",
    "                    epochs=1,\n",
    "                    verbose=2,\n",
    "                    callbacks=[\n",
    "                        LearningRateScheduler(lambda _: 1e-3 * (0.5 ** global_epoch))\n",
    "                    ]\n",
    "                )\n",
    "        val_preds = model.predict(x_valid)\n",
    "        AUC = 0\n",
    "        for i in range(6):\n",
    "             AUC += roc_auc_score(y_valid[:,i], val_preds[:,i])/6.\n",
    "        print(AUC)\n",
    "\n",
    "    pred += model.predict(x_test, batch_size = 1024, verbose = 1)/SEEDS\n",
    "    np.save('pred', pred)\n",
    "    model.save_weights('model_weights_'+str(ii)+'.h5')\n",
    "    os.system('gzip '+'model_weights_'+str(ii)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission[list_classes] = (pred)\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
