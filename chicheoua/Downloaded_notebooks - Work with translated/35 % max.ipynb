{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "5afdb577-5e12-4aae-ab95-697ce2fcb3b2",
    "_uuid": "a6e926edfd31362ac957d8a2a76f57fc78ebc433"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import functools\n",
    "import operator\n",
    "import fasttext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, InputSpec, GlobalMaxPool1D, GlobalAvgPool1D, Masking\n",
    "from keras.layers import LSTM, GRU, Bidirectional, Dropout, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers import concatenate\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras import initializers, regularizers, constraints\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from textacy.preprocess import preprocess_text\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textacy\n",
    "# import textacy\n",
    "# textacy\n",
    "# textacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(textacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5aa68170-5233-4fdc-b530-b920151b35e1",
    "_uuid": "e0b1d5a7bf690a70c1a7de1cfbbf0b69780843c9"
   },
   "source": [
    "# Embedding Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "1a665f5c-6166-400b-bddb-4d08e57b2738",
    "_uuid": "03ca2d3e503ba8d3f46a67849a1b94578ae938bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Vectors\\glove.840B.300d.pkl\n",
      "../Vectors\\wiki.ny.bin\n",
      "../Vectors\\wiki.ny.vec\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(os.scandir('../Vectors'), key=lambda x: x.stat().st_size, reverse=True):\n",
    "    print(i.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Translated/cleaned/train.csv')\n",
    "test = pd.read_csv('../Translated/cleaned/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "72483c23-8145-4860-b8d1-b9d5cf758404",
    "_uuid": "5dbc6c0165cba74a63b9b31906ab832b0d4a1779"
   },
   "outputs": [],
   "source": [
    "max_features = 60000\n",
    "maxlen = 250\n",
    "embed_size = 300\n",
    "\n",
    "file_path = \"weights_base.best.hdf5\"\n",
    "emb_file = '../Vectors/wiki.ny.bin'\n",
    "unused = set([])\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(reduce_len=True)\n",
    "lem = WordNetLemmatizer()\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "other_stop_w = pd.read_csv('words_shared_by_all.csv')\n",
    "list_classes = train.Label.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "4d198ec4-6faf-4960-aa09-3d9013ca4a19",
    "_uuid": "1a1cc2a5a9c0fb8cb7125f450882ccddf87b8ea2"
   },
   "outputs": [],
   "source": [
    "CONTEXT_DIM = 100\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, regularizer=regularizers.l2(1e-10), **kwargs):\n",
    "        self.regularizer = regularizer\n",
    "        self.supports_masking = True\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3        \n",
    "        self.W = self.add_weight(name='W',\n",
    "                                 shape=(input_shape[-1], CONTEXT_DIM),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.b = self.add_weight(name='b',\n",
    "                                 shape=(CONTEXT_DIM,),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.u = self.add_weight(name='u',\n",
    "                                 shape=(CONTEXT_DIM,),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True, \n",
    "                                 regularizer=self.regularizer)        \n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x, dim):\n",
    "        \"\"\"Computes softmax along a specified dim. Keras currently lacks this feature.\n",
    "        \"\"\"\n",
    "        if K.backend() == 'tensorflow':\n",
    "            import tensorflow as tf\n",
    "            return tf.nn.softmax(x, dim)\n",
    "        elif K.backend() == 'theano':\n",
    "            # Theano cannot softmax along an arbitrary dim.\n",
    "            # So, we will shuffle `dim` to -1 and un-shuffle after softmax.\n",
    "            perm = np.arange(K.ndim(x))\n",
    "            perm[dim], perm[-1] = perm[-1], perm[dim]\n",
    "            x_perm = K.permute_dimensions(x, perm)\n",
    "            output = K.softmax(x_perm)\n",
    "\n",
    "            # Permute back\n",
    "            perm[dim], perm[-1] = perm[-1], perm[dim]\n",
    "            output = K.permute_dimensions(x, output)\n",
    "            return output\n",
    "        else:\n",
    "            raise ValueError(\"Backend '{}' not supported\".format(K.backend()))\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        ut = K.tanh(K.bias_add(K.dot(x, self.W), self.b)) * self.u\n",
    "\n",
    "        # Collapse `attention_dims` to 1. This indicates the weight for each time_step.\n",
    "        ut = K.sum(ut, axis=-1, keepdims=True)\n",
    "\n",
    "        # Convert those weights into a distribution but along time axis.\n",
    "        # i.e., sum of alphas along `time_steps` axis should be 1.\n",
    "        self.at = self.softmax(ut, dim=1)\n",
    "        if mask is not None:\n",
    "            self.at *= K.cast(K.expand_dims(mask, -1), K.floatx())\n",
    "\n",
    "        # Weighted sum along `time_steps` axis.\n",
    "        return K.sum(x * self.at, axis=-2)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "fffffe3c-8df6-4d6e-9f7f-ca176ee22cd0",
    "_uuid": "fae26f773b7699a6c5347635a42b793f78aaed43"
   },
   "outputs": [],
   "source": [
    "def create_embedding(emb_file, word_index):\n",
    "    if emb_file.endswith('bin'):\n",
    "        embeddings_index = fasttext.load_model(emb_file)\n",
    "    else:\n",
    "        embeddings_index = pd.read_table(emb_file,\n",
    "                                         sep=\" \",\n",
    "                                         index_col=0,\n",
    "                                         header=None,\n",
    "                                         quoting=csv.QUOTE_NONE,\n",
    "                                         usecols=range(embed_size + 1),\n",
    "                                         dtype={h: np.float32 for h in range(1, embed_size + 1)},\n",
    "                                         engine='c',\n",
    "        )\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    # Initialize Random Matrix\n",
    "    if emb_file.endswith('bin'):\n",
    "        mean, std = 0.007565171, 0.29283202\n",
    "    else:\n",
    "        mean, std = embeddings_index.values.mean(), embeddings_index.values.std()\n",
    "\n",
    "    embedding_matrix = np.random.normal(mean, std, (nb_words, embed_size))\n",
    "\n",
    "    with tqdm(total=nb_words, desc='Embeddings', unit=' words') as pbar:\n",
    "        for word, i in word_index.items():\n",
    "            if i >= nb_words:\n",
    "                continue\n",
    "            if emb_file.endswith('bin'):\n",
    "                if embeddings_index.get_word_id(word) != -1:\n",
    "                    embedding_matrix[i] = embeddings_index.get_word_vector(word).astype(np.float32)\n",
    "                    pbar.update()\n",
    "            else:\n",
    "                if word in embeddings_index.index:\n",
    "                    embedding_matrix[i] = embeddings_index.loc[word].values\n",
    "                    pbar.update()\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_embedding(emb_file):\n",
    "    return Embedding(min(max_features, len(tokenizer.word_index)), embed_size,\n",
    "                     weights=[create_embedding(emb_file, tokenizer.word_index)],\n",
    "                     input_length=maxlen,\n",
    "                     trainable=False\n",
    "    )\n",
    "\n",
    "def tokenize(s):\n",
    "    return re.sub('([{}“”¨«»®´·º½¾¿¡§£₤‘’])'.format(string.punctuation), r' \\1 ', s).split()\n",
    "\n",
    "def replace_numbers(s):\n",
    "    dictionary = {\n",
    "        '&': ' and ',\n",
    "        '@': ' at ',\n",
    "        '0': ' zero ',\n",
    "        '1': ' one ',\n",
    "        '2': ' two ',\n",
    "        '3': ' three ',\n",
    "        '4': ' four ',\n",
    "        '5': ' five ',\n",
    "        '6': ' six ',\n",
    "        '7': ' seven ',\n",
    "        '8': ' eight ',\n",
    "        '9': ' nine ',\n",
    "    }\n",
    "    for k, v in dictionary.items():\n",
    "        s = s.replace(k, v)\n",
    "    return s\n",
    "\n",
    "def text_cleanup(s, remove_unused=True):\n",
    "    \"\"\"\n",
    "    This function receives ss and returns clean word-list\n",
    "    \"\"\"\n",
    "    # Remove leaky elements like ip, user, numbers, newlines\n",
    "    s = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"_ip_\", s)\n",
    "    s = re.sub(\"\\[\\[.*\\]\", \"\", s)\n",
    "    s = re.sub('\\n', ' ', s)\n",
    "    s = replace_numbers(s)\n",
    "\n",
    "    # Split the sentences into words\n",
    "    s = tweet_tokenizer.tokenize(s)\n",
    "\n",
    "    # Lemmatize\n",
    "    s = [lem.lemmatize(word, \"v\") for word in s]\n",
    "\n",
    "    # Remove Stopwords\n",
    "    s = ' '.join([w for w in s if not w in eng_stopwords])\n",
    "    \n",
    "    s = preprocess_text(s, fix_unicode=True,\n",
    "                           lowercase=True,\n",
    "                           no_currency_symbols=True,\n",
    "                           transliterate=True,\n",
    "                           no_urls=True,\n",
    "                           no_emails=True,\n",
    "                           no_contractions=True,\n",
    "                           no_phone_numbers=True,\n",
    "                           no_punct=True).strip()\n",
    "    \n",
    "    if remove_unused:\n",
    "        s = ' '.join([i for i in s.split() if i not in unused])\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train.Label)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ARTS AND CRAFTS', 'CULTURE', 'ECONOMY', 'EDUCATION', 'FARMING',\n",
       "       'FLOODING', 'HEALTH', 'LAW/ORDER', 'LOCALCHIEFS', 'MUSIC',\n",
       "       'OPINION/ESSAY', 'POLITICS', 'RELATIONSHIPS', 'RELIGION', 'SOCIAL',\n",
       "       'SOCIAL ISSUES', 'SPORTS', 'TRANSPORT', 'WILDLIFE/ENVIRONMENT',\n",
       "       'WITCHCRAFT'], dtype='<U20')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(lb)\n",
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "d1c534c1-446d-4e2e-82b6-490f01cbeb1d",
    "_uuid": "68cf14532fb4887f7ca908722f423a1befc51d63"
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv').sample(frac=1)\n",
    "# test  = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n",
    "\n",
    "train['Text'] = train.Text.fillna(\"_na_\").apply(text_cleanup)\n",
    "test['Text']  = test.Text.fillna(\"_na_\").apply(text_cleanup)\n",
    "\n",
    "list_sentences_train = train.Text.tolist()\n",
    "list_sentences_test  = test.Text.tolist()\n",
    "\n",
    "y = y_train\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
    "tokenizer.fit_on_texts(list_sentences_train + list_sentences_test)\n",
    "\n",
    "X_t  = sequence.pad_sequences(tokenizer.texts_to_sequences(list_sentences_train), maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(tokenizer.texts_to_sequences(list_sentences_test),  maxlen=maxlen)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_t, y, test_size=0.1, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "375b51da-1e7e-4ded-9cac-268477c7e7ef",
    "_uuid": "c961648e9c1fd511fdc7ce34fcb9465300bc6581"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings:   2%|█                                                          | 1062/60000 [00:00<00:07, 8296.52 words/s]\n"
     ]
    }
   ],
   "source": [
    "embedding = get_embedding(emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "9d6fa6c1-47db-45d7-ad20-324c9c1adaf9",
    "_uuid": "563bce691e753f2f40d8692f68412b5844548ec0"
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "\n",
    "    def __init__(self, verbose=True):\n",
    "        super(RocAucEvaluation, self).__init__()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs   = logs or {}\n",
    "        x_val  = self.validation_data[0]\n",
    "        y_val  = self.validation_data[1]\n",
    "        y_pred = self.model.predict(x_val, verbose=0)\n",
    "        try:\n",
    "            current  = roc_auc_score(y_val, y_pred)\n",
    "        except ValueError:\n",
    "            # Bug in AUC metric when TP = 100%\n",
    "            # https://github.com/scikit-learn/scikit-learn/issues/1257\n",
    "            current = 1.0\n",
    "\n",
    "        logs['roc_auc'] = current\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"val_roc_auc: {:.6f}\".format(current))\n",
    "\n",
    "def create_model(embedding=None):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "\n",
    "    x = embedding(inp)\n",
    "    x = Bidirectional(GRU(512, return_sequences=True))(x)\n",
    "    x = Attention()(x)\n",
    "    x = Dense(20, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3, clipnorm=4), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "ecadb92c-20f6-4726-82e3-c11bd7dff535",
    "_uuid": "b83fb812dc4c15f3e818e5b5fb0a38dcb8b0c36e"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "model = create_model(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "6955df70-b504-48d2-81b2-393d56d9a49b",
    "_uuid": "ebed2370d2a3f909d71a99ed243d7ddbc72399f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 250)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 250, 300)          18000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 250, 1024)         2500608   \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 1024)              102600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                20500     \n",
      "=================================================================\n",
      "Total params: 20,623,708\n",
      "Trainable params: 2,623,708\n",
      "Non-trainable params: 18,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "f5ebda59-5eaf-4ae3-a655-debacce9c73d",
    "_uuid": "ffca917b8c73557477a27533cb3a1ca75c4f428d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "41/41 [==============================] - 197s 5s/step - loss: 2.6066 - accuracy: 0.2043 - val_loss: 2.5243 - val_accuracy: 0.1597\n",
      "Epoch 2/15\n",
      "41/41 [==============================] - 197s 5s/step - loss: 2.4998 - accuracy: 0.2059 - val_loss: 2.5201 - val_accuracy: 0.1944\n",
      "Epoch 3/15\n",
      "41/41 [==============================] - 196s 5s/step - loss: 2.4020 - accuracy: 0.2128 - val_loss: 2.3789 - val_accuracy: 0.2014\n",
      "Epoch 4/15\n",
      "41/41 [==============================] - 196s 5s/step - loss: 2.2388 - accuracy: 0.2833 - val_loss: 2.4158 - val_accuracy: 0.2222\n",
      "Epoch 5/15\n",
      "41/41 [==============================] - 200s 5s/step - loss: 1.9861 - accuracy: 0.3576 - val_loss: 2.1430 - val_accuracy: 0.3125\n",
      "Epoch 6/15\n",
      "41/41 [==============================] - 201s 5s/step - loss: 1.6481 - accuracy: 0.4768 - val_loss: 2.1746 - val_accuracy: 0.3333\n",
      "Epoch 7/15\n",
      "41/41 [==============================] - 198s 5s/step - loss: 1.3422 - accuracy: 0.5658 - val_loss: 2.0743 - val_accuracy: 0.3472\n",
      "Epoch 8/15\n",
      "41/41 [==============================] - 200s 5s/step - loss: 0.9896 - accuracy: 0.6811 - val_loss: 2.2723 - val_accuracy: 0.3750\n",
      "Epoch 9/15\n",
      "41/41 [==============================] - 198s 5s/step - loss: 0.6835 - accuracy: 0.7864 - val_loss: 2.5915 - val_accuracy: 0.3750\n",
      "Epoch 10/15\n",
      "41/41 [==============================] - 194s 5s/step - loss: 0.3733 - accuracy: 0.9048 - val_loss: 2.5846 - val_accuracy: 0.3611\n",
      "Epoch 11/15\n",
      "41/41 [==============================] - 197s 5s/step - loss: 0.2430 - accuracy: 0.9528 - val_loss: 2.7037 - val_accuracy: 0.3542\n",
      "Epoch 12/15\n",
      " 2/41 [>.............................] - ETA: 1:38 - loss: 0.1741 - accuracy: 0.9688"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e4e6344537ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m               \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m               \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"max\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m               \u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcooldown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m           ]\n\u001b[0;32m     14\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=[\n",
    "#               RocAucEvaluation(verbose=True),\n",
    "              ModelCheckpoint(file_path,    monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "              EarlyStopping(patience=10,    monitor=\"val_accuracy\", mode=\"max\"),\n",
    "              ReduceLROnPlateau(patience=0, monitor='val_accuracy', mode='max', cooldown=2, min_lr=1e-7, factor=0.3)\n",
    "          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bec86d64-6479-41a0-878f-d278149a7424",
    "_uuid": "ec45d87d15ec16f126fdae0dd249664e4638b74a"
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "\n",
    "sample_submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n",
    "sample_submission[list_classes] = model.predict(X_te, verbose=True)\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01a68f6c-bd45-4026-b0a4-3d7425d2119e",
    "_uuid": "f1340e04736c2ec59b0eeb22759571334a2b18f5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
