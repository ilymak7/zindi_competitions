{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84bb1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgbm# .loc[df_test.sku_name == new_id_test[0]]\n",
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"Feature\", \"Importance\"]].groupby(\"Feature\").mean().sort_values(by=\"Importance\", ascending=False)[:10].index\n",
    "    best_features = feature_importance_df_[[\"Feature\", \"Importance\"]].groupby(\"Feature\").mean().sort_values(by=\"Importance\", ascending=False)[:50]\n",
    "    best_features.reset_index(inplace=True)\n",
    "    print(best_features.dtypes)\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=best_features)\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feeaa3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "ss = pd.read_csv('SampleSubmission.csv')\n",
    "dict_s = pd.read_csv('DataDictionary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412699d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellin = ['sellin','sellin_channel_1','sellin_channel_2','sellin_channel_3','sellin_channel_4',\n",
    "         'sellin_channel_5','sellin_channel_6','sellin_channel_7','sellin_channel_8']\n",
    "sellout = ['sellout','sellout_channel_1','sellout_channel_2','sellout_channel_3','sellout_channel_4',\n",
    "         'sellout_channel_5','sellout_channel_6','sellout_channel_7','sellout_channel_8','sellout_channel_9',\n",
    "           'sellout_channel_10']\n",
    "onhand= ['onhand_inventory','onhand_inventory_channel_1','onhand_inventory_channel_2','onhand_inventory_channel_3','onhand_inventory_channel_4',\n",
    "         'onhand_inventory_channel_5','onhand_inventory_channel_6','onhand_inventory_channel_7','onhand_inventory_channel_8','onhand_inventory_channel_9',\n",
    "           'onhand_inventory_channel_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6621032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import calendar\n",
    "def count_weekends(m,y):\n",
    "    day_to_count = calendar.SUNDAY\n",
    "\n",
    "    matrix = calendar.monthcalendar(y,m)\n",
    "\n",
    "    num_days = sum(1 for x in matrix if x[day_to_count] != 0)\n",
    "    return num_days%4\n",
    "test['Weeks'] = test.apply(lambda x: count_weekends(x.month, x.year), axis=1)\n",
    "train['date'] = (train['year']-2016)*12 + (train['month']-1) \n",
    "test['date'] = (test['year']-2016)*12 + (test['month']-1) # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77996546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train.copy()\n",
    "df_test = test.copy()\n",
    "df_train[sellin+sellout+onhand+['leftover_inventory','starting_inventory']] = df_train[sellin+sellout+onhand+['leftover_inventory','starting_inventory']]/1013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98813304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.columns\n",
    "pcl = df_train.groupby(['sku_name','year','month'])['product_lifecycle_stage'].apply(lambda x : x.unique()[0]).reset_index()\n",
    "pcl_test = df_train.groupby('sku_name')['product_lifecycle_stage'].apply(lambda x:x.values[-1])\n",
    "df_train.drop('product_lifecycle_stage',axis = 1,inplace = True)\n",
    "fea1 = sellin+sellout+onhand+['leftover_inventory','starting_inventory']\n",
    "fea2 = [f for f in df_train.columns.tolist() if f not in fea1]\n",
    "# new_target = df_train.groupby(['sku_name','year','month'])['sellin'].sum().reset_index()\n",
    "new_target = df_train.groupby(['sku_name','year','month'])[fea1].sum().reset_index()\n",
    "new_target2 = df_train[fea2].groupby(['sku_name','year','month']).mean().reset_index()\n",
    "df_train = new_target.merge(pcl,on = ['sku_name','year','month'],how = 'left').merge(new_target2,on = ['sku_name','year','month'],how = 'left')\n",
    "df_test['product_lifecycle_stage'] = df_test['sku_name'].map(pcl_test)\n",
    "price = df_train.groupby(['sku_name','year','month'])['price'].apply(lambda x : x.unique()[0]).reset_index()\n",
    "price_test = df_train.groupby('sku_name')['price'].apply(lambda x:x.values[-1])\n",
    "# df_test['price'] = df_test['sku_name'].map(price_test)\n",
    "\n",
    "target = 'sellin'\n",
    "prediction_columns = df_test.columns.tolist()\n",
    "encoding_columns = [t for t in df_train.columns if t not in prediction_columns]\n",
    "mao = df_train.groupby('sku_name').size()\n",
    "df_test['size'] = df_test['sku_name'].map(mao)\n",
    "df_test['size'].fillna(0,inplace = True)\n",
    "df_train['size'] = df_train['sku_name'].map(mao)\n",
    "df_train['sellin_old'] = df_train.groupby('sku_name')[['sellin']].shift(1).fillna(0)\n",
    "df_train['sellin_old2'] = df_train.groupby('sku_name')[['sellin']].shift(2).fillna(0)\n",
    "df_train['period_old'] = df_train.groupby(['sku_name'])['date'].diff().fillna(0)\n",
    "df_train['period_old2'] = df_train.groupby(['sku_name'])['date'].diff(2)\n",
    "ma_sell = df_train[df_train.sku_name.isin(df_test.sku_name)].groupby('sku_name').apply(lambda x : x['sellin'].values[-1])\n",
    "ma_date = df_train[df_train.sku_name.isin(df_test.sku_name)].groupby('sku_name').apply(lambda x : x['date'].values[-1])\n",
    "\n",
    "ma_sell2 = df_train.loc[(df_train.sku_name.isin(df_test.sku_name))& (df_train['size']>1)].groupby('sku_name').apply(lambda x : x['sellin'].values[-2])\n",
    "ma_date2 = df_train.loc[(df_train.sku_name.isin(df_test.sku_name))& (df_train['size']>1)].groupby('sku_name').apply(lambda x : x['date'].values[-2])\n",
    "df_test['sellin_old'] = df_test['sku_name'].map(ma_sell)\n",
    "df_test['period_old'] = df_test['sku_name'].map(ma_date)\n",
    "df_test['sellin_old2'] = df_test['sku_name'].map(ma_sell2)\n",
    "df_test['period_old2'] = df_test['sku_name'].map(ma_date2)\n",
    "df_test['period_old'] = df_test['date'] - df_test['period_old']\n",
    "df_test['period_old2'] = df_test['date'] - df_test['period_old2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae38d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['medians'] = df_train['sku_name'].map(df_train.groupby('sku_name')['sellin'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ade505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "#     y_true_rescaled = sca.inverse_transform(y_true.values.reshape(-1,1))\n",
    "#     y_pred_rescaled = sca.inverse_transform(y_pred.reshape(-1,1))\n",
    "    return  np.mean(np.abs(y_true - y_pred))*1013#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6e4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = df_train.sku_name.isin(df_test.sku_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f8a2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmspe(df_train[mask1]['sellin'],df_train[mask1]['medians'])\n",
    "df_train['pc'] = df_train.groupby('sku_name')['sellin'].pct_change().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1825996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(df_train.pc)\n",
    "# df_train.corr()['pc'].sort_values(ascending = False,key = abs)\n",
    "# df_train['sellin'].cumprod()\n",
    "df_train['cum_pc'] = df_train.groupby('sku_name').apply(lambda x : (x['pc']+1).cumprod()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d785c48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28023,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train['cum_pc']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
