{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install chart-studio\nclear_output()\nfrom sklearn.impute import KNNImputer\nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom datetime import datetime as dt\nimport itertools\nfrom tqdm import tqdm\nfrom scipy.stats import norm, t\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nimport gc\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn import metrics\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\npd.set_option('display.max_columns', None)\nplt.style.use('fivethirtyeight') \nfrom pylab import rcParams\nfrom plotly import tools\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport statsmodels.api as sm\nfrom numpy.random import normal, seed\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.tsa.arima_model import ARIMA\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import Pool, CatBoostRegressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom keras.callbacks import EarlyStopping\nimport os \nimport random\n# import numpy as np \nfrom catboost import Pool, CatBoostRegressor\n\nDEFAULT_RANDOM_SEED = 2021\n\ndef seedBasic(seed=DEFAULT_RANDOM_SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \n# tensorflow random seed \nimport tensorflow as tf \ndef seedTF(seed=DEFAULT_RANDOM_SEED):\n    tf.random.set_seed(seed)\n    \n# torch random seed\nimport torch\ndef seedTorch(seed=DEFAULT_RANDOM_SEED):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n      \n# basic + tensorflow + torch \ndef seedEverything(seed=DEFAULT_RANDOM_SEED):\n    seedBasic(seed)\n    seedTF(seed)\n    seedTorch(seed)\nseedEverything(42)\nseed = 42\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-21T18:40:32.442109Z","iopub.execute_input":"2022-09-21T18:40:32.442643Z","iopub.status.idle":"2022-09-21T18:40:50.589440Z","shell.execute_reply.started":"2022-09-21T18:40:32.442522Z","shell.execute_reply":"2022-09-21T18:40:50.587821Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}}]},{"cell_type":"code","source":"# train","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:40:50.591798Z","iopub.execute_input":"2022-09-21T18:40:50.592966Z","iopub.status.idle":"2022-09-21T18:40:50.602042Z","shell.execute_reply.started":"2022-09-21T18:40:50.592909Z","shell.execute_reply":"2022-09-21T18:40:50.600246Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_ori = pd.read_csv('/kaggle/input/airquoi/train.csv',parse_dates=['date'])\ntest_ori = pd.read_csv('/kaggle/input/airquoi/test.csv',parse_dates=['date'])\nss =pd.read_csv('/kaggle/input/airquoi/SampleSubmission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:40:50.604172Z","iopub.execute_input":"2022-09-21T18:40:50.604887Z","iopub.status.idle":"2022-09-21T18:40:50.914019Z","shell.execute_reply.started":"2022-09-21T18:40:50.604831Z","shell.execute_reply":"2022-09-21T18:40:50.912825Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:40:50.917062Z","iopub.execute_input":"2022-09-21T18:40:50.917972Z","iopub.status.idle":"2022-09-21T18:40:50.923291Z","shell.execute_reply.started":"2022-09-21T18:40:50.917907Z","shell.execute_reply":"2022-09-21T18:40:50.921670Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{}},{"cell_type":"code","source":"def cust_metric(y_pred, ds):\n    y_true = ds.get_label()\n    sq_errs = abs(y_true - y_pred)\n    avg_per_id = pd.Series(sq_errs).groupby(ds.site_code).mean()\n#     print(avg_per_id)\n    return 'avg_worst_by_id', avg_per_id.max(), False\n\ndef _get_X_Y_DF_from_CV(train_X, train_Y, train_index, validation_index):\n        X_train, X_validation = (\n            train_X.iloc[train_index],\n            train_X.iloc[validation_index],\n        )\n        y_train, y_validation = (\n            train_Y.iloc[train_index],\n            train_Y.iloc[validation_index],\n        )\n        return X_train, X_validation, y_train, y_validation\ndef cust_score(y_predicted):\n    l = []\n    a = train.site_latitude.unique()\n    for i in a:\n        y_true = train.loc[train.site_latitude == i,'pm2_5']\n        y_hat = y_predicted[y_true.index]\n        l.append(metrics.mean_absolute_error(y_true,y_hat))\n    return pd.DataFrame({'site_latitude':a,\n                        'err': np.array(l)})\ndef preds_to_sub(test,preds,name = None,log = False):\n    if log :\n        test['pm2_5'] = np.exp(preds)\n    else:\n        test['pm2_5'] = preds\n    sub = test[['ID','pm2_5']]\n    if name:\n        sub.to_csv(name+'.csv', index = False)\n    return sub\ndef masked_w_aver(a,weights):\n    ma = np.ma.MaskedArray(a, mask=np.isnan(a))\n    return np.ma.average(ma, weights=weights)\ndef auto_regress(train,test,i,col_tar,output_col,preds_col):\n    a = train.site_latitude.unique()\n    time_ser = fancy_interpol(train,i,th=0.8,col_pred = 'pm2_5',col_tar = col_tar)\n    time_ser = time_ser.resample('D').mean().interpolate()\n    \n    time_ser_test = test.loc[test.site_latitude == a[i]].sort_values(by = 'date').copy()\n    time_ser_test.index = time_ser_test.date \n    time_ser_test = (time_ser_test[output_col])\n    time_ser_test = time_ser_test.resample('D').mean().interpolate()\n\n    train_sample = time_ser.values\n    p_val = 0.05\n    try:\n        seas  = 1\n        model = sm.tsa.SARIMAX(train_sample,order=(2,1,0),enforce_stationarity=True,\n                       seasonal_order=(0,0,1,7),trend= None)\n        result = model.fit(maxiter=1000,disp=False)\n        assert result.pvalues[2]<p_val\n    except:\n        seas = 0\n        model = sm.tsa.SARIMAX(train_sample,order=(2,1,0),trend=None)\n        result = model.fit(maxiter=1000,disp=False)\n#     print(result.summary())\n#     predicted_result = result.predict(start=1, end=train_sample.shape[0]-1)\n    y_true_dates = train.loc[train.site_latitude == a[i]].sort_values(by = 'date')['date'].values\n    y_true = train.loc[train.site_latitude == a[i]].sort_values(by = 'date')['pm2_5'].values\n    y_hat = train.loc[train.site_latitude == a[i]].sort_values(by = 'date')[preds_col].values\n    correct = result.predict(start=1, end=time_ser.shape[0]+time_ser_test.shape[0])\n    y_true_dates_test = test.loc[test.site_latitude == a[i]].sort_values(by = 'date')['date'].values\n    mask_train = time_ser.index.isin(y_true_dates)\n    mask_test = time_ser_test.index.isin(y_true_dates_test)\n    train.loc[train.site_latitude == a[i],output_col] = correct[:time_ser.shape[0]][mask_train]\n    test.loc[test.site_latitude == a[i],output_col] = correct[time_ser.shape[0]:][mask_test]\n#     train.loc[train.site_latitude == a[i],'seas'] = seas\n#     test.loc[test.site_latitude == a[i],'seas'] = seas\n    return train, test\ndef fancy_interpol(train,i,th=0.8,col_pred = 'pm2_5',col_tar = 'pm2_5'):\n#     th = 0.8\n    a = train.site_latitude.unique()\n    mi = train.pivot(index = 'date',columns = 'site_latitude',values = col_tar)[a[i]].dropna().index.min()\n    ma = train.pivot(index = 'date',columns = 'site_latitude',values = col_tar)[a[i]].dropna().index.max()\n\n    df_pred = train.pivot(index = 'date',columns = 'site_latitude',values = col_pred)[mi:ma]\n    df_tar = train.pivot(index = 'date',columns = 'site_latitude',values = col_tar)[mi:ma]\n\n    w_s = df_pred.corr()[a[i]].sort_values(ascending = False)\n    filtered_ws = w_s[(w_s>th)& (w_s<1)]\n    if len(filtered_ws)*len(df_tar[df_tar[a[i]].isna()])>0 : \n        df_tar.loc[df_tar[a[i]].isna(),a[i]] = df_tar.loc[df_tar[a[i]].isna(),filtered_ws.index].apply(lambda x: masked_w_aver(x, weights=filtered_ws.values),axis = 1)\n    return df_tar.loc[mi:ma,a[i]].astype('float')","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:40:50.925967Z","iopub.execute_input":"2022-09-21T18:40:50.926616Z","iopub.status.idle":"2022-09-21T18:40:50.964361Z","shell.execute_reply.started":"2022-09-21T18:40:50.926553Z","shell.execute_reply":"2022-09-21T18:40:50.962737Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess(train,test,scale = True,target_encode = False,\n               include_date = True,remove_nans = True, remove_multi = True):\n    le = LabelEncoder()\n    train['Train']=1\n    test['Train']=0\n    all_data=pd.concat([train,test])\n    other_cols = ['pm2_5', 'Train', 'month', 'year', 'day',\n               'week', 'dow', 'woy', 'rel_date']\n    if include_date:\n        all_data['month'] = all_data.date.dt.month\n        all_data['year'] = all_data.date.dt.year\n        all_data['day'] = all_data.date.dt.day\n        all_data['week'] = all_data.date.dt.week\n        all_data['dow'] = all_data.date.dt.dayofweek\n        all_data['woy'] = all_data.date.dt.weekofyear\n    all_data[['site_latitude','site_longitude']] =  all_data[['site_latitude','site_longitude']] .round(5)\n    all_data['device'] = le.fit_transform(all_data['device'].values.reshape(-1,1))\n    le = LabelEncoder()\n    all_data['site_code'] = le.fit_transform(all_data.site_latitude.astype('str').values.reshape(-1,1))\n    train = all_data.loc[all_data.Train == 1].reset_index(drop = True)\n    test = all_data.loc[all_data.Train == 0].reset_index(drop = True)\n    if target_encode:\n        train,test = target_encode(train,test,target = 'pm2_5',groupss = 'site_code')\n    if remove_nans:\n        tr,te = cust_imputer(train.copy(), test.copy())\n        all_data = pd.concat([tr,te]).reset_index(drop = True)\n        s_codes = all_data.site_code.unique()\n        distances = np.zeros((s_codes.shape[0],s_codes.shape[0]))\n        for i in np.sort(s_codes):\n            for j in np.sort(s_codes):\n                distances[i,j] =  get_dist(all_data, i,j)\n        distances = pd.DataFrame(index = np.sort(s_codes), columns = np.sort(s_codes),data = distances)\n        SO2_cols = ['SulphurDioxide_SO2_column_number_density',\n                    'SulphurDioxide_SO2_column_number_density_amf',\n               'SulphurDioxide_SO2_slant_column_number_density',\n               'SulphurDioxide_cloud_fraction',\n                'SulphurDioxide_sensor_azimuth_angle',\n               'SulphurDioxide_sensor_zenith_angle',\n               'SulphurDioxide_solar_azimuth_angle',\n               'SulphurDioxide_solar_zenith_angle',\n               'SulphurDioxide_SO2_column_number_density_15km']\n        CO_cols = ['CarbonMonoxide_CO_column_number_density',\n               'CarbonMonoxide_H2O_column_number_density',\n               'CarbonMonoxide_cloud_height', 'CarbonMonoxide_sensor_altitude',\n               'CarbonMonoxide_sensor_azimuth_angle',\n               'CarbonMonoxide_sensor_zenith_angle',\n               'CarbonMonoxide_solar_azimuth_angle',\n               'CarbonMonoxide_solar_zenith_angle']\n        NO2_cols = ['NitrogenDioxide_NO2_column_number_density',\n               'NitrogenDioxide_tropospheric_NO2_column_number_density',\n               'NitrogenDioxide_stratospheric_NO2_column_number_density',\n               'NitrogenDioxide_NO2_slant_column_number_density',\n               'NitrogenDioxide_tropopause_pressure',\n               'NitrogenDioxide_absorbing_aerosol_index',\n               'NitrogenDioxide_cloud_fraction', 'NitrogenDioxide_sensor_altitude',\n               'NitrogenDioxide_sensor_azimuth_angle',\n               'NitrogenDioxide_sensor_zenith_angle',\n               'NitrogenDioxide_solar_azimuth_angle',\n               'NitrogenDioxide_solar_zenith_angle']\n        HCHO_cols = ['Formaldehyde_tropospheric_HCHO_column_number_density',\n               'Formaldehyde_tropospheric_HCHO_column_number_density_amf',\n               'Formaldehyde_HCHO_slant_column_number_density',\n               'Formaldehyde_cloud_fraction', 'Formaldehyde_solar_zenith_angle',\n               'Formaldehyde_solar_azimuth_angle', 'Formaldehyde_sensor_zenith_angle',\n               'Formaldehyde_sensor_azimuth_angle']\n        UV_cols = ['UvAerosolIndex_absorbing_aerosol_index',\n               'UvAerosolIndex_sensor_altitude', 'UvAerosolIndex_sensor_azimuth_angle',\n               'UvAerosolIndex_sensor_zenith_angle',\n               'UvAerosolIndex_solar_azimuth_angle',\n               'UvAerosolIndex_solar_zenith_angle']\n        O3_cols = ['Ozone_O3_column_number_density_amf',\n               'Ozone_O3_slant_column_number_density',\n               'Ozone_O3_effective_temperature', 'Ozone_cloud_fraction',\n               'Ozone_sensor_azimuth_angle', 'Ozone_sensor_zenith_angle',\n               'Ozone_solar_azimuth_angle', 'Ozone_solar_zenith_angle']\n        cloud_cols = ['Cloud_cloud_fraction', 'Cloud_cloud_top_pressure',\n               'Cloud_cloud_top_height', 'Cloud_cloud_base_pressure',\n               'Cloud_cloud_base_height', 'Cloud_cloud_optical_depth',\n               'Cloud_surface_albedo', 'Cloud_sensor_azimuth_angle',\n               'Cloud_sensor_zenith_angle', 'Cloud_solar_azimuth_angle',\n               'Cloud_solar_zenith_angle']\n        target = ['pm2_5']\n        all_cols = SO2_cols+cloud_cols+O3_cols+UV_cols+HCHO_cols+NO2_cols+CO_cols\n\n        other_cols_test = ['Train', 'month', 'year', 'day',\n               'week', 'dow', 'woy', 'rel_date']\n        for col in tqdm(all_cols):\n            for s_co in s_codes:\n                all_data = distance_w_impute(col,s_co,all_data,distances)\n        test = all_data[all_data.ID.isin(test_ori.ID)].drop_duplicates().reset_index(drop = True)\n        train = all_data[all_data.ID.isin(train_ori.ID)].drop_duplicates().reset_index(drop = True)\n        kn = KNNImputer(n_neighbors=5)\n        cols = train.drop('pm2_5',axis = 1).select_dtypes(include = ['float']).columns\n        train[cols] = kn.fit_transform(train[cols])\n        test[cols] = kn.transform(test[cols])\n#         cols = \n        if scale:\n            rs= RobustScaler()\n            train[cols] = rs.fit_transform(train[cols])\n            test[cols] = rs.transform(test[cols])\n    if remove_multi:\n        cols = train.select_dtypes(include = ['int','float']).columns\n        l_drop = []\n        for col in cols:\n            df = train.pivot(index = 'date',columns = 'site_code',values = col)\n            if df.corr().mean().mean()>0.99:\n                l_drop.append(col)\n        train = train.drop([l for l  in l_drop[1:] if l not in other_cols],axis = 1 )\n        test = test.drop([l for l  in l_drop[1:] if l not in other_cols],axis = 1 )\n    return train.sort_index(), test.sort_index()\ndef get_dist(all_data, i,j):\n    return np.sqrt((all_data.loc[all_data.site_code == i,'site_latitude'].unique()[0]- all_data.loc[all_data.site_code == j,'site_latitude'].unique()[0])**2 + (all_data.loc[all_data.site_code == i,'site_longitude'].unique()[0]- all_data.loc[all_data.site_code == j,'site_longitude'].unique()[0])**2)\n\ndef distance_w_impute(col,s_co,all_data,distances):\n    ind = all_data.loc[(all_data.site_code == s_co)& (all_data[col].isna()),'date'].values\n    df3 = all_data.loc[(all_data.site_code != s_co)& (all_data.date.isin(ind)),['date',col,'site_code']]\n    df3['weights'] = df3['site_code'].map(distances[s_co]**(-1))\n    df3[col] = df3[col]*df3['weights']\n    maps = df3.groupby('date').apply(lambda x : x[col].sum()/(x['weights'].sum()))\n    all_data.loc[(all_data.site_code == s_co)& (all_data[col].isna()),col] = all_data.loc[(all_data.site_code == s_co)& (all_data[col].isna()),'date'].map(maps)\n    return all_data\ndef cust_imputer(train, test):\n    all_data = pd.concat([train,test]).reset_index(drop = True)\n    sensor_az_ang = ['SulphurDioxide_sensor_azimuth_angle','CarbonMonoxide_sensor_azimuth_angle','NitrogenDioxide_sensor_azimuth_angle',\n          'Formaldehyde_sensor_azimuth_angle','UvAerosolIndex_sensor_azimuth_angle','Ozone_sensor_azimuth_angle',\n         'Cloud_sensor_azimuth_angle']\n    sensor_zen_ang = ['SulphurDioxide_sensor_zenith_angle','CarbonMonoxide_sensor_zenith_angle','NitrogenDioxide_sensor_zenith_angle',\n          'Formaldehyde_sensor_zenith_angle','UvAerosolIndex_sensor_zenith_angle','Ozone_sensor_zenith_angle',\n         'Cloud_sensor_zenith_angle']\n    solar_az_ang = ['SulphurDioxide_solar_azimuth_angle','CarbonMonoxide_solar_azimuth_angle','NitrogenDioxide_solar_azimuth_angle',\n          'Formaldehyde_solar_azimuth_angle','UvAerosolIndex_solar_azimuth_angle','Ozone_solar_azimuth_angle',\n         'Cloud_solar_azimuth_angle']\n    solar_zen_ang = ['SulphurDioxide_solar_zenith_angle','CarbonMonoxide_solar_zenith_angle','NitrogenDioxide_solar_zenith_angle',\n          'Formaldehyde_solar_zenith_angle','UvAerosolIndex_solar_zenith_angle','Ozone_solar_zenith_angle',\n         'Cloud_solar_zenith_angle']\n    X =  all_data[sensor_az_ang]\n    imputer = KNNImputer(n_neighbors=5)\n    all_data.loc[X.isna().sum(axis = 1)<7,sensor_az_ang] = imputer.fit_transform(X[X.isna().sum(axis = 1)<7]) \n    X =  all_data[sensor_zen_ang]\n    imputer = KNNImputer(n_neighbors=5)\n    all_data.loc[X.isna().sum(axis = 1)<7,sensor_zen_ang] = imputer.fit_transform(X[X.isna().sum(axis = 1)<7])\n    X =  all_data[solar_az_ang]\n    imputer = KNNImputer(n_neighbors=5)\n    all_data.loc[X.isna().sum(axis = 1)<7,solar_az_ang] = imputer.fit_transform(X[X.isna().sum(axis = 1)<7]) \n    X =  all_data[solar_zen_ang]\n    imputer = KNNImputer(n_neighbors=5)\n    all_data.loc[X.isna().sum(axis = 1)<7,solar_zen_ang] = imputer.fit_transform(X[X.isna().sum(axis = 1)<7])\n    return all_data.loc[:train.shape[0]], all_data.loc[train.shape[0]:]\n","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:40:50.966846Z","iopub.execute_input":"2022-09-21T18:40:50.967512Z","iopub.status.idle":"2022-09-21T18:40:51.023657Z","shell.execute_reply.started":"2022-09-21T18:40:50.967447Z","shell.execute_reply":"2022-09-21T18:40:51.021426Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"def train_keras(baseline_model,train,features,target,test_data,kf,split_by):\n    inp = train[features].shape[1]\n    cats = train[split_by].values\n    X_scaled = train[features].values\n    Y = train[target].values\n    \n    y_oof = np.zeros(shape=(len(X_scaled),1))\n    y_predicted = np.zeros(shape=(len(test_data),1))\n    cv_scores = []\n    test_data = test_data[features]\n    models = []\n    for i, (train_index, test_index) in enumerate(kf.split(X_scaled, cats)):\n        print(' keras kfold: {}  of  {} : '.format(i+1, K_FOLDS ))\n        X_train, X_valid = X_scaled[train_index], X_scaled[test_index]\n        y_train, y_valid = Y[train_index], Y[test_index]\n        my_model = baseline_model(inp)\n        my_model.fit(X_train, y_train,\n                     validation_data=(X_valid, y_valid),\n                     epochs=100,\n                     batch_size = 64,\n                     callbacks=[EarlyStopping(patience=20)],\n                     verbose=0)\n        \n        models.append(my_model)\n        y_oof[test_index] = my_model.predict(X_valid)\n        y_predicted += my_model.predict(test_data.values) \n        del my_model\n        gc.collect()\n        cv_oof_score = metrics.mean_absolute_error(y_valid, y_oof[test_index])\n        cv_scores.append(cv_oof_score)\n        print(f\"CV OOF Score for fold {i+1} is {cv_oof_score}\")\n\n#         del validation_index, X_validation, y_validation\n#         gc.collect()\n\n    y_predicted /= K_FOLDS\n    oof_score = round(metrics.mean_absolute_error(Y, y_oof), 5)\n    avg_cv_scores = round(sum(cv_scores) / len(cv_scores), 5)\n    std_cv_scores = round(np.array(cv_scores).std(), 5)\n    return y_predicted,models,y_oof,oof_score \ndef train_lgb(X,features,target,test_data,params,kf,split_by):\n    features_importance= pd.DataFrame({'Feature':[], 'Importance':[]})\n    models =[]\n    train_X = X[features]\n    train_Y = X[target]\n    split_by = X[split_by]\n    test_data = test_data[features]\n    test_X = test_data.copy()\n    print(f\"Shape of train_X : {train_X.shape}, test_X: {test_X.shape}, train_Y: {train_Y.shape}\")\n    \n    predictors = list(train_X.columns)\n    # print(f\"List of features to be used {list(predictors)}\")\n\n    # Selecting n_splits to be 3, since class 42 has \n    # just 3 instances\n#     kf = KFold(random_state=seed_lgb,n_splits=K_FOLDS, shuffle=shuffle_lgb)\n#     kf = StratifiedKFold(random_state=seed_lgb,n_splits=K_FOLDS, shuffle=shuffle_lgb)\n    y_oof_lgb = np.zeros(shape=(len(train_X),))\n    y_predicted_lgb = np.zeros(shape=(len(test_X),))\n    cv_scores = []\n    fold = 0\n    n_folds = kf.get_n_splits()\n    for train_index, validation_index in kf.split(X=train_X, y=split_by):\n        fold += 1\n        print(f\"fold {fold} of {n_folds}\")\n\n        X_train, X_validation, y_train, y_validation = _get_X_Y_DF_from_CV(\n            train_X, train_Y, train_index, validation_index\n        )\n\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_eval = lgb.Dataset(X_validation, y_validation, reference=lgb_train)\n\n        model = lgb.train(\n            params,\n            lgb_train,\n            valid_sets=[lgb_train, lgb_eval],\n            verbose_eval=-1,\n#             early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n#             num_boost_round=N_ESTIMATORS,\n            feature_name=predictors,\n            categorical_feature=\"auto\",\n        )\n        del lgb_train, lgb_eval, train_index, X_train, y_train\n        gc.collect()\n\n        y_oof_lgb[validation_index] = model.predict(\n            X_validation, num_iteration=model.best_iteration\n        )\n\n        y_predicted_lgb += model.predict(\n            test_data.values, num_iteration=model.best_iteration\n        )\n        fold_importance_df= pd.DataFrame({'Feature':[], 'Importance':[]})\n        fold_importance_df['Feature']= predictors\n        fold_importance_df['Importance']= model.feature_importance()\n        fold_importance_df[\"fold\"] = fold + 1\n        features_importance = pd.concat([features_importance, fold_importance_df], axis=0)\n        models.append(model)\n\n        best_iteration = model.best_iteration\n        print(f\"Best number of iterations for fold {fold} is: {best_iteration}\")\n\n        cv_oof_score = metrics.mean_absolute_error(y_validation, y_oof_lgb[validation_index])\n        cv_scores.append(cv_oof_score)\n        print(f\"CV OOF Score for fold {fold} is {cv_oof_score}\")\n\n        del validation_index, X_validation, y_validation\n        gc.collect()\n\n    y_predicted_lgb /= n_folds\n    oof_score = round(metrics.mean_absolute_error(train_Y, y_oof_lgb), 5)\n    avg_cv_scores = round(sum(cv_scores) / len(cv_scores), 5)\n    std_cv_scores = round(np.array(cv_scores).std(), 5)\n    return y_predicted_lgb,models,y_oof_lgb,oof_score,features_importance\ndef train_catbo(train_X,features, target,test_X,params,kf,split_by):\n    y_oof = np.zeros(shape=(len(train_X),))\n    y_predicted = np.zeros(shape=(len(test_X),))\n    train_Y= train_X[target]\n    split_by = train_X[split_by]\n    train_X = train_X[features]\n    test_X = test_X[features]\n    cv_scores = []\n    models = []\n    fold = 0\n    n_folds = kf.get_n_splits()\n    for train_index, validation_index in kf.split(X=train_X, y=split_by):\n        fold += 1\n        print(f\"fold {fold} of {n_folds}\")\n        X_train, X_validation, y_train, y_validation = _get_X_Y_DF_from_CV(\n            train_X, train_Y, train_index, validation_index\n        )\n        train_pool = Pool(data=X_train, label=y_train)\n        eval_pool = Pool(data=X_validation, label=y_validation.values) \n        model = CatBoostRegressor(**params)\n        model.fit(train_pool,plot=True,eval_set=eval_pool)\n        del train_index, X_train, y_train\n        gc.collect()\n        models.append(model)\n        y_oof[validation_index] = model.predict(\n            X_validation )\n\n        y_predicted += model.predict(\n            test_X.values\n        )\n        cv_oof_score = metrics.mean_absolute_error(y_validation, y_oof[validation_index])\n        cv_scores.append(cv_oof_score)\n        print(f\"CV OOF Score for fold {fold} is {cv_oof_score}\")\n\n        del validation_index, X_validation, y_validation\n        gc.collect()\n\n    y_predicted /= n_folds\n    oof_score = round(metrics.mean_absolute_error(train_Y, y_oof), 5)\n    avg_cv_scores = round(sum(cv_scores) / len(cv_scores), 5)\n    std_cv_scores = round(np.array(cv_scores).std(), 5)\n    return y_predicted,models,y_oof,oof_score\ndef train_xgb(X,features,target,test_data,params,kf,split_by,num_iter=1500,es = 100,ve = 0):\n#     features = X.columns\n    y = X[target]\n    split_by = X[split_by]\n    X = X[features].values\n    y_oof = np.zeros(shape=(len(X),))\n    y_predicted = np.zeros(shape=(len(test_data), ))\n    test_data = test_data[features]\n    cv_scores = []\n    models = []\n    n_folds = kf.get_n_splits()\n    for i, (train_index, test_index) in enumerate(kf.split(X, split_by)):\n        print(' xgb kfold: {}  of  {} : '.format(i+1, n_folds ))\n        X_train, X_valid = X[train_index], X[test_index]\n        y_train, y_valid = y[train_index], y[test_index]\n        d_train = xgb.DMatrix(X_train, y_train) \n        d_valid = xgb.DMatrix(X_valid, y_valid) \n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        xgb_model = xgb.train(params, d_train, num_iter, watchlist,\n                              early_stopping_rounds=es, \n                            verbose_eval=ve)\n        models.append(xgb_model)\n        y_oof[test_index] = xgb_model.predict(xgb.DMatrix(X_valid), \n                            ntree_limit=xgb_model.best_ntree_limit)\n        y_predicted += xgb_model.predict(xgb.DMatrix(test_data[features].values), \n                            ntree_limit=xgb_model.best_ntree_limit) \n        \n        cv_oof_score = metrics.mean_absolute_error(y_valid, y_oof[test_index])\n        cv_scores.append(cv_oof_score)\n        print(f\"CV OOF Score for fold {i+1} is {cv_oof_score}\")\n\n    y_predicted /= n_folds\n    oof_score = round(metrics.mean_absolute_error(y, y_oof), 5)\n    avg_cv_scores = round(sum(cv_scores) / len(cv_scores), 5)\n    std_cv_scores = round(np.array(cv_scores).std(), 5)\n    return y_predicted,models,y_oof,oof_score \ndef train_aaa(train,test):\n    tr,te = train_all(train.copy(),test.copy(),target,features,SEED,K_fols,seeds,shuffles)\n    train_99 = train[train.pm2_5<train.pm2_5.quantile(0.999)].reset_index(drop = True).copy()\n    tr_99,te_99 = train_all(train_99,test.copy(),target,features,SEED,K_fols,seeds,shuffles)\n    train_999 = train[train.pm2_5<train.pm2_5.quantile(0.999)].reset_index(drop = True).copy()\n    tr_999,te_999 = train_all(train_999,test.copy(),target,features,SEED,K_fols,seeds,shuffles)\n    cols = ['ID','pm2_5', 'month', 'year', 'day',\n       'week', 'dow', 'woy', 'site_code', 'preds_lgb', 'preds_xgb',\n       'preds_cat']\n    tr_keras = tr[['month', 'year', 'day',\n       'week', 'dow', 'woy', 'site_code', 'preds_lgb', 'preds_xgb',\n       'preds_cat','pm2_5']]\n    a_train, a_test, b_train, b_test = train_test_split(tr_keras.drop(target,axis = 1), \n                                                    tr_keras[target], test_size=0.5,\n                                                    random_state=20)\n    tas,vel = keras_overfit(a_train,b_train,a_test,b_test,te[a_train.columns])\n    tr['preds0'] = vel\n    te['preds0'] = tas\n    tr_keras = tr_99[['month', 'year', 'day',\n       'week', 'dow', 'woy', 'site_code', 'preds_lgb', 'preds_xgb',\n       'preds_cat','pm2_5']]\n    a_train, a_test, b_train, b_test = train_test_split(tr_keras.drop(target,axis = 1), \n                                                    tr_keras[target], test_size=0.5,\n                                                    random_state=20)\n    tas,vel = keras_overfit(a_train,b_train,a_test,b_test,te_99[a_train.columns])\n    tr_99['preds1'] = vel\n    te_99['preds1'] = tas\n    tr_keras = tr_999[['month', 'year', 'day',\n       'week', 'dow', 'woy', 'site_code', 'preds_lgb', 'preds_xgb',\n       'preds_cat','pm2_5']]\n    a_train, a_test, b_train, b_test = train_test_split(tr_keras.drop(target,axis = 1), \n                                                    tr_keras[target], test_size=0.5,\n                                                    random_state=20)\n    tas,vel = keras_overfit(a_train,b_train,a_test,b_test,te_999[a_train.columns])\n    tr_999['preds2'] = vel\n    te_999['preds2'] = tas\n    tr.index = tr.ID\n    te.index = te.ID\n    tr_99.index = tr_99.ID\n    te_99.index = te_99.ID\n    tr_999.index = tr_999.ID\n    te_999.index = te_999.ID\n    train['preds0'] = train['ID'].map(tr['preds0'])\n    test['preds0'] = test['ID'].map(te['preds0'])\n    train['preds1'] = train['ID'].map(tr_99['preds1'])\n    test['preds1'] = test['ID'].map(te_99['preds1'])\n    train['preds2'] = train['ID'].map(tr_999['preds2'])\n    test['preds2'] = test['ID'].map(te_999['preds2'])\n    train['errors0'] = train['pm2_5']-train['preds0']\n    train['errors1'] = train['pm2_5']-train['preds1']\n    train['errors2'] = train['pm2_5']-train['preds2']\n    a = train.site_latitude.unique()\n    for j in range(3):\n        train['correct'+str(j)]= 0\n        test['correct'+str(j)] = 0\n        for i in range(a.shape[0]):\n            train,test = auto_regress(train,test,i,'errors'+str(j),'correct'+str(j),'preds'+str(j))\n        train['refined_preds'+str(j)] = train['preds'+str(j)] + train['correct'+str(j)]\n    return train, test\n\ndef train_all(train,test,target,features,SEED,k_folds,seeds,shuffles):\n    seedEverything(SEED)\n    \n    kf_cat = KFold(n_splits = K_fols['cat'], shuffle=shuffles['cat'],\n                   random_state = seeds['cat'])\n    y_predicted_cat,models_cat,y_oof_cat,oof_score_cat =train_catbo(train,features,target,test,params['params_cat'],\n                                                                    kf_cat, 'pm2_5')\n    seedEverything(SEED)\n\n    kf_xgb = KFold(n_splits = K_fols['xgb'], shuffle=shuffles['xgb'],\n                   random_state = seeds['xgb'])\n\n    y_predicted_xgb,models_xgb,y_oof_xgb,oof_score_xgb=train_xgb(train,features,target,test,\n                                                             params['params_xgb'],kf_xgb,'pm2_5',num_iter=1000,es = 30,ve = 100)\n    \n    seedEverything(SEED)\n    kf_lgb = KFold(n_splits = K_fols['lgb'], shuffle=shuffles['lgb'],random_state = seeds['lgb'])\n\n    y_predicted_lgb,models_lgb,y_oof_lgb,oof_score_lgb,features_importance_lgb = train_lgb(train,features,target,test,params['params_lgb'],kf_lgb,'pm2_5')\n\n    train['preds_lgb'] = y_oof_lgb                                                                                 \n    train['preds_xgb'] = y_oof_xgb\n    train['preds_cat'] = y_oof_cat                                                                                 \n    test['preds_lgb'] = y_predicted_lgb                                                                                 \n    test['preds_xgb'] = y_predicted_xgb\n    test['preds_cat'] = y_predicted_cat                                                                                 \n    return train,test\n","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:40:51.026529Z","iopub.execute_input":"2022-09-21T18:40:51.027150Z","iopub.status.idle":"2022-09-21T18:40:51.100686Z","shell.execute_reply.started":"2022-09-21T18:40:51.027084Z","shell.execute_reply":"2022-09-21T18:40:51.098773Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Parameters and features","metadata":{}},{"cell_type":"code","source":"SEED = 42\nseeds = {'lgb':None,\n        'cat': 42,\n        'xgb':43}\nshuffles ={'lgb' : False,\n          'cat' : True,\n          'xgb' : True}\nK_fols = {'lgb':3,\n         'cat':4,\n         'xgb':4}\nlgb_params  = {'objective': 'regression',\n            'metric': 'mae',\n            'n_estimators':5000,\n            'verbose':-1,\n    'learning_rate': 0.0635938724410916,\n 'num_leaves': 31,\n 'colsample_bytree': 0.2556455958822984,\n 'subsample': 0.29787449702328844,\n 'max_depth': 10,\n 'min_child_samples': 16,\n 'reg_alpha': 1.4868037975722531e-06,\n 'reg_lambda': 0.0005428092641970505,\n 'cat_smooth': 95} \nparams_cat =  {'eval_metric': 'RMSE',\n#                              'iterations': 10,\n                'verbose' : 0,\n                'use_best_model': True,\n                'random_seed' : seeds['cat'],\n                'learning_rate': 0.07574755563809993, 'depth': 6, 'l2_leaf_reg': 0.39323612155779863, 'random_strength': 0.20259568870497338, 'grow_policy': 'Lossguide', 'max_bin': 67, 'min_data_in_leaf': 10, 'bootstrap_type': 'Bernoulli', 'subsample': 0.9244602042319705}\nparams_xgb = {'objective': 'reg:squarederror',\n              'eval_metric': 'mae','grow_policy': 'lossguide', 'learning_rate': 0.03982304720613181, 'max_depth': 12, 'max_delta_step': 10, 'min_child_weight': 133, 'colsample_bytree': 0.8709673399493595, 'subsample': 0.7129355859703879, 'max_leaves': 34}\n   \nparams={'params_lgb' : lgb_params,\n      'params_xgb': params_xgb,\n      'params_cat':params_cat}\ndrop_cols = ['ID','date','Train','pm2_5','site_latitudepm2_5enc7',\n       'site_latitudepm2_5enc8', 'site_latitudepm2_5enc6',\n       'site_latitudepm2_5enc5', 'site_latitudepm2_5enc4',\n       'site_latitudepm2_5enc', 'site_latitudepm2_5enc1',\n       'site_latitudepm2_5enc2', 'site_latitudepm2_5enc3','preds']\ntarget = 'pm2_5'","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:41:39.260034Z","iopub.execute_input":"2022-09-21T18:41:39.260797Z","iopub.status.idle":"2022-09-21T18:41:39.279669Z","shell.execute_reply.started":"2022-09-21T18:41:39.260735Z","shell.execute_reply":"2022-09-21T18:41:39.277852Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train,test = preprocess(train_ori,test_ori,scale = True,target_encode = False,\n              include_date = True,remove_nans = False, remove_multi = False)\nfeatures  = [d for d in test.columns if d  not in drop_cols]\n\ntr_1,te_1 = train_aaa(train,test)\ntrain,test = preprocess(train_ori,test_ori,scale = True,target_encode = False,\n              include_date = True,remove_nans = True, remove_multi = True)\nfeatures  = [d for d in test.columns if d  not in drop_cols]\n\ntr_2,te_2 = train_aaa(train,test)\ntrain,test = preprocess(train_ori,test_ori,scale = True,target_encode = True,\n              include_date = True,remove_nans = True, remove_multi = False)\nfeatures  = [d for d in test.columns if d  not in drop_cols]\n\ntr_3,te_3 = train_aaa(train,test)\ntrain,test = preprocess(train_ori,test_ori,scale = True,target_encode = True,\n              include_date = True,remove_nans = True, remove_multi = True)\nfeatures  = [d for d in test.columns if d  not in drop_cols]\n\ntr_4,te_4 = train_aaa(train,test)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:57:26.019216Z","iopub.execute_input":"2022-09-21T18:57:26.019834Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"fold 1 of 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e776721d66d4067bd1e13db8c5030a6"}},"metadata":{}},{"name":"stdout","text":"CV OOF Score for fold 1 is 7.067223042355893\nfold 2 of 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6a421e6e474fb4bd99e5e4ae21ba29"}},"metadata":{}},{"name":"stdout","text":"CV OOF Score for fold 2 is 7.216686751580745\nfold 3 of 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18284da6e747484091063209796da4f3"}},"metadata":{}},{"name":"stdout","text":"CV OOF Score for fold 3 is 7.242074755340273\nfold 4 of 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54fe417ce4e0405db2330a0d8e80078b"}},"metadata":{}},{"name":"stdout","text":"CV OOF Score for fold 4 is 7.696428317528332\n xgb kfold: 1  of  4 : \n[0]\ttrain-mae:56.06458\tvalid-mae:56.64251\n[100]\ttrain-mae:22.52397\tvalid-mae:23.31667\n[200]\ttrain-mae:11.75842\tvalid-mae:12.83409\n[300]\ttrain-mae:8.78974\tvalid-mae:10.26200\n[400]\ttrain-mae:7.50924\tvalid-mae:9.34085\n[500]\ttrain-mae:6.80118\tvalid-mae:8.94031\n[600]\ttrain-mae:6.26975\tvalid-mae:8.68247\n[700]\ttrain-mae:5.85660\tvalid-mae:8.55866\n[800]\ttrain-mae:5.51960\tvalid-mae:8.46620\n[900]\ttrain-mae:5.22919\tvalid-mae:8.38877\n[999]\ttrain-mae:4.98517\tvalid-mae:8.32844\nCV OOF Score for fold 1 is 8.327862326797186\n xgb kfold: 2  of  4 : \n[0]\ttrain-mae:56.21452\tvalid-mae:56.19273\n[100]\ttrain-mae:22.73260\tvalid-mae:22.65811\n[200]\ttrain-mae:11.93064\tvalid-mae:12.35795\n[300]\ttrain-mae:9.03697\tvalid-mae:9.91649\n[400]\ttrain-mae:7.73298\tvalid-mae:9.05789\n[500]\ttrain-mae:7.00870\tvalid-mae:8.64184\n[600]\ttrain-mae:6.48426\tvalid-mae:8.41069\n[700]\ttrain-mae:6.08165\tvalid-mae:8.28360\n[800]\ttrain-mae:5.75449\tvalid-mae:8.20164\n[900]\ttrain-mae:5.46211\tvalid-mae:8.11242\n[999]\ttrain-mae:5.22635\tvalid-mae:8.05917\nCV OOF Score for fold 2 is 8.05514669774744\n xgb kfold: 3  of  4 : \n[0]\ttrain-mae:56.37780\tvalid-mae:55.70298\n[100]\ttrain-mae:22.81377\tvalid-mae:22.49510\n[200]\ttrain-mae:12.07364\tvalid-mae:12.10360\n[300]\ttrain-mae:9.04219\tvalid-mae:9.76618\n[400]\ttrain-mae:7.74731\tvalid-mae:8.87166\n[500]\ttrain-mae:7.02765\tvalid-mae:8.42567\n[600]\ttrain-mae:6.50593\tvalid-mae:8.17742\n[700]\ttrain-mae:6.12355\tvalid-mae:8.03990\n[800]\ttrain-mae:5.79732\tvalid-mae:7.89917\n[900]\ttrain-mae:5.49992\tvalid-mae:7.81686\n[999]\ttrain-mae:5.25951\tvalid-mae:7.76220\nCV OOF Score for fold 3 is 7.7621982209304\n xgb kfold: 4  of  4 : \n[0]\ttrain-mae:56.17941\tvalid-mae:56.29811\n[100]\ttrain-mae:22.79293\tvalid-mae:22.49255\n[200]\ttrain-mae:11.89430\tvalid-mae:12.60895\n[300]\ttrain-mae:8.90940\tvalid-mae:10.27063\n[400]\ttrain-mae:7.63717\tvalid-mae:9.36706\n[500]\ttrain-mae:6.89815\tvalid-mae:8.96594\n[600]\ttrain-mae:6.40505\tvalid-mae:8.73193\n[700]\ttrain-mae:6.00779\tvalid-mae:8.57868\n[800]\ttrain-mae:5.67427\tvalid-mae:8.48537\n[900]\ttrain-mae:5.37420\tvalid-mae:8.41274\n[991]\ttrain-mae:5.14877\tvalid-mae:8.38139\nCV OOF Score for fold 4 is 8.379584324398134\nShape of train_X : (9923, 75), test_X: (4254, 75), train_Y: (9923,)\nfold 1 of 3\nBest number of iterations for fold 1 is: 0\nCV OOF Score for fold 1 is 7.300694761378092\nfold 2 of 3\nBest number of iterations for fold 2 is: 0\nCV OOF Score for fold 2 is 7.551775873436175\nfold 3 of 3\nBest number of iterations for fold 3 is: 0\nCV OOF Score for fold 3 is 7.808018297594651\nfold 1 of 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99acedc2de944e6986526a69b5c5f4a9"}},"metadata":{}},{"name":"stdout","text":"CV OOF Score for fold 1 is 6.976930397515213\nfold 2 of 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99411be4e04746478335fb9d6427bf85"}},"metadata":{}}]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tr_99.columns\n# tr_99.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tas.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tas,vel = keras_overfit(a_train,b_train,a_test,b_test,te_99[a_train.columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def keras_overfit(_train,_train_y,_val,_val_y,test):\n    seedEverything(SEED)\n    all_d = pd.concat([_train,_val]).sort_index()\n    inp = _train.shape[1]\n    y_oof = np.zeros(all_d.shape[0])\n    model = Sequential()\n    model.add(Dense(1, input_dim = inp, activation = 'linear')) # Rectified Linear Unit Activation Function\n    model.compile(loss = 'mse', optimizer = tf.keras.optimizers.RMSprop(lr=0.01),\n                  metrics = [\"mean_absolute_error\"])\n    model.fit(_train, _train_y,\n                     validation_data=(_val, _val_y),\n                     epochs=5,\n                     batch_size = 64,\n                     callbacks=[EarlyStopping(patience=20)],\n                     verbose=1)\n    y_oof = model.predict(all_d)\n    y_predicted = model.predict(test.values)\n    return y_predicted,y_oof","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tr.groupby('site_code').apply(lambda x : metrics.mean_absolute_error(x['pm2_5'],x['preds_xgb'])).sort_values(ascending = False)\n# te_99['pm2_5'] = te_99[['preds_lgb','preds_cat','preds_xgb']].mean(axis = 1)\n# tr_keras = tr[['']]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# te_999[['ID','pm2_5']].to_csv('goofy2.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import optimizers\ndef baseline_model(inp):\n    # Create model here\n    model = Sequential()\n    model.add(Dense(100, input_dim = inp, activation = 'tanh')) # Rectified Linear Unit Activation Function\n    model.add(Dropout(0.3))\n\n#     model.add(Dense(32, activation = 'tanh'))\n    model.add(Dense(8, activation = 'linear'))\n    \n    model.add(Dense(1, activation = 'linear')) # Softmax for multi-class classification\n    # Compile model here\n    model.compile(loss = 'mse', optimizer = tf.keras.optimizers.RMSprop(lr=0.025),\n                  metrics = ['mse'],)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" \nSEED = 42\nseedEverything(SEED)\nK_FOLDS = 2\nkf = KFold(n_splits = K_FOLDS, shuffle=False,random_state = None)\nsplit_by = 'pm2_5'\ny_predicted_keras,models_keras,y_oof_keras,oof_score_keras = train_keras(bl,train,features,train['pm2_5'],test,kf,split_by)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_blend,_,w = models_to_blend()\n# w","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_to_sub(test,preds_blend,'blend31')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EARLY_STOPPING_ROUNDS = 30\nN_ESTIMATORS = 5000\nK_FOLDS = 2\nlgb_params = {\n    \"objective\": \"regression\",\n    \"boosting_type\": \"gbdt\",\n    \"learning_rate\": 0.1,\n    \"n_jobs\": 4,\n    \"seed\": SEED,\n    \"max_depth\": 13,\n#     \"num_leaves\":10,\n    \"force_col_wise\":True,\n#     'min_data_in_leaf' : 5,\n#     'feature_fraction':0.5,\n#     'bagging_fraction':0.8,\n#     \"max_bin\": 255,\n#     'reg_lambda': 0.1,  # L1 regularization term on weights\n#     'reg_lambda': 10,\n    \"metric\": \"MAE\",\n    \"verbose\": -1,\n}\nkf = KFold(n_splits = K_FOLDS, shuffle=False,random_state = None)\ny_predicted_lgb,models_lgb,y_oof_lgb,oof_score_lgb,features_importance_lgb = train_lgb(train,features,\n                                                                                       target,test,\n                                                                                       lgb_params,kf,'pm2_5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import Pool, CatBoostRegressor\nparams_cat = {'iterations':5000,\n        'learning_rate':0.025,\n#         'random_strength':0.1,\n        'early_stopping_rounds':30, \n        'depth':3,\n        'loss_function':'RMSE',\n        'eval_metric':'MAE',\n        'verbose' : 100,\n#         'leaf_estimation_method':'Newton'\n             }\nkf = KFold(n_splits = K_FOLDS, shuffle=True,random_state = SEED)\n\ny_predicted_cat,models_cat,y_oof_cat,oof_score_cat =train_catbo(train[features],\n                                                                        train['pm2_5'],test[features],params_cat,\n                                                                kf, 'pm2_5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_xgb = {\"objective\":\"reg:squarederror\",'learning_rate': 0.1,\n           'max_depth': 3}#, 'subsample': 0.9,\n#           'colsample_bytree': 0.9}\n\ny_predicted_xgb,models_xgb,y_oof_xgb,oof_score_xgb=train_xgb(train[features],\n                                                                        train['pm2_5'],test[features],\n                                                             params_xgb,kf,'pm2_5',num_iter=1000,es = 30,ve = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def models_to_blend():\n    w_cat = cust_score(y_oof_cat).sort_values(by = 'err', ascending = False)['err'].iloc[:4].mean()\n    w_lgb = cust_score(y_oof_lgb).sort_values(by = 'err', ascending = False)['err'].iloc[:4].mean()\n    w_xgb = cust_score(y_oof_xgb).sort_values(by = 'err', ascending = False)['err'].iloc[:4].mean()\n    w_keras = cust_score(y_oof_keras).sort_values(by = 'err', ascending = False)['err'].iloc[:4].mean()\n    weigths = [1/w_cat,1/w_lgb,1/w_xgb,1/w_keras]\n    blend_preds = np.average(np.vstack([y_predicted_cat,y_predicted_xgb,y_predicted_cat,y_predicted_keras.squeeze()]),axis = 0,weights=weigths)\n    blend_preds_train = np.average(np.vstack([y_oof_cat,y_oof_xgb,y_oof_cat,y_oof_keras.squeeze()]),axis = 0,weights=weigths)\n    w_blend = cust_score(blend_preds_train).sort_values(by = 'err', ascending = False)['err'].iloc[:4].mean()\n    weights = [w_cat,w_lgb,w_xgb,w_keras,w_blend]\n    print(\"mae of cat : {}.-------- worst {}\".format(metrics.mean_absolute_error(train['pm2_5'],y_oof_cat).round(3),w_cat.round(3)))\n    print(\"mae of lgb : {}..-------- worst {}\".format(metrics.mean_absolute_error(train['pm2_5'],y_oof_lgb).round(3),w_lgb.round(3)))\n    print(\"mae of xgb : {}..-------- worst {}\".format(metrics.mean_absolute_error(train['pm2_5'],y_oof_xgb).round(3),w_xgb.round(3)))\n    print(\"mae of keras : {}..-------- worst {}\".format(metrics.mean_absolute_error(train['pm2_5'],y_oof_keras).round(3),w_keras.round(3)))\n    print(\"mae of blend : {}..-------- worst {}\".format(metrics.mean_absolute_error(train['pm2_5'],blend_preds_train).round(3),w_blend.round(3)))\n          \n    return blend_preds,blend_preds_train,weights\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_,w = models_to_blend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.vstack([y_predicted_lgb,y_predicted_xgb]).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_predicted_xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.average(np.vstack([y_predicted_lgb,y_predicted_xgb]),axis = 0,weights=[10,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_to_sub(test,y_predicted_lgb+,'lgb_scaled3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nSEED = 42\nK_FOLDS = 4\ntarget = 'pm2_5'\ndef objective(trial):\n    params = {\n                #'iterations' : 10000, replaced by early stopping\n                'eval_metric': 'RMSE',\n                'verbose' : 500,\n                'use_best_model': True,\n                'random_seed' : SEED,\n                'learning_rate' :trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n                \"depth\": trial.suggest_int(\"depth\", 1, 9),\n                'l2_leaf_reg' :trial.suggest_loguniform('l2_leaf_reg', 1e-8, 10),\n                'random_strength' : trial.suggest_loguniform('random_strength', 0.1, 10),\n                'grow_policy':trial.suggest_categorical ('grow_policy', ['Lossguide','SymmetricTree']),\n                'max_bin': trial.suggest_int(\"max_bin\", 20, 500),\n                'min_data_in_leaf':trial.suggest_int('min_data_in_leaf', 1, 100),\n                \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"])\n            }\n    \n    if params['grow_policy'] == 'Lossguide':\n        params['max_leaves']:trial.suggest_int('max_leaves', 1, 20)\n    if params[\"bootstrap_type\"] == \"Bayesian\":\n        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif params[\"bootstrap_type\"] == \"Bernoulli\":\n        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n        \n    \n    kf = KFold(n_splits = K_FOLDS, shuffle=True,random_state = SEED)\n\n\n    y_predicted_cat,models_cat,y_oof_cat,oof_score_cat =train_catbo(train,features,target,test,params,\n                                                                    kf=kf, split_by = 'pm2_5')\n    \n        \n    return cust_score(y_oof_cat).sort_values(by = 'err', ascending = False)['err'].iloc[:4].mean()\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features\n# study.best_params\n# {'learning_rate': 0.09336356367527626,\n#  'depth': 8,\n#  'l2_leaf_reg': 0.00033354798569319656,\n#  'random_strength': 10.395143072706773,\n#  'grow_policy': 'SymmetricTree',\n#  'max_bin': 111,\n#  'min_data_in_leaf': 94,\n#  'bootstrap_type': 'Bernoulli',\n#  'subsample': 0.4351151265075187}### params with MAE, metric\n# cat_b_params = {'learning_rate': 0.07574755563809993, 'depth': 6, 'l2_leaf_reg': 0.39323612155779863, 'random_strength': 0.20259568870497338, 'grow_policy': 'Lossguide', 'max_bin': 67, 'min_data_in_leaf': 10, 'bootstrap_type': 'Bernoulli', 'subsample': 0.9244602042319705}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study.direction\nK_FOLDS = 5\n# kf = KFold(n_splits=N_split)\nSEED = 43\nimport optuna\n\ndef objective(trial):\n   \n    params = { 'objective': 'reg:squarederror',\n              'eval_metric': 'mae',\n#             'tree_method': 'hist',\n            'grow_policy' : trial.suggest_categorical ('grow_policy', ['lossguide','depthwise']),\n            'learning_rate':trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n            'max_depth': trial.suggest_int('max_depth', 3, 20),# a virer avec'depthwise'\n#             'reg_alpha': trial.suggest_loguniform('reg_alpha', 1, 10),\n#             'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-15, 10.0),\n            'max_delta_step':trial.suggest_int('max_delta_step', 1, 10),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n            'colsample_bytree':trial.suggest_loguniform('colsample_bytree', 0.4, 1.0),\n            'subsample': trial.suggest_loguniform('subsample', 0.4, 1.0),\n            'seed':1\n                }\n    if params['grow_policy'] == 'lossguide':\n        params['max_leaves'] = trial.suggest_int('max_leaves', 1, 100)   \n        \n    kf = KFold(n_splits = K_FOLDS, shuffle=True,random_state = SEED)\n\n    y_predicted_xgb,models_xgb,y_oof_xgb,oof_score_xgb=train_xgb(train,features,target,test,\n                                                             params,kf,'pm2_5',num_iter=1000,es = 30,ve = 500)\n        \n    return cust_score(y_oof_xgb).sort_values(by = 'err', ascending = False)['err'].iloc[:4].mean()\n\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4 folds\n# xgb_params = {'grow_policy': 'depthwise', 'learning_rate': 0.10769618165955801, 'max_depth': 14, 'max_delta_step': 2, 'min_child_weight': 132, 'colsample_bytree': 0.6353119293029151, 'subsample': 0.6254457606715289,'tree_method': 'hist'}\n# xgb_params = {'grow_policy': 'lossguide', 'learning_rate': 0.03982304720613181, 'max_depth': 12, 'max_delta_step': 10, 'min_child_weight': 133, 'colsample_bytree': 0.8709673399493595, 'subsample': 0.7129355859703879, 'max_leaves': 34}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# N_split = 2\n# kf = KFold(n_splits=N_split)\n# EARLY_STOPPING_ROUNDS = 30\nN_ESTIMATORS = 5000\nK_FOLDS = 3\nimport optuna\n\ndef objective(trial):\n    params = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'n_estimators':5000,\n            'verbose':-1,\n            'learning_rate': trial.suggest_float(\"learning_rate\", 0.04,0.4),\n            'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n            'colsample_bytree':trial.suggest_float(\"colsample\", 0.1,0.3),\n            'subsample': trial.suggest_float(\"subsample\", 0.1,0.3),\n            'max_depth': trial.suggest_int('max_depth', 3, 10),\n            'min_child_samples': trial.suggest_int('min_child_samples', 3, 2000),\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n            'cat_smooth':trial.suggest_int('cat_smooth', 1, 100)\n            }\n    \n    kf = KFold(n_splits = K_FOLDS, shuffle=False,random_state = None)\n    y_predicted_lgb,models_lgb,y_oof_lgb,oof_score_lgb,features_importance_lgb = train_lgb(train,features,\n                                                                                  target,test,params = params,kf = kf,split_by = 'pm2_5') \n    return cust_score(y_oof_lgb).sort_values(by = 'err', ascending = False)['err'].iloc[:4].mean()\n\n\nstudy = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=20)\nbest_params_lgb = {'objective': 'regression',\n            'metric': 'mae',\n            'n_estimators':5000,\n            'verbose':-1,\n    'learning_rate': 0.0635938724410916,\n 'num_leaves': 31,\n 'colsample': 0.2556455958822984,\n 'subsample': 0.29787449702328844,\n 'max_depth': 10,\n 'min_child_samples': 16,\n 'reg_alpha': 1.4868037975722531e-06,\n 'reg_lambda': 0.0005428092641970505,\n 'cat_smooth': 95} ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"study.best","metadata":{}},{"cell_type":"code","source":"study.best_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}